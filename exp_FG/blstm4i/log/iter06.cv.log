speech-HP-Z2-Tower-G9
nnet-train-multistream-perutt --cross-validate=true --randomize=false --verbose=0 --num-streams=10 --max-frames=15000 --feature-transform=exp_FG/blstm4i/final.feature_transform 'ark:copy-feats scp:exp_FG/blstm4i/cv.scp ark:- | apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_cv10/utt2spk scp:data-fbank/train_cv10/cmvn.scp ark:- ark:- | add-deltas --delta-order=2 ark:- ark:- |' 'ark:ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- |' exp_FG/blstm4i/nnet/nnet_iter06 
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:243) Not in compute-exclusive mode.  Suggestion: use 'nvidia-smi -c 3' to set compute exclusive mode
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:438) Selecting from 1 GPUs
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:453) cudaSetDevice(0): NVIDIA RTX A2000 12GB	free:11620M, used:410M, total:12031M, free/total:0.96589
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:501) Device: 0, mem_ratio: 0.96589
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:382) Trying to select device: 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:511) Success selecting device 0 free mem ratio: 0.96589
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:FinalizeActiveGpu():cu-device.cc:338) The active GPU is [0]: NVIDIA RTX A2000 12GB	free:11106M, used:924M, total:12031M, free/total:0.923168 version 8.6
copy-feats scp:exp_FG/blstm4i/cv.scp ark:- 
apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_cv10/utt2spk scp:data-fbank/train_cv10/cmvn.scp ark:- ark:- 
add-deltas --delta-order=2 ark:- ark:- 
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:144) CROSS-VALIDATION STARTED
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Read():nnet/nnet-matrix-buffer.h:191) Read() started... Buffer size in MB: 0, max 3072, having 0 utterances.
ali-to-post ark:- ark:- 
ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl 'ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |' ark:- 
LOG (copy-feats[5.5.1074~1-71f3]:main():copy-feats.cc:143) Copied 296 feature matrices.
LOG (apply-cmvn[5.5.1074~1-71f3]:main():apply-cmvn.cc:159) Applied cepstral mean and variance normalization to 296 utterances, errors on 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:303) ### After 0 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:304) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.616411, max 0.65931, mean 0.00393689, stddev 0.085431, skewness 0.0131734, kurtosis 0.689007 ) 
  f_w_gifo_r_   ( min -0.423845, max 0.409684, mean -0.000584791, stddev 0.0781972, skewness -0.00163398, kurtosis 0.0165687 ) 
  f_bias_   ( min -0.347334, max 1.37568, mean 0.212595, stddev 0.462292, skewness 1.06384, kurtosis -0.652125 ) 
  f_peephole_i_c_   ( min -0.509369, max 0.501211, mean -0.00289568, stddev 0.129123, skewness 0.0847997, kurtosis 1.54164 ) 
  f_peephole_f_c_   ( min -0.677315, max 0.897532, mean 0.00410478, stddev 0.174811, skewness 0.249118, kurtosis 4.62533 ) 
  f_peephole_o_c_   ( min -0.506917, max 0.4708, mean -0.0109365, stddev 0.188117, skewness 0.197994, kurtosis -0.255551 ) 
  f_w_r_m_   ( min -0.515665, max 0.492348, mean 0.000606092, stddev 0.103025, skewness 0.00139011, kurtosis 0.0110598 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -1.52948, max 1.0024, mean 0.00620802, stddev 0.0921309, skewness -0.148393, kurtosis 3.98057 ) 
  b_w_gifo_r_   ( min -0.365011, max 0.343452, mean -0.000185902, stddev 0.071249, skewness 0.000138502, kurtosis -0.273844 ) 
  b_bias_   ( min -0.362876, max 1.21022, mean 0.206201, stddev 0.452877, skewness 1.04238, kurtosis -0.689403 ) 
  b_peephole_i_c_   ( min -0.379005, max 0.296419, mean 0.00613369, stddev 0.0979482, skewness -0.0601382, kurtosis 1.0279 ) 
  b_peephole_f_c_   ( min -0.632696, max 0.725181, mean 0.0133486, stddev 0.171829, skewness 0.623662, kurtosis 3.69561 ) 
  b_peephole_o_c_   ( min -0.561635, max 0.501367, mean -0.0161429, stddev 0.193552, skewness -0.16468, kurtosis 0.301024 ) 
  b_w_r_m_   ( min -0.397783, max 0.377919, mean -0.000271505, stddev 0.0920643, skewness 0.000596733, kurtosis -0.0551715 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.955387, max 0.750977, mean -0.000155917, stddev 0.107488, skewness 0.00584201, kurtosis 0.0635016 ) , lr-coef 1, max-norm 0
  bias ( min -0.0882712, max 2.44244, mean -7.21775e-10, stddev 0.0801137, skewness 23.2236, kurtosis 680.523 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:305) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -11.004, max 12.2048, mean 0.00682255, stddev 0.969149, skewness 0.287893, kurtosis 3.83541 ) 
[1] output of <BlstmProjected> ( min -4.14554, max 4.32974, mean -0.00191442, stddev 0.752087, skewness 0.00467454, kurtosis 0.89929 ) 
[2] output of <Tanh> ( min -0.999499, max 0.999653, mean -0.00141996, stddev 0.513442, skewness 0.000698605, kurtosis -0.83534 ) 
[3] output of <AffineTransform> ( min -12.7865, max 20.7383, mean 0.00763731, stddev 2.39982, skewness 0.675965, kurtosis 2.15175 ) 
[4] output of <Softmax> ( min 2.90883e-12, max 0.999346, mean 0.000779991, stddev 0.0169719, skewness 39.6417, kurtosis 1815.21 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:334) ### After 79212 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:335) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.616411, max 0.65931, mean 0.00393689, stddev 0.085431, skewness 0.0131734, kurtosis 0.689007 ) 
  f_w_gifo_r_   ( min -0.423845, max 0.409684, mean -0.000584791, stddev 0.0781972, skewness -0.00163398, kurtosis 0.0165687 ) 
  f_bias_   ( min -0.347334, max 1.37568, mean 0.212595, stddev 0.462292, skewness 1.06384, kurtosis -0.652125 ) 
  f_peephole_i_c_   ( min -0.509369, max 0.501211, mean -0.00289568, stddev 0.129123, skewness 0.0847997, kurtosis 1.54164 ) 
  f_peephole_f_c_   ( min -0.677315, max 0.897532, mean 0.00410478, stddev 0.174811, skewness 0.249118, kurtosis 4.62533 ) 
  f_peephole_o_c_   ( min -0.506917, max 0.4708, mean -0.0109365, stddev 0.188117, skewness 0.197994, kurtosis -0.255551 ) 
  f_w_r_m_   ( min -0.515665, max 0.492348, mean 0.000606092, stddev 0.103025, skewness 0.00139011, kurtosis 0.0110598 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -1.52948, max 1.0024, mean 0.00620802, stddev 0.0921309, skewness -0.148393, kurtosis 3.98057 ) 
  b_w_gifo_r_   ( min -0.365011, max 0.343452, mean -0.000185902, stddev 0.071249, skewness 0.000138502, kurtosis -0.273844 ) 
  b_bias_   ( min -0.362876, max 1.21022, mean 0.206201, stddev 0.452877, skewness 1.04238, kurtosis -0.689403 ) 
  b_peephole_i_c_   ( min -0.379005, max 0.296419, mean 0.00613369, stddev 0.0979482, skewness -0.0601382, kurtosis 1.0279 ) 
  b_peephole_f_c_   ( min -0.632696, max 0.725181, mean 0.0133486, stddev 0.171829, skewness 0.623662, kurtosis 3.69561 ) 
  b_peephole_o_c_   ( min -0.561635, max 0.501367, mean -0.0161429, stddev 0.193552, skewness -0.16468, kurtosis 0.301024 ) 
  b_w_r_m_   ( min -0.397783, max 0.377919, mean -0.000271505, stddev 0.0920643, skewness 0.000596733, kurtosis -0.0551715 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.955387, max 0.750977, mean -0.000155917, stddev 0.107488, skewness 0.00584201, kurtosis 0.0635016 ) , lr-coef 1, max-norm 0
  bias ( min -0.0882712, max 2.44244, mean -7.21775e-10, stddev 0.0801137, skewness 23.2236, kurtosis 680.523 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:336) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -7.66434, max 7.26584, mean -0.0147177, stddev 0.961893, skewness 0.577064, kurtosis 2.62679 ) 
[1] output of <BlstmProjected> ( min -4.15416, max 4.31142, mean -0.00225566, stddev 0.710537, skewness -0.042682, kurtosis 1.61759 ) 
[2] output of <Tanh> ( min -0.999507, max 0.99964, mean -0.000205194, stddev 0.481134, skewness -0.0142239, kurtosis -0.546911 ) 
[3] output of <AffineTransform> ( min -12.1564, max 20.6675, mean 0.0108702, stddev 2.27629, skewness 0.844606, kurtosis 3.44551 ) 
[4] output of <Softmax> ( min 4.07056e-12, max 0.998371, mean 0.000780871, stddev 0.0187006, skewness 36.8961, kurtosis 1543.11 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:346) PER-CLASS PERFORMANCE:
@@@ Frames per-class: [ 1670 30 8 79 45 6 24 17 70 220 22 36 34 39 10 53 43 120 22 20 34 66 38 35 87 73 52 35 43 17 86 33 19 32 16 75 106 10 5 1873 21251 22 63 135 22 30 80 98 44 43 9 67 66 27 41 51 107 54 285 40 48 24 308 222 21 35 16 33 248 15 54 49 139 81 14 4 45 35 47 26 17 66 4 14 44 25 18 49 111 11 94 64 72 34 66 27 32 33 78 42 15 34 9 135 37 103 31 64 37 21 67 0 93 69 23 2 34 10 32 37 25 34 18 19 28 43 46 13 4 26 88 25 22 43 33 62 12 98 32 25 22 78 11 62 24 13 88 37 45 30 117 35 60 38 37 129 16 23 64 982 29 24 10 20 31 33 34 16 52 44 64 25 22 25 3 9 30 11 42 35 13 12 11 8 32 32 9 20 32 48 32 42 56 36 35 41 27 16 5 29 35 31 12 7 9 45 27 81 21 16 19 72 29 0 46 70 68 37 26 33 50 37 32 9 5 26 23 15 14 128 52 51 59 20 20 8 73 5 17 32 22 16 4 39 60 11 16 7 19 20 8 7 52 22 37 15 32 12 39 94 72 13 59 17 69 35 25 11 13 18 58 26 24 119 16 14 9 5 13 35 18 40 17 83 10 20 20 32 21 19 33 104 48 60 79 47 20 14 27 49 56 7 20 34 199 37 4 3 21 22 15 27 39 6 10 11 22 25 47 21 6 14 6 60 3 43 26 14 8 10 70 4 15 29 11 133 11 35 22 51 21 57 14 109 15 36 29 5 21 20 18 19 114 55 69 35 21 21 64 44 39 43 15 21 37 31 46 70 32 25 50 48 101 37 32 62 34 53 184 59 33 24 47 16 35 9 79 14 45 12 20 16 36 46 8 52 28 13 12 31 17 29 40 53 13 20 9 73 85 128 108 21 7 44 30 32 12 46 15 32 90 31 20 32 52 21 237 45 11 21 215 49 25 30 31 29 71 13 57 48 98 46 27 41 19 60 21 4 38 48 50 5 63 71 0 0 25 47 11 16 42 44 49 71 27 15 49 22 22 11 30 67 41 56 15 28 24 35 7 14 15 34 7 8 38 0 9 28 57 50 12 13 39 28 40 746 35 13 19 19 18 27 12 16 52 60 21 82 1 33 49 89 6 38 42 18 51 39 26 11 19 7 23 28 18 51 98 35 37 10 34 11 56 273 7 39 31 18 21 15 24 22 15 25 6 21 53 59 24 25 15 4 49 28 14 9 8 3 8 29 34 0 8 18 51 52 48 23 3 60 16 24 35 46 16 52 37 72 3 38 59 15 5 40 14 54 52 32 51 4 14 21 187 40 33 16 15 18 19 52 32 28 37 29 41 52 10 6 95 18 46 53 17 8 5 18 31 15 14 27 7 8 17 7 93 28 14 64 22 15 8 48 40 29 32 15 10 26 38 92 13 54 242 3 84 0 22 55 226 137 6 121 19 57 20 16 55 85 4 17 33 53 42 81 83 26 30 36 55 64 9 66 84 47 14 13 91 43 19 13 23 37 17 19 27 24 0 59 88 26 15 20 111 16 50 7 32 22 5 17 15 28 38 22 27 28 48 52 41 28 16 114 105 37 112 43 4 69 17 87 95 11 45 15 22 24 24 65 3 27 29 73 63 47 17 34 18 53 17 37 17 20 6 14 36 78 20 8 14 23 47 6 6 22 3 11 89 3 5 18 20 146 25 8 25 44 29 28 22 26 34 24 73 34 85 42 37 35 79 40 39 34 32 23 18 104 65 9 45 19 21 17 48 42 21 40 28 25 27 73 23 34 2 186 119 39 90 27 23 23 38 48 0 28 66 71 45 13 24 7 25 19 16 23 24 129 50 34 21 69 32 14 26 16 14 79 10 15 35 83 24 21 61 28 128 42 66 19 12 25 28 26 60 26 36 30 128 60 8 15 24 59 47 2 19 63 42 22 4 29 30 49 65 12 8 20 32 71 48 32 17 59 28 24 21 20 22 26 24 15 35 0 17 35 13 29 14 47 49 7 13 6 17 31 19 38 15 53 143 34 68 37 23 6 34 29 7 46 92 30 48 31 3 30 16 40 24 44 69 43 19 65 170 29 5 28 134 73 20 11 82 86 29 13 21 12 10 24 12 81 163 12 193 6 28 39 45 31 23 36 65 11 39 113 30 11 28 125 45 52 34 87 20 29 16 97 42 97 26 45 39 15 109 53 18 59 52 33 29 52 31 45 25 45 136 11 24 33 61 26 28 29 106 30 33 18 48 56 86 92 23 37 23 14 2 14 36 346 138 60 357 25 129 51 39 29 11 53 15 14 13 44 38 41 41 8 18 32 16 20 21 17 8 96 16 23 156 45 91 21 6 43 48 25 45 29 8 22 23 31 42 3 198 10 14 29 25 39 64 39 11 26 28 39 96 13 53 5 34 9 11 21 67 28 24 50 19 10 11 22 23 15 13 30 30 176 37 84 80 124 21 74 32 81 43 23 25 68 52 83 27 20 0 33 56 14 23 30 22 132 113 9 71 18 15 16 5 38 61 32 70 35 44 36 82 12 19 25 56 12 24 164 178 24 42 24 19 11 41 67 112 11 27 62 39 5 26 82 58 18 39 25 15 60 20 122 54 35 24 51 28 24 13 180 152 39 59 14 42 123 70 441 14 35 36 425 22 33 13 41 32 48 41 161 16 18 110 56 36 262 66 33 47 21 15 29 84 58 48 47 2 22 14 22 35 120 98 20 59 274 17 15 22 15 59 16 0 27 31 40 47 63 69 16 68 32 39 426 46 55 29 28 47 14 42 42 14 17 28 110 23 16 27 12 36 25 38 21 57 45 37 40 25 7 21 46 54 29 44 49 16 46 13 48 16 103 37 35 12 34 ]
@@@ Loss per-class: [ 0.675088 1.24425 1.38111 0.983361 1.23772 2.72756 1.38314 0.621457 2.5762 0.925541 2.26566 2.87325 0.481636 2.93768 2.46043 3.51846 2.23759 2.36096 3.80851 0.733405 1.75543 1.90672 4.59874 1.89733 0.897459 1.85622 1.28634 4.3726 1.36544 1.27065 4.27653 5.99706 1.84885 5.37828 0.294073 0.627266 2.70897 1.72347 3.55898 0.95558 1.3074 3.30635 4.05462 1.62399 1.3066 2.68855 2.23675 6.97379 0.859633 0.813467 2.35838 2.78717 2.10405 2.28687 1.43097 1.25369 1.45757 0.660694 1.26682 1.52707 3.61812 1.79341 0.780595 1.60758 3.43077 1.96077 1.49763 3.02799 0.600506 2.56193 1.44687 2.19882 1.7623 1.56776 0.875633 0.954643 2.92917 0.354681 2.65231 1.33293 2.00617 2.1946 3.457 0.806934 1.6386 1.88756 3.13607 2.72336 1.47466 2.2205 2.05914 0.555689 1.98429 2.8596 4.23665 4.60788 0.876315 1.69002 2.06351 0.64891 4.86235 2.10226 3.0464 1.63206 1.40336 3.48816 2.43736 2.89561 2.07826 1.04846 2.25863 0 2.92143 1.7068 1.35283 1.80906 1.73351 2.02126 1.60666 2.5698 2.14275 3.15587 2.18898 1.56381 0.838214 2.53016 3.20983 2.73378 0.387545 1.41156 1.31403 1.85834 1.13725 2.05617 2.41711 1.27861 1.67128 0.677312 1.8823 3.18623 3.22045 1.34101 1.47524 2.00567 3.61353 0.364403 2.00091 2.25165 2.48073 1.48383 1.12197 2.03311 2.94885 1.8893 1.28574 0.360421 1.18035 2.03135 1.64282 2.75232 1.18427 2.06017 2.04244 5.28878 1.20423 3.10413 2.80942 1.60683 0.905129 3.12632 1.69815 1.71698 1.83496 2.62622 3.04567 0.653781 1.51525 1.59655 2.27788 2.8052 1.59668 2.29499 6.73524 0.820341 1.42972 2.04601 1.00952 1.68726 2.30465 2.83387 0.624593 3.01375 1.95092 2.47316 1.98705 1.18261 2.53431 2.29172 2.95614 0.605552 2.47831 2.92405 1.31031 2.10875 2.9653 1.2595 2.17529 1.36673 3.12499 2.80227 1.44548 2.30343 0.767606 0 1.8591 1.15777 2.14293 1.89042 1.86783 1.31562 2.73357 2.35448 2.00521 0.98916 2.55515 2.66475 4.80382 3.96809 0.660034 3.02541 1.89918 1.70713 1.60331 2.07195 3.72656 3.35203 1.29038 2.96428 1.99524 1.17573 2.89079 2.12537 2.71943 0.879357 2.76358 3.68623 1.13099 4.61277 2.25528 3.18675 2.94556 2.74638 2.92525 1.21893 2.32419 0.801162 1.40254 0.652992 2.39953 0.804215 1.5331 1.40972 2.70701 1.9038 2.34765 0.0943788 1.69655 1.56579 0.54168 0.722864 2.7348 0.991751 4.57225 3.46028 2.44063 5.21012 0.220166 4.00139 1.21303 1.55399 0.96713 1.82332 1.21551 3.75074 0.975851 1.99777 0.606396 1.20267 8.66246 1.23078 1.59968 1.70208 1.06319 0.382375 3.00892 3.30371 1.21469 2.25502 3.27743 6.75617 1.69658 1.59424 1.9866 2.66787 0.449456 1.25419 4.6864 3.14747 3.60551 3.57851 1.46168 2.28478 1.52747 0.915818 4.86952 3.4407 1.32007 1.58277 1.69239 6.08475 4.08366 1.56691 4.21294 1.30913 4.30534 2.44253 3.09908 4.07515 1.86047 1.28415 1.86207 1.29222 1.23483 1.80117 0.798683 1.76219 2.36836 2.38343 2.46751 2.49254 1.85417 1.73256 0.753433 1.39432 2.83357 2.23703 1.84243 2.66726 1.65022 2.58196 3.1268 1.80889 0.813562 2.66918 2.34891 1.90829 2.4162 2.12532 2.78879 4.09574 1.38856 2.87311 2.26953 2.3807 1.90451 4.66029 1.14827 1.42045 2.76649 3.40623 3.29155 2.1734 1.38549 1.63237 0.579939 0.857838 1.21177 0.737368 2.39394 2.05018 2.00055 2.04729 0.667492 0.536007 1.42971 1.95234 2.04792 2.27723 2.16194 5.0678 1.84745 1.88762 3.02127 2.66912 2.9624 1.38818 2.48556 3.99273 1.95238 2.34771 3.71679 1.28017 1.18421 0.797252 2.84383 0.360168 1.00795 2.41546 1.57922 2.81203 4.64962 2.18444 1.11832 2.66314 1.96336 0.737147 0.682944 0.519029 3.44869 1.43861 1.98998 0.966335 0.5459 0.769689 5.08777 1.53498 0.54964 0.918733 4.04664 1.12871 1.98129 1.78649 2.7411 2.78269 3.12433 1.75626 1.69007 2.8899 2.67956 1.55854 1.67098 1.50511 2.87229 2.64902 5.99083 0.976574 5.03276 2.96276 2.12767 2.4466 2.07104 1.97297 2.86275 3.71685 0 0 0.234937 1.44855 1.10417 3.11278 0.425226 1.71739 2.24014 0.917257 2.04699 2.30762 2.26557 0.769232 1.31044 2.65366 1.2247 1.27779 2.28236 1.2218 2.39813 1.28636 0.508594 2.63353 4.85883 2.6868 4.4526 1.23156 1.57611 2.8085 3.30251 0 1.44974 3.05864 1.19789 1.80459 1.39815 2.50555 2.06965 6.19177 1.09677 2.0532 1.28902 3.7641 3.27491 1.34769 1.82761 1.00481 2.33686 4.76038 0.898407 1.32765 1.92292 0.803865 4.44871 1.50712 5.83609 1.8486 1.60121 1.30569 2.56611 1.67083 2.43277 2.24618 3.0204 1.70542 3.74377 2.25649 2.87462 3.32829 2.93705 2.41025 2.9987 3.05039 3.52244 1.69556 4.14047 1.77478 4.61104 4.0875 1.98617 1.72504 1.90091 0.969761 2.10524 2.721 2.02198 6.07451 2.94717 1.76695 2.98756 1.69998 1.02157 2.02286 1.93793 1.94451 2.92125 2.78882 1.40118 0.931784 3.49351 0.923975 1.55362 2.0399 1.7286 2.63714 3.46783 0 1.90861 1.86481 1.11325 2.29181 1.02588 1.79493 1.21177 6.38621 2.72927 1.82121 3.27141 1.04962 4.36342 6.70613 3.98486 1.21315 1.16388 4.45761 3.18244 2.26491 1.56228 3.83012 2.39043 1.03625 5.07773 3.2586 0.999253 1.45648 0.980946 1.9479 1.29587 1.89246 3.24733 2.34723 1.62581 2.25805 3.84024 1.01076 1.93873 4.1888 4.2212 2.62601 1.20018 1.95977 2.63775 1.66132 0.594066 1.63701 2.19971 0.934555 3.47517 0.215122 1.26478 2.00046 4.49636 1.57662 2.6757 3.89638 1.54006 1.06005 1.69808 1.68116 5.44969 0.88261 1.2801 2.92105 1.28039 2.31384 4.93096 3.91148 1.87099 3.63585 0.669027 2.12719 1.50084 1.36672 2.13873 4.5788 3.4539 3.15986 0.928279 1.22086 1.53213 0 2.39847 1.62358 5.17769 1.81611 4.82748 2.14133 2.04179 0.719546 1.74374 3.12397 2.30579 7.28828 4.72108 1.35406 3.13199 1.90436 1.95723 3.24829 1.63845 4.12731 5.26908 1.68269 2.9574 1.39029 6.4948 1.49796 1.47263 2.40699 5.72896 6.67392 5.00921 4.19067 3.22706 5.44675 2.92095 3.51187 1.31432 4.59341 3.49438 3.01445 0 2.08815 2.77389 3.14851 1.8997 3.07696 5.99031 0.962136 3.05347 1.94186 0.98773 4.5822 2.08191 1.98656 2.04047 2.92585 1.84391 2.28412 2.88445 2.45083 2.4085 2.84 1.08117 2.15497 5.02934 1.38697 0.60533 1.15069 0.97439 0.953306 2.48284 3.46505 1.12806 3.88479 2.35581 6.11401 1.68771 0.580267 3.03269 2.88181 2.75274 3.95283 1.7461 1.14569 1.76446 1.23367 1.78401 4.30896 0.708757 1.81502 5.12498 2.49728 1.68546 2.60477 2.40276 2.81128 1.00149 4.08376 1.30307 2.49059 0.759708 2.51744 1.69972 1.89551 1.92074 1.43231 2.23342 2.09905 1.68478 2.16841 6.35853 3.82305 2.04376 1.66252 1.32091 1.08522 2.13317 1.55372 2.17113 3.36998 2.18918 5.10566 2.29043 3.36179 1.42058 1.85001 3.80793 3.27803 1.38226 1.34489 2.91967 3.25068 2.54166 2.81579 2.59662 1.13834 2.2289 2.19996 4.30892 1.75443 1.05169 2.32049 2.00771 1.44426 2.31003 4.95266 0.945908 4.04254 2.7529 3.54917 2.36695 1.54141 2.4919 0.718485 5.70279 4.16248 3.64894 1.66691 0.988738 2.79131 1.62661 2.01631 1.16349 3.42206 1.91581 3.88423 0 2.33771 1.82993 2.52533 2.61597 3.70941 3.70732 2.28675 1.78986 2.85386 1.34687 2.67832 5.53554 3.41626 6.52177 2.0051 0.683835 2.48843 1.69713 3.70512 3.15276 6.18123 2.59393 1.13395 2.64439 2.71768 0.49804 0.338555 3.37883 1.54418 1.20754 1.37472 5.03171 2.29866 5.6699 1.75571 3.31171 2.04677 3.28599 3.14694 3.28049 2.64625 1.12134 1.76175 1.59754 1.80185 2.18309 1.67056 3.06397 2.06104 2.2927 2.06177 3.24283 1.6767 1.61836 2.52546 4.19732 2.19488 2.51977 1.82948 1.11515 1.90018 2.1388 4.1935 2.3854 1.03677 1.25956 1.76009 0.752978 0.912874 0.939611 3.97554 2.69745 2.85814 3.69973 1.51675 1.29733 2.997 1.53506 0 2.63076 1.12386 2.57572 1.89779 2.73856 1.88834 2.32327 2.71208 2.05066 2.25943 2.93343 2.14344 2.87449 1.85813 1.10646 1.20225 1.38075 1.81845 5.27786 1.86418 2.52916 4.44812 1.55224 3.36173 1.64938 2.02766 2.82933 1.7041 1.69215 2.8588 3.76811 2.18561 3.22921 1.38818 1.1624 3.45812 2.65616 2.8368 1.18345 3.03813 2.31715 2.0119 2.29816 2.63257 3.72001 2.07907 2.21696 2.76766 2.41041 2.6234 2.2759 2.2616 3.02113 3.14742 2.08567 2.10267 2.1895 2.103 1.6866 1.49698 6.00739 0.810648 2.87021 1.715 2.81002 3.78761 1.34791 1.77559 3.56738 3.43 3.45055 3.51694 2.98288 1.07819 5.15219 2.29822 0.395323 2.27644 2.83373 1.4027 1.60464 4.70826 1.94498 3.29807 3.94538 2.93754 4.01469 1.86998 0.691651 1.97612 3.98993 1.61402 2.92779 1.39487 1.86433 3.94199 1.76263 1.85541 2.31728 1.49692 2.12191 1.77225 4.07181 2.99214 2.16216 3.39976 2.08615 1.89513 2.33032 1.34193 2.37974 1.14474 3.28441 1.03407 1.77634 4.75779 0.816787 4.87615 2.61796 1.84282 2.58983 1.74878 0.760271 2.68692 3.53558 2.33868 4.41279 2.40106 3.37484 0.90464 2.14446 2.22745 2.10696 2.0029 4.8044 2.56456 2.19304 3.78775 5.21138 0.428195 3.31052 4.16358 2.17898 2.3106 3.59381 2.16147 1.89935 3.22008 3.37485 1.86295 2.08168 4.83996 0.958111 1.74154 3.10157 3.10638 2.52501 4.53962 1.18837 5.18924 3.59084 2.64775 3.06455 2.5117 3.69643 1.30738 2.87491 0.944992 1.95001 2.68854 1.81522 2.02336 3.33256 2.44674 2.36534 1.94908 2.33401 0.887276 2.56533 0.870007 2.55129 2.93246 1.13611 3.30772 2.88528 2.45263 1.37458 2.55682 2.54943 3.82662 2.43666 3.13046 3.33207 1.44403 0.959378 3.63677 4.30724 0.987965 2.63925 2.12029 2.12484 3.16687 1.48004 3.15549 2.86261 0.479858 2.06262 2.93275 3.19303 3.08968 3.67624 3.41398 1.69718 2.87927 1.46766 2.47031 1.32168 0.682874 2.28205 4.92389 0 2.82101 3.35863 2.07517 1.97171 0.546712 4.49752 1.54603 3.36812 3.85431 2.55028 3.98718 1.19031 4.92597 1.55528 1.45135 4.34231 1.23276 3.83292 2.64732 2.70187 1.90008 6.63798 1.92458 2.88178 5.17214 6.02984 3.77455 1.61948 1.50347 2.89778 1.03841 2.49973 3.8895 2.49353 2.09184 1.40333 4.03082 1.34544 3.76726 1.77201 1.88279 1.86645 5.50498 2.57743 1.53778 4.57749 5.38806 3.27233 3.44604 3.15825 3.57089 2.94654 1.56054 1.26148 3.54549 3.15928 1.78349 3.85246 0.795438 1.77236 1.16855 2.65504 4.25182 2.55354 3.85623 0.690173 3.39622 1.0952 2.55417 5.34004 1.38999 0.966478 0.948368 2.91865 1.3854 1.6906 2.45553 3.00607 2.80363 3.87988 0.97395 3.30507 2.21983 1.69578 3.20112 2.40478 6.36915 3.30052 1.09606 3.38313 2.40739 1.83175 4.26077 2.31394 2.94501 3.3893 1.52118 3.75587 2.20538 3.2102 1.35345 1.9762 3.82476 3.12599 1.6072 0.803394 5.02261 2.87448 1.83534 2.7748 1.49079 4.36629 2.31088 0 3.01809 1.79961 3.63937 3.79667 2.06792 3.87329 1.67714 3.0981 1.28666 2.36047 1.87786 2.64658 1.89122 2.45075 5.36975 2.70826 3.49736 1.08616 1.74382 2.5493 3.07416 3.99128 1.07832 2.57339 2.65526 2.34651 1.90277 1.01971 4.56393 1.19205 1.42358 2.92646 4.43406 1.91178 1.26 1.00555 4.28172 0.708932 0.860492 1.37117 2.61745 1.39307 1.39013 1.9043 1.44155 2.51721 3.55743 3.1251 2.01349 1.81978 2.23256 2.14044 1.57028 ]
@@@ Frame-accuracy per-class: [ 80.2754 72.1311 58.8235 72.956 74.7253 15.3846 69.3878 80 39.7163 82.0862 35.5556 38.3562 84.058 32.9114 0 16.8224 34.4828 41.4938 22.2222 92.6829 43.4783 61.6541 5.19481 56.338 76.5714 48.9796 72.381 0 55.1724 62.8571 10.4046 0 46.1538 12.3077 90.9091 87.4172 30.9859 38.0952 0 70.4564 53.5492 31.1111 48.8189 61.2546 53.3333 26.2295 19.8758 2.03046 69.6629 75.8621 0 26.6667 30.0752 32.7273 65.0602 58.2524 52.093 84.4037 60.2452 54.321 14.433 48.9796 75.8509 61.1236 4.65116 36.6197 48.4848 23.8806 79.6781 58.0645 53.211 26.2626 50.8961 55.2147 82.7586 88.8889 37.3626 92.9577 37.8947 71.6981 45.7143 33.0827 0 75.8621 35.9551 27.451 10.8108 34.3434 60.9865 8.69565 44.4444 86.8217 41.3793 26.087 43.609 3.63636 70.7692 56.7164 48.4076 82.3529 12.9032 46.3768 0 58.3026 48 33.8164 47.619 20.155 37.3333 69.7674 35.5556 0 24.5989 66.1871 51.0638 0 46.3768 9.52381 46.1538 40 39.2157 28.9855 5.40541 51.2821 73.6842 41.3793 21.5054 22.2222 88.8889 60.3774 70.0565 39.2157 62.2222 34.4828 20.8955 70.4 56 76.1421 46.1538 27.451 4.44444 63.6943 34.7826 43.2 4.08163 96.2963 49.7175 21.3333 13.1868 65.5738 67.234 33.8028 42.9752 25.974 64 93.4363 54.5455 12.766 55.814 31.6539 54.2373 44.898 38.0952 0 63.4921 11.9403 23.1884 60.6061 74.2857 13.4831 38.7597 66.6667 40 19.6078 0 73.6842 62.2951 52.1739 32.9412 33.8028 22.2222 0 0 70.5882 73.8462 43.0769 63.1579 24.3902 12.3077 16.4948 89.2308 28.2353 49.5575 49.3151 45.0704 57.8313 29.0909 30.303 18.1818 91.5254 28.169 25.3968 64 26.6667 10.5263 65.9341 21.8182 69.9387 27.907 6.06061 46.1538 45.5172 77.9661 0 49.4624 66.6667 27.7372 45.3333 22.6415 77.6119 25.7426 45.3333 55.3846 63.1579 36.3636 26.4151 8.51064 0 82.7586 25.6809 51.4286 56.3107 52.1008 29.2683 29.2683 23.5294 62.585 0 5.71429 61.5385 8.88889 36.3636 0 75.9494 26.4463 0 66.6667 13.3333 35.8974 19.5122 23.5294 26.6667 13.3333 71.1111 40 58.0645 55.3846 64 22.7848 81.4815 64.8276 66.6667 30.2521 45.7143 40.2878 98.5915 35.2941 34.7826 96.2963 86.4865 30.7692 60.3774 16.3265 15.8996 24.2424 0 94.7368 0 59.2593 53.5211 70.2703 41.9753 51.4286 22.7545 76.1905 34.1463 82.9268 67.6923 0 66.6667 44.7761 51.6746 65.9794 90.9091 26.4151 29.4737 53.6585 20.6897 25.4545 0 47.7876 26.6667 48.7805 26.087 85.213 72 0 0 41.8605 0 58.0645 36.3636 68.3544 76.9231 0 34.7826 80 66.6667 56.8421 0 0 48.2759 0 57.8512 0 22.9885 22.6415 0 35.2941 57.1429 39.7163 22.2222 70.9677 40.678 78.2609 50.9363 34.7826 36.6197 35.5556 34.9515 69.7674 46.9565 82.7586 65.7534 6.45161 30.137 30.5085 0 46.5116 43.9024 37.8378 56.4103 70.7424 12.6126 43.1655 53.5211 27.907 23.2558 24.8062 17.9775 53.1646 20.6897 38.7097 23.2558 48 9.52381 64.5161 62.4113 43.0769 31.3725 9.90099 35.0515 57.1429 64 83.0769 72 69.5652 74.7664 34.6883 53.7815 50.7463 32.6531 80 84.8485 50.7042 63.1579 61.6352 27.5862 54.9451 0 39.0244 48.4848 19.1781 21.5054 0 55.2381 35.0877 0 48 44.4444 0 61.0169 69.1358 85.9813 14.8148 92.6829 52.6316 36.7347 52.6316 18.677 4.60829 32.5581 66.6667 33.7079 49.1803 73.8462 80 81.7204 6.45161 52.3077 44.1989 73.0159 82.9268 83.0769 11.4286 51.1628 82.1053 68.1319 0 69.7674 46.8677 46.4646 27.451 32.7869 31.746 50.8475 53.1469 22.2222 31.3043 59.7938 36.5482 55.914 25.4545 38.5542 0 76.0331 0 0 46.7532 20.6186 55.4455 18.1818 23.622 37.7622 0 0 94.1176 42.1053 78.2609 36.3636 91.7647 47.191 22.2222 72.7273 47.2727 32.2581 38.3838 75.5556 62.2222 26.087 55.7377 66.6667 14.4578 69.0265 19.3548 56.1404 77.551 45.0704 26.6667 20.6897 0 69.5652 40 11.7647 15.5844 0 52.6316 28.0702 69.5652 41.5842 48 22.2222 27.8481 3.50877 56.7901 50.3684 67.6056 0 0 20.5128 54.0541 65.4545 8 0 70.4762 42.9752 51.1628 77.5758 0 56.7164 0 40.2235 61.5385 51.9481 30.5882 54.0541 33.0097 35.443 7.54717 17.3913 5.12821 26.6667 25.5319 7.01754 21.6216 0 35.533 22.5352 10.6667 76.1905 20.2899 52.1739 7.07965 17.5503 53.3333 50.6329 31.746 59.4595 41.8605 19.3548 44.898 0 38.7097 47.0588 15.3846 55.814 78.5047 47.0588 57.1429 27.451 12.9032 66.6667 60.6061 80.7018 6.89655 63.1579 47.0588 28.5714 47.0588 37.2881 0 0 35.2941 54.0541 71.8447 47.619 82.4742 46.8085 0 3.30579 18.1818 48.9796 19.7183 64.5161 12.1212 1.90476 24 64.8276 28.5714 5.19481 13.4454 58.0645 54.5455 4.93827 27.5862 64.2202 28.5714 27.6923 60.1942 66.6667 68.9655 41.8605 66.1333 49.3827 26.8657 42.4242 64.5161 43.2432 0 68.5714 27.6923 17.5439 10.6667 20.339 67.4699 38.0952 28.5714 30.7692 85.8639 37.8378 40.8602 74.7664 11.4286 94.1176 54.5455 27.027 12.6984 64.5161 20.6897 3.63636 40 70.5882 45.7143 40 29.9465 66.6667 62.069 31.0078 53.3333 32.2581 0 20.6186 64.1975 20.339 76.9231 51.6129 47.619 41.5094 31.1688 15.1351 14.8148 18.3486 77.5258 0 46.1538 0 40 59.4595 30.9051 47.2727 0 32.0988 30.7692 85.2174 58.5366 18.1818 32.4324 3.50877 0 62.8571 8.95522 41.1215 47.0588 30.6748 41.9162 7.54717 0 60.274 25.2252 65.1163 0 46.6165 63.9053 23.1579 0 14.8148 14.2077 2.29885 25.641 0 34.0426 21.3333 62.8571 15.3846 14.5455 32.6531 0 40.3361 37.2881 26.4151 45.1613 43.9024 0.896861 60.6061 35.6436 26.6667 80 4.44444 18.1818 40 51.6129 17.5439 49.3506 53.3333 21.8182 42.1053 47.4227 11.4286 62.6506 17.5439 0 55.8952 80.5687 74.6667 78.2222 75.8621 22.2222 8.63309 74.2857 12.5714 35.6021 0 50.5495 96.7742 8.88889 20.4082 32.6531 19.8473 57.1429 61.8182 54.2373 55.7823 51.9685 21.0526 80 52.1739 10.8108 29.9065 57.1429 45.3333 45.7143 29.2683 61.5385 6.89655 41.0959 45.8599 78.0488 0 55.1724 51.0638 54.7368 46.1538 61.5385 53.3333 0 34.7826 4.46927 0 18.1818 32.4324 68.2927 78.4983 23.5294 11.7647 27.451 11.236 44.0678 24.5614 57.7778 11.3208 63.7681 40.8163 10.8844 0 43.2749 56.4706 26.6667 22.5352 35.2201 24.6914 27.8481 75.3623 46.1538 34.0426 0 57.4163 79.3893 31.5789 37.3626 61.5385 27.907 0 65.9794 18.8235 23.2558 24.6914 45.614 50.9804 25.4545 78.9116 4.25532 0 0 61.6622 73.6402 17.7215 49.7238 43.6364 68.0851 17.0213 46.7532 2.06186 0 49.1228 45.1128 27.972 30.7692 7.40741 8.16327 13.3333 50.9804 15.3846 54.5455 51.0638 16.3265 25.4826 7.92079 49.2754 93.0233 34.5324 46.1538 13.7931 33.9623 0 13.7931 66.6667 0 12.9032 92.9577 93.4132 8.16327 46.5116 61.7886 56.1404 9.33852 30.5882 22.5564 51.2821 16 31.3725 7.01754 22.6415 9.91736 15.0943 54.7945 55.7377 54.4747 42.9752 47.0588 51.6129 40.8163 36.9748 33.6842 0 15.3846 59.8425 65.8824 31.1111 0 20.339 26.2295 26.2626 68.7023 32 47.0588 9.7561 36.9231 67.1329 76.2887 52.3077 80 75.6303 70.1754 4.08163 0 39.0244 0 64.1509 57.1429 12.9032 59.1549 0 45.7143 67.6056 22.2222 44.0678 6.89655 48.4211 40.404 26.6667 22.2222 15.3846 22.8571 34.9206 30.7692 41.5584 64.5161 67.2897 73.1707 43.4783 1.45985 48 25.5319 0 57.971 30.5085 66.6667 38.7097 19.4595 52.459 51.5464 38.0952 0 42.623 24.2424 49.3827 61.2245 20.2247 30.2158 18.3908 76.9231 27.4809 58.0645 33.8983 0 35.0877 23.0483 36.7347 24.3902 34.7826 40 18.4971 40.678 44.4444 27.907 32 28.5714 24.4898 16 23.3129 57.4924 32 0 76.9231 42.1053 55.6962 43.956 22.2222 59.5745 27.3973 27.4809 17.3913 30.3797 6.1674 22.9508 60.8696 0 22.3108 87.9121 32.381 28.9855 64 24.3902 0 30.303 16.4103 30.5882 18.4615 18.8679 39.5604 81.0127 58.0645 25.5708 54.2056 43.2432 42.0168 41.9048 20.8955 50.8475 43.8095 44.4444 63.7363 31.3725 48.3516 8.79121 17.3913 36.7347 20.8955 47.1545 37.7358 31.5789 54.2373 38.4977 52.459 11.9403 75.6757 49.4845 7.07965 82.0809 15.1351 29.7872 53.3333 38.2979 48.2759 80 27.5862 16.4384 36.9408 15.1625 34.7107 23.4965 70.5882 36.2934 46.6019 48.1013 33.8983 0 39.2523 12.9032 0 0 87.6404 15.5844 26.506 50.6024 47.0588 21.6216 33.8462 42.4242 34.1463 18.6047 45.7143 11.7647 15.544 66.6667 59.5745 42.8115 15.3846 37.1585 0 61.5385 0 28.866 19.6078 21.978 40.678 0 57.7778 25.5319 92.0635 51.7647 0 53.4005 38.0952 27.5862 30.5085 35.2941 48.1013 37.2093 75.9494 26.087 79.2453 38.5965 17.7215 55.9585 14.8148 20.5607 0 75.3623 31.5789 8.69565 23.2558 31.1111 14.0351 8.16327 55.4455 66.6667 9.52381 0 66.6667 17.0213 32.2581 37.037 29.5082 62.2951 44.1926 29.3333 95.858 54.6584 20.8835 9.30233 18.7919 24.6154 25.7669 45.977 29.7872 50.9804 32.1168 60.9524 86.2275 29.0909 14.6341 0 23.8806 8.84956 41.3793 29.7872 95.082 0 49.8113 33.4802 0 47.5524 5.40541 58.0645 6.06061 18.1818 51.9481 0 61.5385 14.1844 25.3521 35.9551 49.3151 6.06061 32 15.3846 11.7647 15.9292 0 44.898 69.9088 29.1317 73.4694 23.5294 16.3265 20.5128 43.4783 60.241 13.3333 65.7778 0 58.1818 51.2 40.5063 0 30.1887 61.8182 0 0 30.3797 19.6078 25.8065 13.2231 43.9024 53.8776 62.3853 0 32.6531 40.7767 28.0702 77.551 29.6296 66.482 46.5574 12.6582 23.5294 20.6897 89.4118 37.247 73.7589 44.3941 6.89655 67.6056 60.274 70.7403 17.7778 68.6567 51.8519 36.1446 15.3846 14.433 21.6867 65.0155 12.1212 21.6216 55.2036 21.2389 43.8356 8 30.0752 77.6119 33.6842 23.2558 45.1613 0 24.8521 1.7094 12.3711 44.2105 0 22.2222 13.7931 57.7778 25.3521 10.7884 26.3959 58.5366 72.2689 8.74317 0 32.2581 26.6667 64.5161 13.4454 24.2424 0 7.27273 63.4921 37.037 8.42105 42.5197 7.19424 42.4242 2.91971 64.6154 53.1646 51.1137 32.2581 46.8468 30.5085 3.50877 27.3684 6.89655 65.8824 47.0588 27.5862 28.5714 7.01754 68.7783 29.7872 42.4242 29.0909 40 65.7534 15.6863 80.5195 65.1163 8.69565 0 34.6667 61.7284 62.7451 13.3333 83.7209 73.1183 58.7156 23.7288 56.1798 52.5253 18.1818 64.5161 37.037 10.3093 0 52.1739 48 36.6197 24 57.971 ]

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:347) Done 295 files, 0 with no tgt_mats, 0 with other errors. [CROSS-VALIDATION, 0.0213376 min, fps61872]
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:353) AvgLoss: 2.02365 (Xent), [AvgXent: 2.02365, AvgTargetEnt: 0]
progress: []
FRAME_ACCURACY >> 47.1078% <<

WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Close():kaldi-io.cc:515) Pipe ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- | had nonzero return status 36096
