speech-HP-Z2-Tower-G9
nnet-train-multistream-perutt --cross-validate=false --randomize=true --verbose=0 --num-streams=10 --max-frames=15000 --learn-rate=0.00004 --momentum=0.9 --l1-penalty=0 --l2-penalty=0 --feature-transform=exp_FG/blstm4i/final.feature_transform 'ark:copy-feats scp:exp_FG/blstm4i/train.scp ark:- | apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_tr90/utt2spk scp:data-fbank/train_tr90/cmvn.scp ark:- ark:- | add-deltas --delta-order=2 ark:- ark:- |' 'ark:ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- |' exp_FG/blstm4i/nnet/nnet_iter05_learnrate0.00004_tr0.9972_cv2.0506 exp_FG/blstm4i/nnet/nnet_iter06 
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:243) Not in compute-exclusive mode.  Suggestion: use 'nvidia-smi -c 3' to set compute exclusive mode
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:438) Selecting from 1 GPUs
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:453) cudaSetDevice(0): NVIDIA RTX A2000 12GB	free:11620M, used:410M, total:12031M, free/total:0.96589
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:501) Device: 0, mem_ratio: 0.96589
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:382) Trying to select device: 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:511) Success selecting device 0 free mem ratio: 0.96589
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:FinalizeActiveGpu():cu-device.cc:338) The active GPU is [0]: NVIDIA RTX A2000 12GB	free:11106M, used:924M, total:12031M, free/total:0.923168 version 8.6
copy-feats scp:exp_FG/blstm4i/train.scp ark:- 
apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_tr90/utt2spk scp:data-fbank/train_tr90/cmvn.scp ark:- ark:- 
add-deltas --delta-order=2 ark:- ark:- 
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:144) TRAINING STARTED
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Read():nnet/nnet-matrix-buffer.h:191) Read() started... Buffer size in MB: 0, max 3072, having 0 utterances.
ali-to-post ark:- ark:- 
ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl 'ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |' ark:- 
LOG (copy-feats[5.5.1074~1-71f3]:main():copy-feats.cc:143) Copied 2624 feature matrices.
LOG (apply-cmvn[5.5.1074~1-71f3]:main():apply-cmvn.cc:159) Applied cepstral mean and variance normalization to 2624 utterances, errors on 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:303) ### After 0 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:304) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.534763, max 0.575347, mean 0.00392206, stddev 0.0828449, skewness 0.012927, kurtosis 0.451838 ) 
  f_w_gifo_r_   ( min -0.396592, max 0.402788, mean -0.000584725, stddev 0.0776611, skewness -0.00136828, kurtosis 0.00727057 ) 
  f_bias_   ( min -0.351968, max 1.33482, mean 0.213049, stddev 0.460779, skewness 1.06586, kurtosis -0.655121 ) 
  f_peephole_i_c_   ( min -0.480793, max 0.485823, mean -0.00317254, stddev 0.126914, skewness 0.119986, kurtosis 1.2403 ) 
  f_peephole_f_c_   ( min -0.697312, max 0.899327, mean 0.00411426, stddev 0.167504, skewness 0.43465, kurtosis 5.04626 ) 
  f_peephole_o_c_   ( min -0.500173, max 0.484067, mean -0.010768, stddev 0.182827, skewness 0.172296, kurtosis -0.275 ) 
  f_w_r_m_   ( min -0.543891, max 0.502568, mean 0.000607769, stddev 0.101406, skewness 0.0022914, kurtosis -0.00444698 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -1.27263, max 0.912504, mean 0.00641589, stddev 0.0886813, skewness -0.0993363, kurtosis 2.71006 ) 
  b_w_gifo_r_   ( min -0.361618, max 0.308054, mean -0.000209816, stddev 0.0702767, skewness 7.55174e-05, kurtosis -0.310435 ) 
  b_bias_   ( min -0.342572, max 1.1919, mean 0.206852, stddev 0.451396, skewness 1.04731, kurtosis -0.689107 ) 
  b_peephole_i_c_   ( min -0.351851, max 0.292614, mean 0.00523755, stddev 0.0943602, skewness -0.110591, kurtosis 0.931311 ) 
  b_peephole_f_c_   ( min -0.668339, max 0.697072, mean 0.0114227, stddev 0.163952, skewness 0.461958, kurtosis 3.85335 ) 
  b_peephole_o_c_   ( min -0.549329, max 0.494342, mean -0.0164604, stddev 0.187329, skewness -0.131879, kurtosis 0.208595 ) 
  b_w_r_m_   ( min -0.405415, max 0.363696, mean -0.000254988, stddev 0.089983, skewness -0.000286319, kurtosis -0.075243 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.923818, max 0.710799, mean -0.000155918, stddev 0.106477, skewness 0.0060584, kurtosis 0.0641959 ) , lr-coef 1, max-norm 0
  bias ( min -0.0827186, max 2.32366, mean -6.98492e-10, stddev 0.0763503, skewness 23.1473, kurtosis 676.469 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:305) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -8.67671, max 9.02116, mean 0.0321104, stddev 1.01793, skewness 0.450612, kurtosis 3.20408 ) 
[1] output of <BlstmProjected> ( min -4.89528, max 4.35098, mean 0.00104339, stddev 0.778031, skewness -0.00543792, kurtosis 1.32592 ) 
[2] output of <Tanh> ( min -0.999888, max 0.999668, mean 0.00102785, stddev 0.5117, skewness -0.00390292, kurtosis -0.744461 ) 
[3] output of <AffineTransform> ( min -17.9658, max 20.3016, mean 0.00695033, stddev 2.37226, skewness 0.799815, kurtosis 3.07978 ) 
[4] output of <Softmax> ( min 3.24957e-15, max 0.999838, mean 0.000779848, stddev 0.019734, skewness 37.2849, kurtosis 1539.82 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:307) 
### BACKWARD PROPAGATION BUFFER CONTENT :
[0] diff of <Input>  ( min -2.92168, max 3.53019, mean -0.00497054, stddev 0.234888, skewness -0.369292, kurtosis 11.822 ) 
[1] diff-output of <BlstmProjected> ( min -0.666108, max 0.625086, mean -0.000222937, stddev 0.0498866, skewness -0.0213322, kurtosis 6.02125 ) 
[2] diff-output of <Tanh> ( min -0.92952, max 0.820105, mean -0.000270116, stddev 0.0660375, skewness -0.000366518, kurtosis 3.80625 ) 
[3] diff-output of <AffineTransform> ( min -0.999998, max 0.959509, mean -2.81276e-07, stddev 0.0188436, skewness -23.8977, kurtosis 1582.98 ) 
[4] diff-output of <Softmax> ( min -0.999998, max 0.959509, mean -2.81276e-07, stddev 0.0188436, skewness -23.8977, kurtosis 1582.98 ) 
### END BACKWARD


LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:308) 
### GRADIENT STATS :
Component 1 : <BlstmProjected>, ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  ### Gradients 
  f_w_gifo_x_corr_   ( min -40.6584, max 45.7406, mean 0.109984, stddev 3.59945, skewness -0.136505, kurtosis 13.1911 ) 
  f_w_gifo_r_corr_   ( min -50.8581, max 36.8222, mean 0.00363516, stddev 2.6382, skewness -0.0810816, kurtosis 11.1139 ) 
  f_bias_corr_   ( min -31.0779, max 39.2773, mean 0.235876, stddev 4.50319, skewness 0.42093, kurtosis 10.692 ) 
  f_peephole_i_c_corr_   ( min -54.2301, max 49.9828, mean 0.00908406, stddev 6.34079, skewness 0.575557, kurtosis 32.9579 ) 
  f_peephole_f_c_corr_   ( min -94.8452, max 70.0331, mean -0.360872, stddev 13.9005, skewness -0.584912, kurtosis 14.0664 ) 
  f_peephole_o_c_corr_   ( min -116.755, max 250, mean 0.132556, stddev 21.0989, skewness 5.16596, kurtosis 66.3078 ) 
  f_w_r_m_corr_   ( min -38.0243, max 34.6016, mean -0.000740497, stddev 4.12438, skewness -0.00892407, kurtosis 4.19974 ) 
  ---
  b_w_gifo_x_corr_   ( min -43.4362, max 48.4248, mean 0.0737548, stddev 3.56068, skewness -0.110236, kurtosis 14.8012 ) 
  b_w_gifo_r_corr_   ( min -36.4076, max 30.9907, mean -0.00799577, stddev 2.62397, skewness -0.00506974, kurtosis 8.69813 ) 
  b_bias_corr_   ( min -33.545, max 41.4055, mean -0.24968, stddev 5.64172, skewness 1.09204, kurtosis 13.1255 ) 
  b_peephole_i_c_corr_   ( min -55.0008, max 20.1398, mean -0.439895, stddev 5.5252, skewness -5.77441, kurtosis 54.9593 ) 
  b_peephole_f_c_corr_   ( min -50.361, max 189.167, mean 0.760625, stddev 13.3248, skewness 8.67632, kurtosis 123.281 ) 
  b_peephole_o_c_corr_   ( min -49.5068, max 54.3371, mean 0.362343, stddev 10.3799, skewness -0.217537, kurtosis 4.63739 ) 
  b_w_r_m_corr_   ( min -29.8725, max 28.5273, mean -0.0133127, stddev 3.81953, skewness 0.00270123, kurtosis 1.13963 ) 

  ### Activations (mostly after non-linearities)
  YI_FW(0..1)^   ( min 0, max 1, mean 0.43113, stddev 0.344822, skewness 0.355988, kurtosis -1.29985 ) 
  YF_FW(0..1)^   ( min 0, max 1, mean 0.640996, stddev 0.316768, skewness -0.533647, kurtosis -0.979095 ) 
  YO_FW(0..1)^   ( min 0, max 1, mean 0.36934, stddev 0.352742, skewness 0.630504, kurtosis -1.1329 ) 
  YG_FW(-1..1)   ( min -1, max 1, mean 0.0175362, stddev 0.866243, skewness -0.0332828, kurtosis -1.79728 ) 
  YC_FW(-R..R)*  ( min -50, max 50, mean 0.295936, stddev 13.0571, skewness 0.0966444, kurtosis 9.73816 ) 
  YH_FW(-1..1)   ( min -1, max 1, mean 0.0339954, stddev 0.685663, skewness -0.0602014, kurtosis -1.24293 ) 
  YM_FW(-1..1)   ( min -1, max 1, mean 0.00461688, stddev 0.331397, skewness -0.0638222, kurtosis 2.76816 ) 
  YR_FW(-R..R)   ( min -4.18458, max 4.35098, mean 0.0194834, stddev 0.760007, skewness 0.0476949, kurtosis 1.26303 ) 
  ---
  YI_BW(0..1)^   ( min 0, max 1, mean 0.462084, stddev 0.333138, skewness 0.228646, kurtosis -1.35743 ) 
  YF_BW(0..1)^   ( min 0, max 1, mean 0.646459, stddev 0.283896, skewness -0.55766, kurtosis -0.69306 ) 
  YO_BW(0..1)^   ( min 0, max 1, mean 0.387538, stddev 0.356104, skewness 0.532909, kurtosis -1.25979 ) 
  YG_BW(-1..1)   ( min -1, max 1, mean 0.0104232, stddev 0.855117, skewness -0.0197855, kurtosis -1.78011 ) 
  YC_BW(-R..R)*  ( min -50, max 50, mean 0.945938, stddev 11.1481, skewness 0.86309, kurtosis 12.4668 ) 
  YH_BW(-1..1)   ( min -1, max 1, mean 0.0382341, stddev 0.695687, skewness -0.0526068, kurtosis -1.30107 ) 
  YM_BW(-1..1)   ( min -0.999993, max 0.999999, mean 0.00495115, stddev 0.340965, skewness 0.0251278, kurtosis 2.16076 ) 
  YR_BW(-R..R)   ( min -4.89528, max 4.26648, mean -0.0174069, stddev 0.791602, skewness -0.046975, kurtosis 1.39783 ) 

  ### Derivatives (w.r.t. inputs of non-linearities)
  DI_FW^  ( min -1, max 1, mean 1.01683e-05, stddev 0.0248403, skewness -0.642375, kurtosis 316.147 ) 
  DF_FW^  ( min -1, max 1, mean 0.000137354, stddev 0.0203823, skewness 0.842369, kurtosis 503.587 ) 
  DO_FW^  ( min -0.977345, max 0.843432, mean 7.10067e-05, stddev 0.0262741, skewness 0.172716, kurtosis 67.811 ) 
  DG_FW   ( min -1, max 1, mean 2.2157e-05, stddev 0.0320425, skewness -0.853238, kurtosis 360.583 ) 
  DC_FW*  ( min -42.8083, max 63.8303, mean -1.72375e-05, stddev 0.587836, skewness 13.0098, kurtosis 3345.93 ) 
  DH_FW   ( min -3.37833, max 3.69988, mean 0.000412816, stddev 0.120024, skewness 0.266896, kurtosis 50.4093 ) 
  DM_FW   ( min -5.15322, max 4.65334, mean 0.000616087, stddev 0.3078, skewness -0.0128506, kurtosis 10.5285 ) 
  DR_FW   ( min -1.17941, max 1.28165, mean 3.90357e-05, stddev 0.0882069, skewness 0.0312538, kurtosis 7.30607 ) 
  ---
  DI_BW^  ( min -1, max 1, mean -0.000178379, stddev 0.0217017, skewness 0.432859, kurtosis 229.019 ) 
  DF_BW^  ( min -1, max 1, mean -3.80704e-05, stddev 0.0160779, skewness 2.88569, kurtosis 351.664 ) 
  DO_BW^  ( min -0.822103, max 0.871536, mean -0.000211808, stddev 0.0206713, skewness 0.132789, kurtosis 52.0695 ) 
  DG_BW   ( min -1, max 1, mean 0.000173483, stddev 0.0349809, skewness 0.670455, kurtosis 224.666 ) 
  DC_BW*  ( min -15.1703, max 14.2862, mean -0.00171619, stddev 0.251865, skewness -5.12812, kurtosis 380.079 ) 
  DH_BW   ( min -2.74116, max 2.76277, mean 2.9011e-06, stddev 0.100006, skewness -0.0760501, kurtosis 43.2781 ) 
  DM_BW   ( min -4.99274, max 3.91949, mean 0.000798498, stddev 0.250375, skewness -0.0473938, kurtosis 8.18119 ) 
  DR_BW   ( min -0.958088, max 0.942822, mean -0.000295636, stddev 0.0864366, skewness 0.00278719, kurtosis 5.29233 ) 
Component 2 : <Tanh>, 
Component 3 : <AffineTransform>, 
  linearity_grad ( min -85.9222, max 89.1723, mean -3.15914e-08, stddev 1.58413, skewness -0.121074, kurtosis 236.669 ) , lr-coef 1, max-norm 0
  bias_grad ( min -185.171, max 152.801, mean -1.13249e-07, stddev 7.18774, skewness -5.88182, kurtosis 501.484 ) , lr-coef 1
Component 4 : <Softmax>, 
### END GRADIENT

LOG (ali-to-pdf[5.5.1074~1-71f3]:main():ali-to-pdf.cc:68) Converted 2919 alignments to pdf sequences.
LOG (ali-to-post[5.5.1074~1-71f3]:main():ali-to-post.cc:73) Converted 2919 alignments.
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:168) MC05_98, missing targets
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:334) ### After 755062 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:335) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.616411, max 0.65931, mean 0.00393689, stddev 0.085431, skewness 0.0131734, kurtosis 0.689007 ) 
  f_w_gifo_r_   ( min -0.423845, max 0.409684, mean -0.000584791, stddev 0.0781972, skewness -0.00163398, kurtosis 0.0165687 ) 
  f_bias_   ( min -0.347334, max 1.37568, mean 0.212595, stddev 0.462292, skewness 1.06384, kurtosis -0.652125 ) 
  f_peephole_i_c_   ( min -0.509369, max 0.501211, mean -0.00289568, stddev 0.129123, skewness 0.0847997, kurtosis 1.54164 ) 
  f_peephole_f_c_   ( min -0.677315, max 0.897532, mean 0.00410478, stddev 0.174811, skewness 0.249118, kurtosis 4.62533 ) 
  f_peephole_o_c_   ( min -0.506917, max 0.4708, mean -0.0109365, stddev 0.188117, skewness 0.197994, kurtosis -0.255551 ) 
  f_w_r_m_   ( min -0.515665, max 0.492348, mean 0.000606092, stddev 0.103025, skewness 0.00139011, kurtosis 0.0110598 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -1.52948, max 1.0024, mean 0.00620802, stddev 0.0921309, skewness -0.148393, kurtosis 3.98057 ) 
  b_w_gifo_r_   ( min -0.365011, max 0.343452, mean -0.000185902, stddev 0.071249, skewness 0.000138502, kurtosis -0.273844 ) 
  b_bias_   ( min -0.362876, max 1.21022, mean 0.206201, stddev 0.452877, skewness 1.04238, kurtosis -0.689403 ) 
  b_peephole_i_c_   ( min -0.379005, max 0.296419, mean 0.00613369, stddev 0.0979482, skewness -0.0601382, kurtosis 1.0279 ) 
  b_peephole_f_c_   ( min -0.632696, max 0.725181, mean 0.0133486, stddev 0.171829, skewness 0.623662, kurtosis 3.69561 ) 
  b_peephole_o_c_   ( min -0.561635, max 0.501367, mean -0.0161429, stddev 0.193552, skewness -0.16468, kurtosis 0.301024 ) 
  b_w_r_m_   ( min -0.397783, max 0.377919, mean -0.000271505, stddev 0.0920643, skewness 0.000596733, kurtosis -0.0551715 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.955387, max 0.750977, mean -0.000155917, stddev 0.107488, skewness 0.00584201, kurtosis 0.0635016 ) , lr-coef 1, max-norm 0
  bias ( min -0.0882712, max 2.44244, mean -7.21775e-10, stddev 0.0801137, skewness 23.2236, kurtosis 680.523 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:336) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -14.8312, max 13.8231, mean 0.00769689, stddev 0.940803, skewness 0.731578, kurtosis 12.6034 ) 
[1] output of <BlstmProjected> ( min -3.88653, max 4.03108, mean 0.000599713, stddev 0.653748, skewness -0.00947197, kurtosis 3.11981 ) 
[2] output of <Tanh> ( min -0.999159, max 0.99937, mean 0.000435677, stddev 0.431252, skewness 0.00313573, kurtosis 0.145318 ) 
[3] output of <AffineTransform> ( min -12.5466, max 19.8712, mean 0.00637671, stddev 2.04133, skewness 0.987172, kurtosis 5.6 ) 
[4] output of <Softmax> ( min 1.14383e-12, max 0.999923, mean 0.000781018, stddev 0.0182432, skewness 43.1427, kurtosis 2010.76 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:338) 
### BACKWARD PROPAGATION BUFFER CONTENT :
[0] diff of <Input>  ( min -4.59246, max 2.9568, mean -0.000576303, stddev 0.231197, skewness -1.14541, kurtosis 26.8716 ) 
[1] diff-output of <BlstmProjected> ( min -0.480278, max 0.475551, mean -2.4094e-05, stddev 0.0410364, skewness -0.0146693, kurtosis 12.4042 ) 
[2] diff-output of <Tanh> ( min -0.505985, max 0.504454, mean 1.2905e-05, stddev 0.0544582, skewness 0.0109626, kurtosis 7.9417 ) 
[3] diff-output of <AffineTransform> ( min -0.998715, max 0.946817, mean -1.00237e-08, stddev 0.015041, skewness -27.1152, kurtosis 2592.74 ) 
[4] diff-output of <Softmax> ( min -0.998715, max 0.946817, mean -1.00237e-08, stddev 0.015041, skewness -27.1152, kurtosis 2592.74 ) 
### END BACKWARD


LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:339) 
### GRADIENT STATS :
Component 1 : <BlstmProjected>, ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  ### Gradients 
  f_w_gifo_x_corr_   ( min -42.1431, max 45.0691, mean 0.0146712, stddev 3.54136, skewness 0.104735, kurtosis 8.81843 ) 
  f_w_gifo_r_corr_   ( min -46.9778, max 46.8614, mean 0.00198923, stddev 3.10112, skewness 0.0155009, kurtosis 9.29216 ) 
  f_bias_corr_   ( min -41.6011, max 31.7899, mean -0.48078, stddev 4.40695, skewness -0.695206, kurtosis 11.8243 ) 
  f_peephole_i_c_corr_   ( min -82.3817, max 52.2182, mean -0.0435023, stddev 7.87678, skewness -2.30718, kurtosis 44.3699 ) 
  f_peephole_f_c_corr_   ( min -119.608, max 70.838, mean -0.127221, stddev 15.0821, skewness -1.61216, kurtosis 17.6651 ) 
  f_peephole_o_c_corr_   ( min -137.703, max 85.9932, mean -0.462684, stddev 18.3225, skewness -0.697103, kurtosis 14.5529 ) 
  f_w_r_m_corr_   ( min -54.941, max 47.9698, mean -0.00773997, stddev 4.57105, skewness -0.0221739, kurtosis 4.55032 ) 
  ---
  b_w_gifo_x_corr_   ( min -94.0274, max 114.441, mean 0.0734851, stddev 4.27938, skewness -0.737864, kurtosis 24.6661 ) 
  b_w_gifo_r_corr_   ( min -39.3332, max 38.9926, mean 0.000131733, stddev 3.22397, skewness -0.0133804, kurtosis 5.33584 ) 
  b_bias_corr_   ( min -29.8341, max 35.769, mean -0.400924, stddev 6.03343, skewness 1.11751, kurtosis 6.58522 ) 
  b_peephole_i_c_corr_   ( min -54.8787, max 81.0722, mean 0.0220876, stddev 7.80778, skewness 2.71425, kurtosis 46.8924 ) 
  b_peephole_f_c_corr_   ( min -59.1943, max 46.3912, mean -0.920891, stddev 11.403, skewness -0.673614, kurtosis 5.09591 ) 
  b_peephole_o_c_corr_   ( min -72.4628, max 84.0465, mean 0.893004, stddev 12.9917, skewness 0.0295866, kurtosis 8.93361 ) 
  b_w_r_m_corr_   ( min -39.9474, max 37.8754, mean 0.020184, stddev 4.76545, skewness -0.00671956, kurtosis 1.02685 ) 

  ### Activations (mostly after non-linearities)
  YI_FW(0..1)^   ( min 0, max 1, mean 0.318367, stddev 0.352524, skewness 0.76673, kurtosis -0.908826 ) 
  YF_FW(0..1)^   ( min 0, max 1, mean 0.46883, stddev 0.389176, skewness 0.0359256, kurtosis -1.59086 ) 
  YO_FW(0..1)^   ( min 0, max 1, mean 0.266913, stddev 0.341694, skewness 1.07802, kurtosis -0.365865 ) 
  YG_FW(-1..1)   ( min -1, max 1, mean 0.0183218, stddev 0.750889, skewness -0.0322525, kurtosis -1.39767 ) 
  YC_FW(-R..R)*  ( min -50, max 50, mean 0.139032, stddev 10.4429, skewness 0.118285, kurtosis 16.3628 ) 
  YH_FW(-1..1)   ( min -1, max 1, mean 0.0253512, stddev 0.585571, skewness -0.0386001, kurtosis -0.605253 ) 
  YM_FW(-1..1)   ( min -1, max 1, mean 0.00442195, stddev 0.278522, skewness -0.0688043, kurtosis 5.17018 ) 
  YR_FW(-R..R)   ( min -3.88653, max 4.03108, mean 0.00635776, stddev 0.63036, skewness 0.0699938, kurtosis 3.00805 ) 
  ---
  YI_BW(0..1)^   ( min 0, max 1, mean 0.340936, stddev 0.352277, skewness 0.63132, kurtosis -1.09368 ) 
  YF_BW(0..1)^   ( min 0, max 1, mean 0.477389, stddev 0.37358, skewness -0.063678, kurtosis -1.51033 ) 
  YO_BW(0..1)^   ( min 0, max 1, mean 0.28593, stddev 0.350614, skewness 0.962655, kurtosis -0.625736 ) 
  YG_BW(-1..1)   ( min -1, max 1, mean 0.0119228, stddev 0.746153, skewness -0.0203574, kurtosis -1.3889 ) 
  YC_BW(-R..R)*  ( min -50, max 50, mean 0.756319, stddev 9.09672, skewness 1.53948, kurtosis 18.8951 ) 
  YH_BW(-1..1)   ( min -1, max 1, mean 0.0309519, stddev 0.594864, skewness -0.0191303, kurtosis -0.693894 ) 
  YM_BW(-1..1)   ( min -0.999995, max 0.999998, mean 0.00245622, stddev 0.29253, skewness -0.0201684, kurtosis 4.08457 ) 
  YR_BW(-R..R)   ( min -3.85518, max 3.82511, mean -0.0051656, stddev 0.672503, skewness -0.0717908, kurtosis 3.22252 ) 

  ### Derivatives (w.r.t. inputs of non-linearities)
  DI_FW^  ( min -1, max 1, mean -0.000151822, stddev 0.0193263, skewness -5.99662, kurtosis 525.594 ) 
  DF_FW^  ( min -1, max 1, mean 1.59559e-05, stddev 0.0148675, skewness 6.04879, kurtosis 684.117 ) 
  DO_FW^  ( min -0.659936, max 0.629132, mean -0.000143736, stddev 0.0196207, skewness 0.0164277, kurtosis 106.848 ) 
  DG_FW   ( min -1, max 1, mean 0.000113605, stddev 0.0246944, skewness 2.86409, kurtosis 645.388 ) 
  DC_FW*  ( min -16.1978, max 6.22833, mean -0.000467047, stddev 0.25608, skewness -18.7415, kurtosis 1038.68 ) 
  DH_FW   ( min -2.56964, max 2.40487, mean 9.30787e-05, stddev 0.0955082, skewness 0.416333, kurtosis 78.9748 ) 
  DM_FW   ( min -3.75897, max 4.19071, mean -5.56969e-05, stddev 0.234939, skewness 0.157204, kurtosis 19.8942 ) 
  DR_FW   ( min -0.947679, max 1.10808, mean 0.000152005, stddev 0.0686767, skewness 0.0582364, kurtosis 15.0361 ) 
  ---
  DI_BW^  ( min -1, max 1, mean -6.06144e-05, stddev 0.0247333, skewness 1.2479, kurtosis 499.713 ) 
  DF_BW^  ( min -1, max 1, mean -1.05189e-05, stddev 0.017652, skewness 0.0366117, kurtosis 1000.78 ) 
  DO_BW^  ( min -0.619265, max 0.732665, mean 0.000145688, stddev 0.0196426, skewness 0.479927, kurtosis 85.1201 ) 
  DG_BW   ( min -1, max 1, mean 0.00025262, stddev 0.0361538, skewness 0.741071, kurtosis 297.565 ) 
  DC_BW*  ( min -16.2519, max 23.4744, mean 0.00398263, stddev 0.451581, skewness 19.8347, kurtosis 1127.97 ) 
  DH_BW   ( min -3.09357, max 3.0523, mean 0.000361967, stddev 0.0980841, skewness -0.64935, kurtosis 82.2183 ) 
  DM_BW   ( min -3.23056, max 3.17353, mean 0.00215293, stddev 0.249324, skewness 0.0762712, kurtosis 12.605 ) 
  DR_BW   ( min -1.1995, max 1.38699, mean -0.00023474, stddev 0.0858198, skewness 0.0150474, kurtosis 9.58059 ) 
Component 2 : <Tanh>, 
Component 3 : <AffineTransform>, 
  linearity_grad ( min -75.4538, max 70.8253, mean -2.90922e-08, stddev 1.77757, skewness -0.104427, kurtosis 131.676 ) , lr-coef 1, max-norm 0
  bias_grad ( min -83.1139, max 66.8875, mean 5.96046e-08, stddev 4.63048, skewness -5.60358, kurtosis 172.857 ) , lr-coef 1
Component 4 : <Softmax>, 
### END GRADIENT

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:346) PER-CLASS PERFORMANCE:
@@@ Frames per-class: [ 181212 144 127 606 532 43 173 190 515 703 138 211 211 608 45 289 271 572 372 153 368 892 307 454 467 612 374 177 314 165 203 169 193 120 162 350 506 100 142 40889 89709 154 463 1777 286 231 687 352 545 459 79 394 481 237 200 663 1037 475 2210 428 257 167 4192 1910 93 230 151 223 4363 199 339 612 931 689 168 53 430 425 404 147 72 468 398 184 543 242 103 448 1376 60 779 717 668 146 541 163 358 830 561 408 249 268 103 1105 608 477 208 629 257 136 769 185 359 2082 165 69 303 74 247 336 172 147 117 128 443 612 117 125 52 316 956 165 305 574 202 297 222 1167 208 209 172 556 182 606 267 121 534 323 470 239 776 321 812 179 268 1139 537 191 554 5133 217 193 202 128 264 242 282 141 453 190 390 248 274 208 168 117 250 72 227 832 107 208 74 120 577 200 74 173 93 435 313 308 638 462 201 239 312 172 59 151 330 221 210 68 77 424 248 789 348 126 86 566 208 142 331 485 286 372 234 328 637 273 293 132 41 150 57 102 173 655 328 426 861 178 245 87 533 72 131 265 150 790 182 357 233 69 187 112 206 273 85 62 302 205 411 179 278 260 193 525 723 141 288 139 513 788 215 97 150 236 273 455 166 615 208 63 196 43 120 342 911 206 87 1156 192 146 131 166 31 221 252 417 373 499 401 233 179 409 182 439 579 96 189 251 1812 272 55 134 245 227 120 170 454 69 211 126 232 303 701 89 59 87 90 418 122 334 282 91 119 125 379 59 189 157 163 620 120 203 224 199 71 381 86 544 80 247 220 94 114 76 256 175 1186 420 378 336 135 140 331 666 306 356 196 203 291 152 221 543 214 149 360 365 670 264 264 692 216 616 923 524 408 234 395 301 375 105 525 80 359 98 456 204 309 258 288 286 181 90 101 263 53 253 167 354 150 174 194 393 579 494 437 273 142 236 159 226 115 606 198 255 508 136 282 304 327 234 1467 199 113 213 747 267 237 238 267 223 423 127 334 351 773 400 668 269 762 507 73 172 259 944 420 60 318 817 125 150 155 430 99 155 360 316 425 603 146 124 301 210 170 102 140 531 288 716 282 161 233 181 349 120 133 242 135 51 141 111 147 205 551 427 131 99 109 133 445 7274 204 271 135 128 125 377 120 372 449 808 108 557 45 280 47 596 57 276 400 121 347 315 193 77 146 96 105 216 193 341 1125 236 219 105 417 109 338 639 130 396 177 211 138 121 275 91 104 194 51 542 461 346 197 315 121 109 444 210 64 135 84 52 96 283 268 315 144 187 498 569 470 194 79 151 71 285 409 404 165 215 321 466 69 209 585 117 83 141 150 364 478 225 439 81 192 156 773 344 306 243 112 146 203 501 258 143 117 231 316 156 83 53 468 163 539 465 176 116 79 157 350 139 86 253 155 101 132 342 440 324 189 390 153 111 192 365 208 291 263 103 139 134 307 402 452 451 5943 88 406 111 189 488 786 548 86 632 99 382 216 141 365 388 65 90 187 430 300 463 604 247 79 314 305 463 45 473 718 330 223 226 1312 201 160 62 170 227 141 90 90 193 68 539 610 241 156 182 257 224 114 99 209 178 32 193 174 235 238 150 123 308 261 195 275 142 111 371 821 518 831 306 124 432 87 357 932 174 327 225 300 36 190 272 120 358 461 397 462 306 105 348 166 390 216 520 219 260 71 179 258 429 201 53 142 256 406 60 128 173 26 88 191 83 60 155 179 1000 125 44 191 230 383 687 216 257 321 233 417 202 1061 342 327 163 248 165 412 229 303 517 124 815 539 79 306 155 255 89 321 245 73 308 228 196 192 269 176 228 101 8591 469 337 966 237 430 177 258 334 85 304 414 487 401 57 178 99 260 178 151 248 261 926 126 248 230 264 241 147 220 155 139 646 219 209 393 1097 85 160 936 605 267 175 213 208 155 166 207 194 260 180 542 294 745 650 41 235 409 387 268 281 172 798 467 256 138 171 316 378 542 197 77 199 253 693 506 326 151 543 272 142 171 242 241 191 161 274 304 47 226 320 127 248 43 388 236 233 244 332 111 261 196 246 68 466 707 315 146 315 282 30 427 201 153 510 1026 278 186 316 45 464 106 316 404 415 543 346 257 452 1347 138 153 366 558 585 171 109 855 530 221 155 160 161 111 196 131 341 863 116 777 103 428 210 393 95 260 221 295 129 320 516 185 97 126 343 492 405 270 645 150 141 159 382 1153 426 252 535 332 205 417 575 597 292 372 180 236 213 343 301 154 349 528 290 243 378 616 213 266 230 676 309 338 165 324 314 639 310 148 365 184 119 116 199 264 1367 367 467 1442 213 598 641 335 156 122 351 114 129 98 489 213 162 386 116 74 274 197 265 173 129 167 753 211 214 1711 286 746 207 83 123 191 204 353 783 57 223 151 323 285 99 1197 111 116 189 130 401 488 428 96 280 248 468 506 117 284 315 328 129 334 124 557 265 243 383 268 66 97 474 153 221 139 391 194 634 156 322 748 309 195 326 651 462 370 172 385 490 392 610 304 249 70 501 324 335 228 370 206 587 560 108 850 256 209 153 168 206 131 204 232 340 200 284 174 301 124 107 220 67 370 1390 1253 311 434 127 136 104 493 416 778 103 335 338 242 66 208 459 231 96 291 160 220 368 145 728 458 351 255 350 348 137 125 1039 1466 285 221 232 361 605 549 4192 543 376 361 3105 243 319 117 225 232 542 309 859 113 244 913 235 271 853 881 252 243 274 129 90 371 274 176 453 107 172 140 172 411 267 778 147 558 2509 132 95 258 162 203 87 21 467 192 231 265 665 552 160 225 162 279 3235 297 303 235 317 297 155 692 494 82 179 192 734 178 103 204 87 225 63 351 225 260 282 352 367 219 77 182 384 386 147 340 273 260 234 61 251 141 663 194 320 94 144 ]
@@@ Loss per-class: [ 0.287195 0.878172 1.11377 0.756652 0.783889 1.37277 1.38064 0.909779 1.04478 1.39358 0.676875 1.49253 1.40983 0.921035 2.94783 0.816616 1.0587 1.38435 1.33988 1.14751 0.815181 1.0699 1.21966 0.728335 0.605157 1.13655 0.7009 2.22571 0.980662 1.64313 1.09063 0.662084 1.12886 1.38715 0.549714 0.901946 1.56577 1.68481 1.32413 1.05609 0.428232 1.12879 1.14916 0.950217 1.37562 0.96362 0.543108 1.45318 1.10733 0.803355 1.31804 1.40707 1.18693 1.49437 0.956114 1.06388 0.832553 0.647771 0.58067 1.21682 1.27399 0.978032 0.348254 0.356659 1.66325 1.42984 1.64413 1.39112 1.01665 0.683988 0.709899 0.693925 0.655855 1.11308 1.06032 1.4488 0.381999 0.280603 0.74855 1.11763 1.13711 1.20762 0.525605 0.792643 1.06732 1.29411 1.6237 1.18913 1.0936 0.921969 0.761958 0.824961 0.816652 2.17754 1.34088 1.90962 0.873094 0.563077 0.690882 0.71039 1.89827 0.948208 1.81311 0.659938 0.534621 0.732293 1.44554 1.24403 1.04955 0.857908 1.62419 1.61248 1.82309 0.547069 1.60806 3.05143 1.4144 1.27893 2.05276 1.18044 1.35541 2.41749 1.69471 1.60339 0.750394 0.62404 1.76573 1.77391 1.31256 0.705069 0.389749 1.53015 0.702547 0.884936 1.90947 0.676674 0.916788 0.437265 1.24547 1.42913 1.39592 0.8457 0.991744 0.819689 0.736583 1.05557 1.33963 1.30978 0.833794 0.642324 0.30314 1.20475 1.3954 1.59284 1.27332 0.675733 0.383019 0.663752 0.743285 0.447628 0.739731 1.31372 1.74554 3.98152 0.989148 1.4237 0.916415 1.25595 0.693378 1.5061 0.525917 1.13488 1.08512 1.23814 1.62198 0.730477 0.85906 0.906927 1.92161 0.331485 1.39493 1.61515 1.22172 1.01888 0.314428 1.28812 2.5642 1.55976 2.27896 0.872807 0.820261 1.1686 0.451845 0.850433 1.14406 1.12608 1.59196 1.61577 1.70641 0.782713 2.00717 1.87346 1.25594 2.201 2.1162 0.945467 2.01623 0.940697 1.07006 1.46058 1.64694 1.29658 0.791345 12.9097 1.52326 0.717864 1.46058 0.769617 0.733519 1.64784 0.934264 0.675579 1.67103 1.08699 1.40168 1.57684 2.42441 0.743014 0.919283 0.940524 1.29301 0.871099 0.941745 2.58854 1.00193 2.30233 0.75825 2.49502 1.60635 1.14311 1.19152 0.510227 1.14084 1.02056 1.45093 2.74294 0.932894 3.97853 1.94641 1.49434 2.95968 1.2882 1.4256 1.32586 1.45804 0.828397 2.16422 0.93813 0.829392 0.769563 0.992831 1.61347 1.30824 2.41561 1.1608 0.774504 1.74969 1.35331 1.46376 1.4893 0.975946 1.22989 2.17833 1.80727 2.47929 2.05818 1.0312 3.13584 1.04451 1.76062 1.14891 1.58841 1.39953 0.742798 1.04906 1.80336 2.16274 1.28627 7.3714 2.43425 1.60929 2.07298 0.733653 0.537468 1.98516 1.27016 1.0521 1.01027 1.40159 1.55616 0.639778 1.23028 1.0647 0.919588 0.332249 0.862456 1.72511 1.47844 1.85797 1.97949 1.30234 1.21074 1.0783 1.25034 1.70473 2.15798 0.673381 1.40012 0.486209 1.61498 2.34596 0.798366 2.36134 0.635449 1.79036 2.31749 1.69833 3.47009 1.63368 1.01115 0.977718 1.41704 1.95099 1.26279 1.4581 0.623593 2.00212 1.57062 1.43489 1.0263 1.1291 0.972736 0.682607 0.943854 1.2156 1.03802 1.24579 1.59527 1.28761 1.4048 1.28116 1.35675 0.522586 0.357829 1.28712 1.11053 1.19041 1.32031 1.79685 0.454695 0.851773 2.14846 1.37699 2.6233 0.876671 1.98904 0.53779 1.23894 0.778122 1.38401 1.51687 1.61407 0.69404 1.44801 1.10484 0.764304 0.923855 0.840982 0.765413 1.23459 1.22603 1.36336 0.722596 0.882954 1.17386 1.31282 1.02518 1.32706 1.07299 1.55386 1.34426 1.65032 1.32215 0.827969 0.84056 1.12599 1.65042 1.13252 1.52663 0.802505 3.33708 0.771796 1.43498 0.935724 2.08156 1.18129 1.13889 1.31729 1.13717 1.47197 2.68821 2.42593 1.49038 1.19939 1.40179 0.810686 1.43542 0.601911 2.50674 0.93474 0.613892 0.806404 0.669814 0.739944 0.917084 1.3858 0.344398 0.576373 2.02817 0.738991 1.04198 1.80671 1.50745 1.56616 1.06665 0.59027 1.08063 1.06993 1.0096 1.18634 0.557641 1.58891 0.577371 1.59948 1.23174 0.8194 2.23832 1.10985 1.24771 0.38546 1.08425 1.37194 1.30327 0.539748 1.00558 4.5004 0.680699 1.28559 1.47275 1.66754 0.668941 1.06646 1.79584 1.07416 1.57535 2.30378 1.44434 1.06256 1.45349 1.46524 0.651852 0.546936 1.14873 0.834057 1.09135 0.598914 0.609544 0.873529 1.06108 1.34563 3.81483 1.72603 1.01656 1.36471 1.28039 0.866191 1.66364 1.28794 0.522737 1.43381 1.33076 1.46574 3.04855 1.80201 0.761713 0.607443 0.75866 1.6881 2.51007 0.835619 1.63963 1.16513 1.3746 1.96746 0.731064 1.20369 2.69142 0.716915 2.53218 1.13495 2.88396 0.772465 1.73692 0.984345 1.06526 1.5341 1.62558 0.852775 2.47341 1.51357 2.57861 1.35026 0.958876 3.37534 1.80384 1.23153 0.671092 1.67966 1.44463 2.75172 1.93759 2.52654 1.33857 1.00649 1.65469 1.907 1.15622 1.1362 1.14276 2.11898 1.31003 1.48957 1.55777 1.73584 2.32409 0.228438 0.792243 1.22633 0.823194 1.24444 1.97845 2.93617 1.21509 1.3004 2.02881 1.06125 1.69938 2.28823 1.94433 1.94637 2.81344 1.82491 1.74254 1.36312 0.610342 0.84449 0.591781 0.997513 1.69715 1.64877 1.8577 1.09013 1.09698 0.760527 1.61684 2.04014 0.979173 0.896582 2.21881 1.22938 0.888968 2.24363 1.48973 2.77867 1.28583 1.25589 0.779184 1.66184 1.58876 1.66665 0.937456 1.14538 1.05411 0.64049 1.698 1.92928 2.61319 1.57904 2.15227 0.998407 2.00831 2.25826 2.47664 2.24601 1.30996 1.163 1.1032 2.13205 0.428582 1.6144 0.943645 0.905037 1.22507 1.35186 1.08858 2.0165 2.56952 1.55684 1.07755 2.23582 1.65178 1.30281 1.54048 0.88308 1.16341 0.929792 1.33269 1.8031 1.32328 1.5959 2.54947 2.81798 1.3014 1.62835 0.841108 1.57249 0.996442 1.47413 1.43924 1.33029 0.832884 0.911102 0.760526 0.875645 0.880285 0.512941 1.84813 1.04981 0.772837 0.989164 4.16527 0.509183 2.39877 1.0234 1.70343 1.81188 1.13425 1.2363 5.35252 1.79523 1.31969 1.25644 0.638888 0.780826 0.789825 1.92565 2.02967 0.915633 1.57126 0.904728 6.40646 1.00515 0.667458 1.1053 2.36803 2.56854 0.913059 1.14903 1.98029 2.93676 1.86125 1.07254 1.3558 1.51436 0.913349 1.58329 1.99563 0.955443 0.848988 2.12215 2.09775 1.10201 1.92979 0.792554 1.7396 1.48269 1.79334 2.54623 2.80223 1.18115 1.02007 0.606417 1.07177 1.48941 1.4015 1.34577 1.28442 1.54456 0.744245 1.36518 3.70118 0.683018 0.450986 0.944613 0.572217 1.03145 1.39441 1.19205 1.82625 2.88442 2.06095 1.48386 1.35519 1.70814 1.37925 2.92826 1.62624 2.09996 1.51277 0.715681 1.29931 0.934343 1.28614 2.15481 1.71837 1.19192 1.9468 1.28577 1.63506 0.766975 1.31442 1.1818 0.941066 1.20111 1.14793 0.999259 1.16424 3.30035 1.15428 1.42262 1.2006 1.95691 1.83687 1.24469 2.06097 2.55118 1.64386 3.1813 2.39528 1.29593 1.77359 0.604487 2.06032 2.54267 2.34383 1.86945 1.61256 0.540119 1.62623 1.17671 1.0559 1.19279 1.15665 0.950229 1.36503 1.47996 2.02274 2.91449 1.25857 1.14113 0.859474 0.805302 1.57238 1.02813 2.8605 0.464462 0.795606 1.44968 1.38944 0.868574 1.20076 2.98557 0.871657 1.2228 3.00202 0.66994 1.49432 1.98121 1.42692 1.37148 2.22594 4.41056 1.6396 0.221254 0.767498 2.06127 0.651792 2.04611 1.33141 2.32836 1.02465 1.14647 4.32594 1.00428 1.27404 1.15808 1.53812 3.36201 2.09076 2.33899 0.716519 1.15125 1.64875 1.89573 1.32834 0.88726 2.7386 1.86259 1.52535 1.40955 1.06817 2.85426 1.5783 1.18316 1.69838 0.945049 1.31921 2.00216 1.38059 0.761692 2.7381 1.49056 0.715627 1.01334 1.144 1.08578 1.75169 1.65011 1.60829 1.46985 2.08414 1.90817 2.26449 1.65716 1.19326 1.27749 0.621163 1.22738 2.37375 1.43569 1.6214 0.96181 1.21208 1.10967 2.12091 0.972976 1.1864 1.76636 1.94826 0.578552 1.50621 1.54221 0.896611 1.55499 2.20793 2.3962 1.15796 0.763935 1.05993 1.45594 0.906758 0.702438 0.885767 2.71437 2.0387 1.64833 1.53308 1.90044 0.781093 1.09975 0.93918 1.06878 1.35692 1.0293 1.85298 0.708278 2.56159 0.952558 1.48916 1.62414 1.91429 1.19804 1.89297 1.18642 1.83496 0.997213 1.46892 0.694633 0.725978 1.27113 2.94953 0.935143 0.941911 5.01761 1.17443 2.06311 1.62845 1.04186 1.43506 1.34359 1.18824 1.70451 3.94153 1.78139 1.43949 1.20547 1.10084 1.40563 1.84543 1.96664 1.02484 0.873704 1.13181 1.97643 2.26966 1.585 1.7125 1.3873 1.90345 2.35296 1.09374 1.12394 1.60366 1.97238 1.21931 1.59943 2.37422 1.02995 2.20268 1.09753 0.84667 0.868768 1.01085 1.55095 0.907837 0.696323 1.1552 1.48894 1.87555 1.29493 2.67587 1.8017 2.34994 1.31017 1.77965 1.43924 3.91229 1.49885 0.831159 1.04635 1.48761 0.993811 1.4421 2.02699 1.75481 2.01391 1.39023 1.35887 2.01638 1.19266 0.8216 1.44651 0.631842 0.813911 1.59518 1.25508 1.02961 1.27263 1.40551 1.51191 1.32432 1.06747 1.83126 1.32104 1.22685 0.749868 1.24592 0.965754 1.32625 1.16087 1.65078 1.30509 0.861707 1.10022 1.07399 1.31201 1.01031 1.24978 1.32267 2.12507 2.25312 1.36906 1.25018 0.755464 1.86068 1.47941 1.66343 1.15831 1.99009 1.26395 0.524112 0.792636 1.45556 0.525812 1.22875 0.859327 2.33132 1.75144 1.39481 2.06146 2.42972 1.2906 1.53349 1.33123 1.54326 2.30782 2.17551 1.62654 1.99593 1.85562 1.96708 1.55798 1.62379 2.1385 0.662247 2.37508 0.717347 2.4974 0.837768 2.44624 1.36701 2.77569 2.04224 1.72968 1.5448 0.46956 2.47013 0.956304 2.11569 1.7612 1.56621 3.11404 0.793263 2.20691 1.74897 1.09412 1.64843 0.891214 1.14788 0.797132 2.54862 1.16936 1.51204 1.69254 0.954457 1.599 1.29566 1.48772 1.27948 2.24497 1.48061 2.13204 2.19962 2.05834 1.37446 0.708497 0.730047 1.78986 2.43273 0.851236 1.85263 2.26399 1.85926 1.24077 1.37459 1.16901 1.87437 0.830411 1.23583 0.940084 1.55906 1.27152 1.20455 1.48985 0.991845 1.7549 1.06695 1.6556 0.744444 0.697229 1.25273 2.30839 3.18817 1.8199 1.56038 2.07387 1.79883 0.925294 2.47796 0.936215 1.08888 1.20027 1.37829 2.11455 1.24072 1.17925 0.864342 1.43596 2.19939 1.00503 1.59748 1.26746 1.91256 1.12744 1.99563 1.06912 2.02986 3.17766 1.67258 1.72939 1.59001 0.360881 0.334639 0.762446 1.59897 2.03287 1.92617 1.77748 1.31149 1.44411 0.801053 1.43735 1.91525 1.13062 0.882849 4.61176 1.48043 0.951883 1.03611 1.72847 1.65182 1.72238 0.988435 2.38827 1.73684 0.884357 0.767014 0.704727 1.33917 1.52919 0.687692 1.11779 1.26258 0.839135 0.682454 1.64398 1.02197 1.41116 1.02777 0.955361 1.0234 0.546047 1.59332 1.44501 0.875511 0.572887 1.71027 1.49904 1.42708 1.65289 1.58348 1.63192 1.94737 0.580283 2.49257 0.60283 0.86019 1.51333 1.45285 1.63334 0.690408 1.06403 2.04721 1.83538 1.3873 2.7496 1.93713 1.20483 1.4713 1.55928 2.5106 1.45064 2.50185 0.940802 1.72329 2.36301 1.60501 1.44117 1.13302 0.806161 1.41605 1.71615 2.1765 0.938149 1.63873 2.49011 9.20156 1.27907 0.978521 1.87747 1.96997 1.64204 2.53615 2.10208 1.46424 0.754744 1.90077 0.709622 1.10843 1.7459 1.22515 1.78313 2.54761 2.01444 1.23471 1.57972 2.69309 2.82683 1.99335 0.740733 1.50649 1.27906 1.60386 2.29034 1.23326 3.67672 1.44078 1.76074 1.58242 1.75629 0.971259 1.45628 0.861244 2.60554 0.80828 0.934443 1.17158 1.94278 1.28597 1.09987 1.96296 1.53682 1.17813 1.19606 2.09854 0.946844 1.09086 2.08654 1.3972 1.81741 ]
@@@ Frame-accuracy per-class: [ 89.5694 80.2768 69.0196 79.6373 78.4977 50.5747 64.5533 70.8661 64.0155 52.5942 83.7545 60.0473 64.3026 72.4733 10.989 79.7927 68.5083 56.0699 56.9128 63.8436 81.1398 64.5378 65.6911 81.4081 85.7754 67.102 83.3111 23.0986 70.9062 54.3807 65.8477 79.646 68.2171 52.2822 85.5385 77.8887 48.5686 45.7711 60.3509 57.4108 84.4994 61.4887 59.3312 69.5921 50.6108 69.1145 84.2182 53.617 61.9615 78.346 59.1195 53.4854 63.9668 52.6316 77.3067 65.26 67.6627 81.8086 82.7415 59.7433 60.1942 71.0448 88.6822 88.9296 45.9893 42.5163 46.8647 59.9553 70.9064 78.1955 80.4124 81.3061 82.6624 64.5395 65.8754 65.4206 88.5017 94.0071 77.8739 71.1864 67.5862 65.1014 86.0728 80.2168 69.9172 57.732 54.1063 70.0111 68.5071 74.3802 77.3573 78.4669 77.4869 36.8601 63.5272 42.2018 75.8717 83.3233 79.4301 82.2521 40.0802 72.9981 36.715 82.3157 83.977 76.2304 59.952 67.0373 69.1262 73.2601 46.5237 56.0647 47.2879 82.3049 42.9003 21.5827 52.7183 61.745 38.7879 63.893 64.9275 46.7797 58.7234 39.6887 78.6922 77.7143 43.4043 52.5896 62.8571 79.9368 87.9247 53.1722 78.2324 75.718 48.3951 82.3529 76.4045 85.7388 67.1463 58.7112 54.4928 74.5732 70.137 75.1855 84.486 79.8354 59.3078 61.5147 72.9012 78.9144 91.6935 60.9642 57.8462 47.3538 64.432 78.4555 88.5581 83.0287 76.1046 85.205 75.8621 61.4987 53.3333 6.22568 69.9433 61.4433 75.0442 66.4311 81.1466 53.5433 83.9949 67.2032 65.2095 70.024 61.7211 76.5957 75.0499 68.9655 45.2747 90.0901 60.4651 54.6763 59.0604 69.7095 93.1602 56.8579 30.8725 50.1441 22.4599 74.3972 78.4689 63.5332 88.0188 74.8108 64.0199 65.5532 43.52 54.4928 50.4202 79.2079 33.888 39.7291 64.133 29.1971 30.9677 74.2049 34.6076 74.0975 68.2927 56.917 56.6474 57.3698 79.1367 0 57.6169 78.2698 54.7993 79.4631 75.9062 56.0122 72.3137 81.5356 54.8552 67.9245 53.012 49.1694 15.6522 83.9024 74.3516 65.4462 65.7534 73.6225 73.2443 27.451 75.3564 38.8571 79.1003 24.8276 31.9392 65.9134 67.1096 81.7204 69.0411 69.3706 56.1028 7.19424 75.7333 14.2222 37.7724 56.6728 24.5614 67.2 60.1653 65.2068 60.9964 77.4373 48.833 72.1689 77.0026 79.9239 72.1493 53.7102 62.3917 51.6129 62.5122 79.5181 51.5081 59.4872 33.887 67.2304 72.3949 55.7629 37.2372 39.4801 35.012 20.4724 74.8092 18.3908 69.7095 47.5912 67.5809 50.3632 44.5714 76.6105 60.7792 51.1945 41.8251 61.2613 0 41.9865 58.6139 42.1557 80.589 86.8869 42.8394 63.3833 70.195 72.0391 60.274 54.1524 80.9318 64.2487 64.9077 69.1849 90.869 80.7339 46.8468 55.7621 45.6212 41.3187 61.4108 69.7947 67.1067 61.8705 50.591 39.5257 81.7204 62.2735 84.3906 55.8659 16.8067 82.2857 30.9392 80.2867 41.6327 27.5037 43.8938 10.929 50.2092 77.2908 69.8287 43.6975 45.3826 66.6667 62.3853 81.386 40.6639 57.4939 58.3519 74.6867 71.3287 69.7248 86.7052 72.9109 57.1429 69.4949 64.8526 47.619 63.7555 64.0523 62.768 60.9687 85.3772 89.893 64.7292 71.9168 52.3985 71.1744 56.7119 86.1215 75.0408 48.2468 62.0865 32.9238 75.4717 39.3443 87.5847 68.9972 76.9231 62.2074 54.6463 55.2668 79.9403 63.8941 57.4669 83.1769 78.5219 78.0211 76.2317 63.8704 64.8715 52.0256 77.6233 73.9635 63.3822 56.872 73.0733 58.3851 71.21 61.9289 56.517 53.3007 68.8207 72.3404 69.6707 68.4119 53.4435 70.7182 54.1872 73.6243 0 84.0237 59.1045 73.0606 46.5116 69.914 66.3239 57.6874 66.7817 55.4095 28.8 39.4881 50.5263 65.1163 61.442 81.6777 58.0087 81.2861 27.204 73.5812 83.3825 69.5971 81.7699 77.8325 74.8092 65.2452 91.9932 83.208 39.6476 81.9672 68.3612 45.9813 59.3684 54.9266 74.0187 83.6689 63.046 70.5882 69.3572 69.1323 84.2922 59.176 79.7307 58.256 65.8361 78.2266 16.3265 71.3043 71.6763 88.8301 67.3008 42.9752 64.0502 83.792 78.0876 0 72.0257 63.4146 47.2362 53.3762 77.6699 69.1943 39.953 65.1201 57.3379 37.751 55.7214 69.8337 60.4106 55.6098 83.274 86.3594 65.8579 79.8325 67.9646 83.5913 82.227 74.9311 61.5165 61.4108 10.4869 50.7216 75.2768 66.0194 63.6042 79.8206 48.1356 71.0462 85.2221 58.4795 53.9924 56.2814 12.7854 47.9401 77.4411 77.8885 80.6846 45.3039 31.7343 63.035 51.7928 60.6623 58.9212 44.2953 81.6463 61.3482 14.7465 79.6413 26.3736 63.8146 27.3684 77.9547 48.6957 66.9078 68.9139 61.7284 50.3597 71.9493 24.8062 61.9355 32.0819 65.285 75.8294 18.4758 45.478 63.836 82.7188 52.4313 67.8815 60.6635 40.9581 21.0046 62.9247 65.0508 49.0421 62.5473 64.2254 69.5035 73.6462 20.5761 51.9056 61.2022 58.3732 55.0129 42.7184 94.9309 78.4399 62.3377 77.9747 65.2932 34.5679 26.484 68.3915 60.3325 38.7597 73.8007 43.787 41.9048 36.2694 41.9753 23.8361 46.9097 53.9792 63.4667 83.2497 76.9096 83.5282 76.0925 54.0881 54.1254 43.3566 68.6515 63.7363 80.5933 49.5468 46.4037 71.8507 75.8842 34.5324 68.7351 71.2212 38.2979 58.6826 21.9081 56.4784 62.0027 77.116 53.2151 53.4699 47.8528 82.0779 72.2045 66.1926 80.4064 50.8972 41.0678 46.2222 57.3379 42.7518 69.7906 41.3926 32.0557 26.383 35.4212 56.872 71.5655 67.0659 29.9065 88.5806 58.104 73.772 79.2696 67.4221 62.6609 62.8931 37.4603 27.3894 63.7993 69.3642 31.5582 46.3023 62.069 64.1509 76.2044 65.3802 76.1171 63.3245 47.8873 57.329 57.3991 19.7403 30.3694 71.4628 55.9177 70.9677 58.9372 76.7025 44.6097 57.561 65.0932 76.4641 71.0963 74.3838 85.8757 69.8647 94.1704 48.0211 68.9867 80.7374 68.186 0 84.2688 21.1055 71.8954 50.3464 38.8693 74.6922 57.9151 3.05344 46.4088 52.8 59.4657 84.193 75.0809 76.5922 36.7677 44.0252 73.4499 54.9918 75.2967 0 69.905 80.7237 66.2632 27.2931 28.2561 72 67.4938 39.8754 9.6 46.3343 69.4505 64.311 55.2486 76.2431 45.9948 27.7372 74.1427 75.6757 29.3996 36.4217 59.726 42.3301 82.8508 44.5415 51.2563 45.3461 23.5294 15.3846 65.6331 71.6332 81.9533 63.3124 59.1362 62.3482 56.4019 66.9216 54.7315 79.1289 62.4561 9.86547 79.677 88.0097 70.7811 85.2676 72.1044 61.0442 63.5838 35.4286 31.049 35.9249 57.8797 64.7328 50.9978 43.594 8.21918 60.3675 48.4404 62.2407 75.5927 65.8722 72.956 63.1351 42.0881 44.5498 65.9971 41.4414 63.2522 46.6513 84.5341 63.3257 64.8752 67.1329 71.8663 61.5087 67.7532 67.4938 9.34579 65.9649 62.768 68.1427 51.2397 43.5798 69.1643 18.8679 37.2881 44.9086 15.5689 26.4463 57.2347 52.3677 84.2579 30.2789 33.7079 18.799 35.5748 54.498 83.6364 52.194 62.5243 73.0949 65.9529 66.1078 70.1235 53.9802 63.6496 32.0611 18.9602 69.6177 67.6737 72.7273 77.9956 56.0132 74.9758 34.5382 87.7989 80.4449 60.3774 63.9478 75.2412 68.1018 33.5196 72.7838 67.6171 13.6054 82.9822 59.9562 41.2214 62.3377 58.256 35.6941 30.1969 44.335 93.0338 78.3813 44.4444 83.7041 44.6316 65.9698 39.4366 71.5667 60.6876 14.0351 71.9212 56.9361 65.8462 53.7983 34.7826 42.577 26.1307 78.6948 70.028 46.8647 47.4849 63.0975 75.4452 34.7826 46.2777 58.5683 60.8696 73.2919 29.8305 61.678 59.164 45.1613 71.1524 58.3144 42.0048 54.892 78.3599 17.5439 60.4361 78.1634 71.346 60.9346 70.0855 47.3068 52.7578 46.3023 53.4535 44.3373 47.3008 30.7102 43.7673 69.6774 62.1392 81.0195 64.8732 26.506 54.7771 52.9915 71.4839 57.7281 69.2718 39.4203 69.6306 65.6684 49.5127 57.0397 86.8805 58.7678 50.4624 70.7834 46.5823 36.129 44.6115 63.5108 80.6056 71.076 58.193 71.9472 84.0846 74.1284 31.5789 32.07 49.4845 50.9317 46.9974 76.161 69.2168 72.9064 75.7895 65.7837 75.195 43.1373 81.6901 27.5862 72.8443 60.4651 48.8223 51.1247 68.2707 40.3587 69.5985 44.2748 70.1826 58.3942 82.5295 79.0106 64.3423 17.7474 72.9002 75.0442 0 68.3041 40.1985 60.5863 63.4672 56.8924 61.4004 64.8794 61.9273 8.79121 46.2863 64.7887 65.0869 71.4462 54.3923 47.1021 49.062 73.7864 75.5801 64.2672 37.5451 29.316 52.9332 54.7896 54.8249 48.3965 33.79 66.277 70.1225 55.9819 43.0868 63.5514 55.1084 34.9776 71.7557 26.616 67.0571 76.8964 78.97 67.0096 54.1063 73.2789 76.4846 70.648 63.8743 50.2879 60.4966 15.9052 49.4208 38.0655 63.5044 49.0566 53.3333 4.74308 55.0218 76.9543 66.3379 56.9316 74.6708 50.4983 40.9894 43.2602 44.183 58.2575 57.6788 51.4851 60.1307 75.7895 63.2603 73.7725 72.8063 58.41 61.1966 69.2617 71.4681 56.2368 59.4848 59.0975 70.3151 46.6019 63.2332 58.8458 80.2065 65.7084 70.0132 60.9895 66.9789 49.1557 62.4729 73.3186 71.4055 69.1285 58.6103 73.6518 60.0954 66.1454 35.7488 39.7306 58.8235 69.3767 76.9874 53.2189 51.6291 55.9546 63.6197 48.7075 68.2353 81.0399 80.0937 53.1328 86.2042 64.0835 74.1214 49.7959 57.468 59.3886 38.61 17.2589 58.2227 63.7002 66.4615 56.9211 32.618 36.2416 53.1876 49.6203 44.4444 36.3112 57.9151 56.7164 39.9469 82.7423 35.8974 76.2489 36.6492 74.8828 33.7349 56.2874 20.2429 39.6867 58.6797 56.5771 87.4282 26.087 73.8255 43.5644 44.5131 57.0928 11.0553 78.4969 40.3587 40.3433 69.657 59.7701 70.4857 73.695 69.7783 27.9793 70.9447 57.9477 48.8794 68.1145 51.0638 55.1845 58.6371 67.5799 37.0656 50.8221 43.3735 36.4126 44.8211 62.8337 82.1382 75.9777 42.1053 42.0513 77.3446 47.557 32.9571 48.7455 60.281 64.2674 70.7644 44.0895 81.2403 67.6019 71.4055 54.2199 56.049 58.7874 57.7297 76.9231 47.5362 68.2231 54.2304 82.8025 82.0639 71.9212 42.485 4.25532 49.8504 59.168 42.0268 47.2648 79.0823 6.77966 73.5319 64.942 66.3594 58.3186 28.8499 59.1885 59.9349 75.9644 51.816 31.9392 73.3496 55.914 63.7298 48.8778 68.5413 46.9914 65.34 43.3735 14.8837 49.8866 56.2963 55.8704 90.3991 91.9027 76.4045 47.8711 40 42.4908 47.8469 55.3191 54.0216 78.8696 59.9034 37.8539 61.4476 72.1649 0 56.1151 71.5996 75.162 47.6684 54.5455 59.19 69.8413 39.6201 47.4227 75.7721 77.6445 80.7966 62.2309 59.0585 79.1966 62.5455 59.761 78.2107 78.418 58.1436 64.5598 59.3548 74.6888 73.1627 70.6096 83.554 50.966 58.6985 73.3057 80.8565 52.9774 62.9108 63.8298 52.3282 49.4624 45.7143 45.5574 84.5841 27.3128 86.7076 75.6431 55.6263 57.4586 55.8875 83.7209 71.2871 48.0493 49.1803 54.0541 27.6243 49.5289 67.031 61.1898 49.6141 31.6279 60.8696 43.4164 73.6232 37.9101 29.1589 56.5189 54.2373 66.4279 74.198 59.6226 49.2147 38.2979 75.0769 49.14 21.7143 0 63.1016 72.7273 50.9719 46.3277 45.6799 28.5973 34.2679 55.4324 88.6154 40.0716 77.9787 65.8824 41.1862 72.6115 53.5433 32.9412 46.9453 67.0036 48.7361 29.0909 32.312 34.8052 78.5568 53.7815 55.0725 58.1907 30.8571 63.8581 7.87402 60.3129 61.6408 58.7332 49.2035 70.0709 62.0408 78.8155 30.9677 77.2603 72.3017 69.599 42.0339 62.5551 59.5978 31.4779 57.9957 82.9268 60.0398 38.8693 76.1115 68.3805 41.8097 55.0265 46.3668 ]

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:347) Done 2622 files, 1 with no tgt_mats, 0 with other errors. [TRAINING, 0.34044 min, fps36965.1]
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:353) AvgLoss: 0.845228 (Xent), [AvgXent: 0.845228, AvgTargetEnt: 0]
progress: []
FRAME_ACCURACY >> 74.0664% <<

