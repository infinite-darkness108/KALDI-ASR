speech-HP-Z2-Tower-G9
nnet-train-multistream-perutt --cross-validate=false --randomize=true --verbose=0 --num-streams=10 --max-frames=15000 --learn-rate=0.00004 --momentum=0.9 --l1-penalty=0 --l2-penalty=0 --feature-transform=exp_FG/blstm4i/final.feature_transform 'ark:copy-feats scp:exp_FG/blstm4i/train.scp ark:- | apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_tr90/utt2spk scp:data-fbank/train_tr90/cmvn.scp ark:- ark:- | add-deltas --delta-order=2 ark:- ark:- |' 'ark:ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- |' exp_FG/blstm4i/nnet/nnet_iter01_learnrate0.00004_tr4.1309_cv3.4422 exp_FG/blstm4i/nnet/nnet_iter02 
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:243) Not in compute-exclusive mode.  Suggestion: use 'nvidia-smi -c 3' to set compute exclusive mode
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:438) Selecting from 1 GPUs
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:453) cudaSetDevice(0): NVIDIA RTX A2000 12GB	free:11515M, used:515M, total:12031M, free/total:0.957153
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:501) Device: 0, mem_ratio: 0.957153
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:382) Trying to select device: 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:511) Success selecting device 0 free mem ratio: 0.957153
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:FinalizeActiveGpu():cu-device.cc:338) The active GPU is [0]: NVIDIA RTX A2000 12GB	free:11001M, used:1029M, total:12031M, free/total:0.91443 version 8.6
copy-feats scp:exp_FG/blstm4i/train.scp ark:- 
apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_tr90/utt2spk scp:data-fbank/train_tr90/cmvn.scp ark:- ark:- 
add-deltas --delta-order=2 ark:- ark:- 
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:144) TRAINING STARTED
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Read():nnet/nnet-matrix-buffer.h:191) Read() started... Buffer size in MB: 0, max 3072, having 0 utterances.
ali-to-post ark:- ark:- 
ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl 'ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |' ark:- 
LOG (copy-feats[5.5.1074~1-71f3]:main():copy-feats.cc:143) Copied 2624 feature matrices.
LOG (apply-cmvn[5.5.1074~1-71f3]:main():apply-cmvn.cc:159) Applied cepstral mean and variance normalization to 2624 utterances, errors on 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:303) ### After 0 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:304) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.300987, max 0.3314, mean 0.00338258, stddev 0.0703507, skewness 0.01947, kurtosis -0.274843 ) 
  f_w_gifo_r_   ( min -0.425629, max 0.408362, mean -0.000551798, stddev 0.0757714, skewness 0.00175001, kurtosis -0.0180464 ) 
  f_bias_   ( min -0.353752, max 1.26876, mean 0.220793, stddev 0.450282, skewness 1.07679, kurtosis -0.654226 ) 
  f_peephole_i_c_   ( min -0.361896, max 0.370562, mean -0.00530176, stddev 0.111805, skewness -0.0609771, kurtosis 0.539747 ) 
  f_peephole_f_c_   ( min -0.391959, max 0.500677, mean 0.00304414, stddev 0.121062, skewness 0.0278671, kurtosis 1.63353 ) 
  f_peephole_o_c_   ( min -0.446873, max 0.40632, mean -0.0173138, stddev 0.149239, skewness 0.246137, kurtosis -0.135267 ) 
  f_w_r_m_   ( min -0.429356, max 0.476343, mean 0.000492608, stddev 0.0937295, skewness -0.00288254, kurtosis -0.0417473 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -0.352789, max 0.321916, mean 0.0054279, stddev 0.0692164, skewness -0.0169064, kurtosis -0.309607 ) 
  b_w_gifo_r_   ( min -0.334154, max 0.30071, mean -0.000337332, stddev 0.065406, skewness 0.000288514, kurtosis -0.549932 ) 
  b_bias_   ( min -0.250738, max 1.15977, mean 0.215072, stddev 0.443922, skewness 1.07697, kurtosis -0.67553 ) 
  b_peephole_i_c_   ( min -0.377528, max 0.250241, mean 0.00623974, stddev 0.0830037, skewness -0.13967, kurtosis 0.901678 ) 
  b_peephole_f_c_   ( min -0.495839, max 0.389634, mean 0.00757896, stddev 0.111162, skewness 0.302725, kurtosis 3.20648 ) 
  b_peephole_o_c_   ( min -0.374631, max 0.314181, mean -0.0147096, stddev 0.133395, skewness -0.0227651, kurtosis -0.450655 ) 
  b_w_r_m_   ( min -0.330995, max 0.332611, mean -2.74286e-05, stddev 0.0778948, skewness -0.00236796, kurtosis -0.230137 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.674318, max 0.60689, mean -0.000155921, stddev 0.100804, skewness 0.00461364, kurtosis 0.0487149 ) , lr-coef 1, max-norm 0
  bias ( min -0.32129, max 1.85085, mean -2.14204e-09, stddev 0.0583134, skewness 25.1665, kurtosis 791.79 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:305) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -8.67671, max 9.02116, mean 0.0321104, stddev 1.01793, skewness 0.450612, kurtosis 3.20408 ) 
[1] output of <BlstmProjected> ( min -4.57636, max 4.49172, mean -0.00272934, stddev 0.729005, skewness 0.0511132, kurtosis 1.32951 ) 
[2] output of <Tanh> ( min -0.999788, max 0.999749, mean -0.0031993, stddev 0.497656, skewness 0.00751922, kurtosis -0.741121 ) 
[3] output of <AffineTransform> ( min -22.2699, max 20.634, mean 0.0164452, stddev 1.77788, skewness 0.523958, kurtosis 3.28065 ) 
[4] output of <Softmax> ( min 1.20798e-14, max 0.995124, mean 0.00077932, stddev 0.013838, skewness 42.0681, kurtosis 2045.48 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:307) 
### BACKWARD PROPAGATION BUFFER CONTENT :
[0] diff of <Input>  ( min -2.29765, max 3.09318, mean 0.00150465, stddev 0.151737, skewness 0.448005, kurtosis 16.2832 ) 
[1] diff-output of <BlstmProjected> ( min -0.608617, max 0.658569, mean -6.92272e-05, stddev 0.0655491, skewness -0.0226353, kurtosis 2.35605 ) 
[2] diff-output of <Tanh> ( min -0.641373, max 0.669615, mean -0.000165371, stddev 0.0853371, skewness -0.0131373, kurtosis 1.07688 ) 
[3] diff-output of <AffineTransform> ( min -0.999995, max 0.949399, mean -4.58387e-07, stddev 0.0251664, skewness -27.1508, kurtosis 1199.81 ) 
[4] diff-output of <Softmax> ( min -0.999995, max 0.949399, mean -4.58387e-07, stddev 0.0251664, skewness -27.1508, kurtosis 1199.81 ) 
### END BACKWARD


LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:308) 
### GRADIENT STATS :
Component 1 : <BlstmProjected>, ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  ### Gradients 
  f_w_gifo_x_corr_   ( min -53.8803, max 33.4128, mean -0.0129069, stddev 2.58802, skewness -0.181292, kurtosis 18.3188 ) 
  f_w_gifo_r_corr_   ( min -55.0732, max 38.9615, mean 0.00462998, stddev 2.0289, skewness -0.223813, kurtosis 29.3264 ) 
  f_bias_corr_   ( min -45.0517, max 21.9121, mean 0.205294, stddev 3.59717, skewness -1.28251, kurtosis 24.4381 ) 
  f_peephole_i_c_corr_   ( min -26.7655, max 20.8236, mean -0.374648, stddev 4.37442, skewness -1.41203, kurtosis 11.5565 ) 
  f_peephole_f_c_corr_   ( min -250, max 71.7656, mean -0.386459, stddev 19.8528, skewness -6.83224, kurtosis 82.992 ) 
  f_peephole_o_c_corr_   ( min -42.7131, max 250, mean 2.33799, stddev 23.0368, skewness 6.41869, kurtosis 57.3796 ) 
  f_w_r_m_corr_   ( min -48.4539, max 50.2602, mean -0.014101, stddev 4.84757, skewness 0.0600256, kurtosis 7.79587 ) 
  ---
  b_w_gifo_x_corr_   ( min -58.6669, max 83.595, mean 0.128085, stddev 3.53982, skewness 0.639606, kurtosis 36.3308 ) 
  b_w_gifo_r_corr_   ( min -56.8004, max 69.6812, mean -0.00548186, stddev 2.66919, skewness 0.120312, kurtosis 18.7673 ) 
  b_bias_corr_   ( min -60.1624, max 41.1584, mean -0.251826, stddev 5.95407, skewness -0.704144, kurtosis 18.2633 ) 
  b_peephole_i_c_corr_   ( min -21.8388, max 58.8375, mean 0.0352241, stddev 5.03745, skewness 4.39038, kurtosis 58.2811 ) 
  b_peephole_f_c_corr_   ( min -241.111, max 231.688, mean -0.120946, stddev 20.6253, skewness -0.62445, kurtosis 105.425 ) 
  b_peephole_o_c_corr_   ( min -49.6392, max 93.509, mean 0.656283, stddev 13.8103, skewness 0.983341, kurtosis 7.68295 ) 
  b_w_r_m_corr_   ( min -30.5455, max 37.7472, mean -0.00229762, stddev 4.84412, skewness 0.00694437, kurtosis 1.22822 ) 

  ### Activations (mostly after non-linearities)
  YI_FW(0..1)^   ( min 0, max 1, mean 0.441232, stddev 0.339211, skewness 0.325208, kurtosis -1.28668 ) 
  YF_FW(0..1)^   ( min 0, max 1, mean 0.613267, stddev 0.32762, skewness -0.418456, kurtosis -1.16131 ) 
  YO_FW(0..1)^   ( min 0, max 1, mean 0.363627, stddev 0.35295, skewness 0.646655, kurtosis -1.12025 ) 
  YG_FW(-1..1)   ( min -1, max 1, mean 0.0307866, stddev 0.881155, skewness -0.0615195, kurtosis -1.82303 ) 
  YC_FW(-R..R)*  ( min -50, max 50, mean 0.368313, stddev 13.575, skewness 0.115294, kurtosis 8.75847 ) 
  YH_FW(-1..1)   ( min -1, max 1, mean 0.0412332, stddev 0.67301, skewness -0.0640707, kurtosis -1.20087 ) 
  YM_FW(-1..1)   ( min -1, max 1, mean -0.000921585, stddev 0.327764, skewness -0.166398, kurtosis 2.84762 ) 
  YR_FW(-R..R)   ( min -3.93837, max 4.49172, mean 0.0111229, stddev 0.697665, skewness 0.173763, kurtosis 1.18256 ) 
  ---
  YI_BW(0..1)^   ( min 0, max 1, mean 0.482118, stddev 0.299112, skewness 0.212555, kurtosis -1.18666 ) 
  YF_BW(0..1)^   ( min 0, max 1, mean 0.659531, stddev 0.246341, skewness -0.505077, kurtosis -0.452179 ) 
  YO_BW(0..1)^   ( min 0, max 0.999997, mean 0.397185, stddev 0.332175, skewness 0.513539, kurtosis -1.17042 ) 
  YG_BW(-1..1)   ( min -1, max 1, mean 0.0135589, stddev 0.835593, skewness -0.0220484, kurtosis -1.74682 ) 
  YC_BW(-R..R)*  ( min -50, max 50, mean 1.01203, stddev 10.7188, skewness 1.15914, kurtosis 14.1136 ) 
  YH_BW(-1..1)   ( min -1, max 1, mean 0.0392369, stddev 0.70352, skewness -0.0471217, kurtosis -1.35019 ) 
  YM_BW(-1..1)   ( min -0.999976, max 0.999983, mean 0.00518406, stddev 0.355495, skewness -0.0241457, kurtosis 1.54633 ) 
  YR_BW(-R..R)   ( min -4.57636, max 3.94923, mean -0.0165536, stddev 0.755479, skewness -0.0374757, kurtosis 1.41381 ) 

  ### Derivatives (w.r.t. inputs of non-linearities)
  DI_FW^  ( min -1, max 1, mean 8.51099e-05, stddev 0.0195586, skewness 0.423668, kurtosis 216.082 ) 
  DF_FW^  ( min -1, max 1, mean -4.15702e-06, stddev 0.0152446, skewness -3.88336, kurtosis 499.404 ) 
  DO_FW^  ( min -1, max 1, mean 0.000104022, stddev 0.0346033, skewness -0.0197947, kurtosis 167.605 ) 
  DG_FW   ( min -1, max 1, mean 2.45077e-05, stddev 0.0266057, skewness 0.700591, kurtosis 326.017 ) 
  DC_FW*  ( min -10.0754, max 9.35498, mean -0.000167233, stddev 0.175528, skewness -0.403796, kurtosis 125.074 ) 
  DH_FW   ( min -10.3149, max 11.503, mean -5.5923e-05, stddev 0.137647, skewness 0.433519, kurtosis 384.064 ) 
  DM_FW   ( min -21.1227, max 16.78, mean 0.000876653, stddev 0.447945, skewness -0.176933, kurtosis 88.5676 ) 
  DR_FW   ( min -2.38257, max 2.74546, mean 5.74468e-05, stddev 0.103455, skewness 0.0162211, kurtosis 22.7537 ) 
  ---
  DI_BW^  ( min -0.659271, max 0.663261, mean -0.000151073, stddev 0.0162171, skewness 0.216518, kurtosis 95.9565 ) 
  DF_BW^  ( min -0.664606, max 0.67555, mean -0.00010749, stddev 0.0135492, skewness 0.341544, kurtosis 102.58 ) 
  DO_BW^  ( min -0.547111, max 0.619204, mean -8.95397e-05, stddev 0.0212152, skewness 0.284656, kurtosis 30.9025 ) 
  DG_BW   ( min -1, max 1, mean 9.11368e-05, stddev 0.0326899, skewness 0.303159, kurtosis 141.434 ) 
  DC_BW*  ( min -4.79747, max 5.16711, mean 0.000945491, stddev 0.137315, skewness 0.834923, kurtosis 56.0322 ) 
  DH_BW   ( min -2.25552, max 2.44038, mean 0.000256007, stddev 0.0752026, skewness -0.0221135, kurtosis 34.1127 ) 
  DM_BW   ( min -3.70049, max 3.14757, mean 0.00181446, stddev 0.217654, skewness -0.0305561, kurtosis 6.04497 ) 
  DR_BW   ( min -1.07432, max 1.31166, mean -0.000283909, stddev 0.0857582, skewness -0.00701191, kurtosis 3.36149 ) 
Component 2 : <Tanh>, 
Component 3 : <AffineTransform>, 
  linearity_grad ( min -150.283, max 149.593, mean -2.27374e-08, stddev 2.71924, skewness 0.927165, kurtosis 446.077 ) , lr-coef 1, max-norm 0
  bias_grad ( min -231.349, max 330.254, mean -3.09944e-07, stddev 12.3376, skewness 9.20594, kurtosis 499.4 ) , lr-coef 1
Component 4 : <Softmax>, 
### END GRADIENT

LOG (ali-to-pdf[5.5.1074~1-71f3]:main():ali-to-pdf.cc:68) Converted 2919 alignments to pdf sequences.
LOG (ali-to-post[5.5.1074~1-71f3]:main():ali-to-post.cc:73) Converted 2919 alignments.
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:168) MC05_98, missing targets
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:334) ### After 755062 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:335) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.355673, max 0.336965, mean 0.00350909, stddev 0.0736455, skewness 0.0134472, kurtosis -0.132953 ) 
  f_w_gifo_r_   ( min -0.445196, max 0.40056, mean -0.000572445, stddev 0.0761557, skewness 0.00094158, kurtosis -0.0106089 ) 
  f_bias_   ( min -0.353702, max 1.28243, mean 0.218177, stddev 0.4533, skewness 1.07433, kurtosis -0.655898 ) 
  f_peephole_i_c_   ( min -0.3764, max 0.369109, mean -0.00579472, stddev 0.11422, skewness -0.0541228, kurtosis 0.69791 ) 
  f_peephole_f_c_   ( min -0.493632, max 0.789383, mean 0.00208594, stddev 0.141345, skewness 0.325635, kurtosis 4.38515 ) 
  f_peephole_o_c_   ( min -0.495588, max 0.423398, mean -0.0151365, stddev 0.159437, skewness 0.243382, kurtosis -0.177994 ) 
  f_w_r_m_   ( min -0.485306, max 0.481097, mean 0.000477928, stddev 0.0957113, skewness 0.000162791, kurtosis -0.0423553 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -0.475277, max 0.44022, mean 0.00600088, stddev 0.0751442, skewness -0.0226455, kurtosis 0.312667 ) 
  b_w_gifo_r_   ( min -0.339587, max 0.30893, mean -0.000269862, stddev 0.0668413, skewness 0.00193539, kurtosis -0.46802 ) 
  b_bias_   ( min -0.293218, max 1.16403, mean 0.211775, stddev 0.446312, skewness 1.06906, kurtosis -0.679125 ) 
  b_peephole_i_c_   ( min -0.341314, max 0.265661, mean 0.00596035, stddev 0.0855359, skewness -0.0249465, kurtosis 0.689641 ) 
  b_peephole_f_c_   ( min -0.420732, max 0.56097, mean 0.0115235, stddev 0.131142, skewness 0.627342, kurtosis 3.35085 ) 
  b_peephole_o_c_   ( min -0.524298, max 0.34616, mean -0.0175643, stddev 0.155377, skewness -0.215479, kurtosis 0.133178 ) 
  b_w_r_m_   ( min -0.358498, max 0.336653, mean -0.000147236, stddev 0.0816128, skewness -0.00302681, kurtosis -0.174974 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.778583, max 0.676899, mean -0.000155921, stddev 0.102461, skewness 0.00622605, kurtosis 0.0582926 ) , lr-coef 1, max-norm 0
  bias ( min -0.0638544, max 1.93261, mean -1.11759e-09, stddev 0.0611704, skewness 25.1519, kurtosis 778.218 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:336) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -14.8312, max 13.8231, mean 0.00769689, stddev 0.940803, skewness 0.731578, kurtosis 12.6034 ) 
[1] output of <BlstmProjected> ( min -4.21807, max 4.21488, mean -0.00247445, stddev 0.645813, skewness -0.031831, kurtosis 3.06675 ) 
[2] output of <Tanh> ( min -0.999566, max 0.999564, mean -0.00102954, stddev 0.430759, skewness -0.01106, kurtosis 0.0960636 ) 
[3] output of <AffineTransform> ( min -14.2558, max 18.6431, mean 0.00260229, stddev 1.74861, skewness 0.828628, kurtosis 5.42276 ) 
[4] output of <Softmax> ( min 7.76684e-13, max 0.997997, mean 0.000781018, stddev 0.0135789, skewness 41.1589, kurtosis 1982.9 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:338) 
### BACKWARD PROPAGATION BUFFER CONTENT :
[0] diff of <Input>  ( min -1.80307, max 2.09308, mean 0.00797999, stddev 0.186742, skewness 0.373789, kurtosis 10.6078 ) 
[1] diff-output of <BlstmProjected> ( min -0.424318, max 0.44591, mean 3.91577e-05, stddev 0.0474594, skewness 0.0149327, kurtosis 5.63546 ) 
[2] diff-output of <Tanh> ( min -0.494847, max 0.504577, mean 0.000148735, stddev 0.0627474, skewness 0.0273916, kurtosis 3.60542 ) 
[3] diff-output of <AffineTransform> ( min -0.999565, max 0.861156, mean -4.67279e-09, stddev 0.0187039, skewness -31.7011, kurtosis 1750.54 ) 
[4] diff-output of <Softmax> ( min -0.999565, max 0.861156, mean -4.67279e-09, stddev 0.0187039, skewness -31.7011, kurtosis 1750.54 ) 
### END BACKWARD


LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:339) 
### GRADIENT STATS :
Component 1 : <BlstmProjected>, ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  ### Gradients 
  f_w_gifo_x_corr_   ( min -47.8511, max 40.3572, mean 0.0856494, stddev 4.11459, skewness -0.0638959, kurtosis 7.01356 ) 
  f_w_gifo_r_corr_   ( min -37.7175, max 35.9833, mean -0.00290102, stddev 3.0907, skewness -0.0301809, kurtosis 6.10627 ) 
  f_bias_corr_   ( min -28.8438, max 27.0157, mean -0.221124, stddev 5.13284, skewness -0.191105, kurtosis 3.34864 ) 
  f_peephole_i_c_corr_   ( min -27.0586, max 44.9403, mean 0.303976, stddev 6.17542, skewness 2.05454, kurtosis 14.6923 ) 
  f_peephole_f_c_corr_   ( min -133.429, max 116.14, mean -0.72498, stddev 17.9937, skewness -0.507738, kurtosis 19.4182 ) 
  f_peephole_o_c_corr_   ( min -154.541, max 185.358, mean -1.0709, stddev 20.2215, skewness 0.983265, kurtosis 32.1667 ) 
  f_w_r_m_corr_   ( min -52.85, max 42.7246, mean 0.00748604, stddev 5.72696, skewness -0.00854917, kurtosis 3.30408 ) 
  ---
  b_w_gifo_x_corr_   ( min -144.148, max 152.172, mean 0.288181, stddev 7.27908, skewness -0.891408, kurtosis 57.4931 ) 
  b_w_gifo_r_corr_   ( min -68.6233, max 85.7974, mean -0.0196149, stddev 3.71344, skewness 0.0471411, kurtosis 15.5257 ) 
  b_bias_corr_   ( min -113.772, max 151.364, mean -1.41796, stddev 13.8888, skewness 0.728022, kurtosis 23.2686 ) 
  b_peephole_i_c_corr_   ( min -108.366, max 178.642, mean -0.269632, stddev 13.8154, skewness 4.87071, kurtosis 98.8293 ) 
  b_peephole_f_c_corr_   ( min -228.962, max 130.13, mean -0.97999, stddev 22.5683, skewness -3.61138, kurtosis 40.9015 ) 
  b_peephole_o_c_corr_   ( min -207.681, max 115.758, mean -1.06922, stddev 29.6734, skewness -1.72209, kurtosis 14.1063 ) 
  b_w_r_m_corr_   ( min -82.1165, max 78.9306, mean 0.0144985, stddev 7.03086, skewness -0.0417983, kurtosis 4.49839 ) 

  ### Activations (mostly after non-linearities)
  YI_FW(0..1)^   ( min 0, max 1, mean 0.33103, stddev 0.349559, skewness 0.679466, kurtosis -1.00614 ) 
  YF_FW(0..1)^   ( min 0, max 1, mean 0.465676, stddev 0.387026, skewness 0.0372266, kurtosis -1.59275 ) 
  YO_FW(0..1)^   ( min 0, max 1, mean 0.277669, stddev 0.341011, skewness 1.00656, kurtosis -0.488549 ) 
  YG_FW(-1..1)   ( min -1, max 1, mean 0.0170264, stddev 0.757399, skewness -0.0290165, kurtosis -1.41587 ) 
  YC_FW(-R..R)*  ( min -50, max 50, mean 0.261186, stddev 10.1714, skewness 0.387752, kurtosis 17.0736 ) 
  YH_FW(-1..1)   ( min -1, max 1, mean 0.0224783, stddev 0.582156, skewness -0.0270748, kurtosis -0.613635 ) 
  YM_FW(-1..1)   ( min -1, max 1, mean -0.000601254, stddev 0.283237, skewness -0.140769, kurtosis 4.53512 ) 
  YR_FW(-R..R)   ( min -3.55464, max 3.87587, mean 0.00811019, stddev 0.616868, skewness 0.117516, kurtosis 2.7086 ) 
  ---
  YI_BW(0..1)^   ( min 0, max 1, mean 0.347454, stddev 0.34036, skewness 0.580251, kurtosis -1.08495 ) 
  YF_BW(0..1)^   ( min 0, max 1, mean 0.483598, stddev 0.36089, skewness -0.134818, kurtosis -1.42196 ) 
  YO_BW(0..1)^   ( min 0, max 1, mean 0.286854, stddev 0.342078, skewness 0.951533, kurtosis -0.598678 ) 
  YG_BW(-1..1)   ( min -1, max 1, mean 0.0100357, stddev 0.735961, skewness -0.0147635, kurtosis -1.3616 ) 
  YC_BW(-R..R)*  ( min -50, max 50, mean 0.689659, stddev 8.83679, skewness 1.31578, kurtosis 20.6775 ) 
  YH_BW(-1..1)   ( min -1, max 1, mean 0.0282058, stddev 0.598387, skewness -0.0144253, kurtosis -0.72298 ) 
  YM_BW(-1..1)   ( min -0.99998, max 0.999998, mean 0.0010925, stddev 0.29983, skewness -0.0559632, kurtosis 3.73499 ) 
  YR_BW(-R..R)   ( min -4.21807, max 4.21488, mean -0.0130297, stddev 0.669618, skewness -0.141598, kurtosis 3.30689 ) 

  ### Derivatives (w.r.t. inputs of non-linearities)
  DI_FW^  ( min -1, max 1, mean 8.70583e-05, stddev 0.0419649, skewness -0.20846, kurtosis 144.765 ) 
  DF_FW^  ( min -1, max 1, mean -0.000166824, stddev 0.0301676, skewness -0.168735, kurtosis 283.569 ) 
  DO_FW^  ( min -1, max 1, mean -8.35393e-06, stddev 0.0658101, skewness 0.159112, kurtosis 89.5019 ) 
  DG_FW   ( min -1, max 1, mean 0.000104801, stddev 0.0494567, skewness 0.0237317, kurtosis 211.138 ) 
  DC_FW*  ( min -15.1865, max 14.2763, mean 0.00225319, stddev 0.374072, skewness -0.342626, kurtosis 142.686 ) 
  DH_FW   ( min -11.5314, max 16.7872, mean 0.00074544, stddev 0.313096, skewness 0.709701, kurtosis 186.989 ) 
  DM_FW   ( min -13.4665, max 19.3761, mean 0.00110686, stddev 0.876861, skewness 0.0958656, kurtosis 34.3179 ) 
  DR_FW   ( min -3.0616, max 2.4822, mean -0.000112111, stddev 0.1774, skewness -0.115565, kurtosis 26.1657 ) 
  ---
  DI_BW^  ( min -0.545729, max 0.509649, mean -1.03483e-05, stddev 0.0140662, skewness -0.0758499, kurtosis 145.273 ) 
  DF_BW^  ( min -0.511368, max 0.508045, mean -8.15681e-05, stddev 0.011349, skewness 0.166488, kurtosis 142.695 ) 
  DO_BW^  ( min -0.361299, max 0.336964, mean 9.7713e-05, stddev 0.0157214, skewness -0.320199, kurtosis 41.2203 ) 
  DG_BW   ( min -0.792272, max 1, mean 0.00016992, stddev 0.0241851, skewness 2.30969, kurtosis 229.15 ) 
  DC_BW*  ( min -2.39329, max 4.5781, mean 0.000607194, stddev 0.131443, skewness 2.20345, kurtosis 92.542 ) 
  DH_BW   ( min -1.88856, max 1.71166, mean 6.63816e-05, stddev 0.0745798, skewness -0.526103, kurtosis 69.1585 ) 
  DM_BW   ( min -2.31305, max 2.31699, mean 5.89823e-05, stddev 0.197621, skewness 0.0471965, kurtosis 8.51898 ) 
  DR_BW   ( min -0.791316, max 0.739376, mean 9.02031e-05, stddev 0.0683215, skewness 0.0478568, kurtosis 5.40384 ) 
Component 2 : <Tanh>, 
Component 3 : <AffineTransform>, 
  linearity_grad ( min -209.634, max 223.331, mean -4.14761e-08, stddev 3.30401, skewness 0.95963, kurtosis 628.282 ) , lr-coef 1, max-norm 0
  bias_grad ( min -317.665, max 511.523, mean 1.66893e-07, stddev 19.0751, skewness 8.97519, kurtosis 496.87 ) , lr-coef 1
Component 4 : <Softmax>, 
### END GRADIENT

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:346) PER-CLASS PERFORMANCE:
@@@ Frames per-class: [ 181212 144 127 606 532 43 173 190 515 703 138 211 211 608 45 289 271 572 372 153 368 892 307 454 467 612 374 177 314 165 203 169 193 120 162 350 506 100 142 40889 89709 154 463 1777 286 231 687 352 545 459 79 394 481 237 200 663 1037 475 2210 428 257 167 4192 1910 93 230 151 223 4363 199 339 612 931 689 168 53 430 425 404 147 72 468 398 184 543 242 103 448 1376 60 779 717 668 146 541 163 358 830 561 408 249 268 103 1105 608 477 208 629 257 136 769 185 359 2082 165 69 303 74 247 336 172 147 117 128 443 612 117 125 52 316 956 165 305 574 202 297 222 1167 208 209 172 556 182 606 267 121 534 323 470 239 776 321 812 179 268 1139 537 191 554 5133 217 193 202 128 264 242 282 141 453 190 390 248 274 208 168 117 250 72 227 832 107 208 74 120 577 200 74 173 93 435 313 308 638 462 201 239 312 172 59 151 330 221 210 68 77 424 248 789 348 126 86 566 208 142 331 485 286 372 234 328 637 273 293 132 41 150 57 102 173 655 328 426 861 178 245 87 533 72 131 265 150 790 182 357 233 69 187 112 206 273 85 62 302 205 411 179 278 260 193 525 723 141 288 139 513 788 215 97 150 236 273 455 166 615 208 63 196 43 120 342 911 206 87 1156 192 146 131 166 31 221 252 417 373 499 401 233 179 409 182 439 579 96 189 251 1812 272 55 134 245 227 120 170 454 69 211 126 232 303 701 89 59 87 90 418 122 334 282 91 119 125 379 59 189 157 163 620 120 203 224 199 71 381 86 544 80 247 220 94 114 76 256 175 1186 420 378 336 135 140 331 666 306 356 196 203 291 152 221 543 214 149 360 365 670 264 264 692 216 616 923 524 408 234 395 301 375 105 525 80 359 98 456 204 309 258 288 286 181 90 101 263 53 253 167 354 150 174 194 393 579 494 437 273 142 236 159 226 115 606 198 255 508 136 282 304 327 234 1467 199 113 213 747 267 237 238 267 223 423 127 334 351 773 400 668 269 762 507 73 172 259 944 420 60 318 817 125 150 155 430 99 155 360 316 425 603 146 124 301 210 170 102 140 531 288 716 282 161 233 181 349 120 133 242 135 51 141 111 147 205 551 427 131 99 109 133 445 7274 204 271 135 128 125 377 120 372 449 808 108 557 45 280 47 596 57 276 400 121 347 315 193 77 146 96 105 216 193 341 1125 236 219 105 417 109 338 639 130 396 177 211 138 121 275 91 104 194 51 542 461 346 197 315 121 109 444 210 64 135 84 52 96 283 268 315 144 187 498 569 470 194 79 151 71 285 409 404 165 215 321 466 69 209 585 117 83 141 150 364 478 225 439 81 192 156 773 344 306 243 112 146 203 501 258 143 117 231 316 156 83 53 468 163 539 465 176 116 79 157 350 139 86 253 155 101 132 342 440 324 189 390 153 111 192 365 208 291 263 103 139 134 307 402 452 451 5943 88 406 111 189 488 786 548 86 632 99 382 216 141 365 388 65 90 187 430 300 463 604 247 79 314 305 463 45 473 718 330 223 226 1312 201 160 62 170 227 141 90 90 193 68 539 610 241 156 182 257 224 114 99 209 178 32 193 174 235 238 150 123 308 261 195 275 142 111 371 821 518 831 306 124 432 87 357 932 174 327 225 300 36 190 272 120 358 461 397 462 306 105 348 166 390 216 520 219 260 71 179 258 429 201 53 142 256 406 60 128 173 26 88 191 83 60 155 179 1000 125 44 191 230 383 687 216 257 321 233 417 202 1061 342 327 163 248 165 412 229 303 517 124 815 539 79 306 155 255 89 321 245 73 308 228 196 192 269 176 228 101 8591 469 337 966 237 430 177 258 334 85 304 414 487 401 57 178 99 260 178 151 248 261 926 126 248 230 264 241 147 220 155 139 646 219 209 393 1097 85 160 936 605 267 175 213 208 155 166 207 194 260 180 542 294 745 650 41 235 409 387 268 281 172 798 467 256 138 171 316 378 542 197 77 199 253 693 506 326 151 543 272 142 171 242 241 191 161 274 304 47 226 320 127 248 43 388 236 233 244 332 111 261 196 246 68 466 707 315 146 315 282 30 427 201 153 510 1026 278 186 316 45 464 106 316 404 415 543 346 257 452 1347 138 153 366 558 585 171 109 855 530 221 155 160 161 111 196 131 341 863 116 777 103 428 210 393 95 260 221 295 129 320 516 185 97 126 343 492 405 270 645 150 141 159 382 1153 426 252 535 332 205 417 575 597 292 372 180 236 213 343 301 154 349 528 290 243 378 616 213 266 230 676 309 338 165 324 314 639 310 148 365 184 119 116 199 264 1367 367 467 1442 213 598 641 335 156 122 351 114 129 98 489 213 162 386 116 74 274 197 265 173 129 167 753 211 214 1711 286 746 207 83 123 191 204 353 783 57 223 151 323 285 99 1197 111 116 189 130 401 488 428 96 280 248 468 506 117 284 315 328 129 334 124 557 265 243 383 268 66 97 474 153 221 139 391 194 634 156 322 748 309 195 326 651 462 370 172 385 490 392 610 304 249 70 501 324 335 228 370 206 587 560 108 850 256 209 153 168 206 131 204 232 340 200 284 174 301 124 107 220 67 370 1390 1253 311 434 127 136 104 493 416 778 103 335 338 242 66 208 459 231 96 291 160 220 368 145 728 458 351 255 350 348 137 125 1039 1466 285 221 232 361 605 549 4192 543 376 361 3105 243 319 117 225 232 542 309 859 113 244 913 235 271 853 881 252 243 274 129 90 371 274 176 453 107 172 140 172 411 267 778 147 558 2509 132 95 258 162 203 87 21 467 192 231 265 665 552 160 225 162 279 3235 297 303 235 317 297 155 692 494 82 179 192 734 178 103 204 87 225 63 351 225 260 282 352 367 219 77 182 384 386 147 340 273 260 234 61 251 141 663 194 320 94 144 ]
@@@ Loss per-class: [ 0.496713 3.56849 3.92249 2.81638 1.94293 7.20723 4.04213 3.03127 2.30715 3.1955 3.25037 4.3582 4.41994 2.76077 8.62804 2.39181 2.66447 3.08928 3.38606 4.47735 2.33915 2.11143 2.86651 2.17945 2.1594 2.50832 2.05324 5.01136 2.90653 4.25823 2.7409 3.34678 3.16383 5.27467 2.08097 3.44413 3.10569 4.78721 5.50944 1.99969 0.915982 3.25086 2.50248 2.69013 3.26919 2.42712 1.49112 2.88971 3.51656 3.03522 5.61319 2.56373 2.76131 3.71237 4.73861 2.56085 2.90443 1.98189 1.66936 2.38558 3.72151 3.12682 1.00504 0.537584 4.75582 2.78418 4.10188 3.5214 2.02346 2.79652 1.76368 2.69711 2.08996 2.48745 3.35675 7.24386 1.40859 0.995317 2.22504 3.93276 6.86858 3.10606 2.463 2.66051 3.12987 4.21563 5.42751 3.21025 2.70833 7.65699 3.02442 2.3736 1.95176 5.30198 2.74668 4.34352 2.69992 1.5587 1.55263 3.89867 4.32735 3.77982 4.99249 2.75942 1.67931 3.13677 4.21006 3.85261 3.31485 3.38746 3.54657 4.40012 4.4089 1.66152 4.65155 7.69718 3.02789 6.33902 4.7643 3.47124 4.51483 6.39981 5.9316 4.54994 2.53041 1.3566 6.92732 4.88761 8.78741 2.55143 1.51846 4.026 2.01989 1.97105 3.7046 3.29505 2.48935 1.11322 2.82078 3.83785 3.54246 1.59479 3.2642 1.97361 3.4413 5.87649 2.81477 4.077 2.87172 1.613 0.991941 2.96294 3.76055 4.87168 3.51157 1.50885 1.10209 3.15213 1.56728 1.05542 3.43412 3.55782 4.50104 6.72926 2.77467 3.18245 2.1748 4.36068 1.962 3.77983 1.9404 3.40103 3.39619 3.89823 5.03321 3.51397 2.12445 5.37779 4.78049 1.06677 4.15645 4.48431 5.69716 3.87987 1.97743 3.38293 8.85824 3.31156 6.04176 2.30382 2.80724 1.90112 1.33333 2.15538 3.57713 3.16302 3.27271 3.84968 5.9321 3.72643 3.69961 4.1948 4.10085 6.17934 6.1358 2.82889 3.8943 2.06632 2.65459 4.22368 6.48276 2.79222 2.13781 12.3366 2.95445 2.51877 2.89246 2.34634 3.07957 3.79361 2.1176 2.60985 3.94357 4.39802 7.35421 4.13969 8.05654 4.988 3.79137 2.89204 3.82443 1.93404 2.43763 4.53962 2.85412 6.85906 3.06962 6.65744 4.01246 4.25922 5.79171 1.46999 4.10558 2.56407 4.0683 6.95557 4.51259 7.23701 3.83251 3.83637 7.608 4.9063 3.55538 3.16662 3.17295 2.4833 3.83731 4.3395 3.14725 2.72161 2.0225 4.47141 3.28791 4.69267 3.43798 2.21025 3.47687 6.05869 3.40887 3.42552 2.40556 3.16787 5.14516 2.89622 4.55537 7.53074 4.41041 7.60678 3.58605 3.70062 2.66107 4.1657 5.67473 2.61099 3.30085 4.03472 5.52261 3.956 9.54749 4.10694 3.16365 3.98228 2.97334 3.73467 3.3806 4.17341 3.58731 2.9566 4.45584 3.96783 1.41793 5.32606 3.58672 1.98151 0.830339 3.44237 7.93957 4.67997 4.35565 4.9174 4.09265 3.5817 3.2125 5.62688 3.95478 4.80966 3.63188 3.32884 1.67 5.46049 8.0043 5.20792 7.25324 2.11154 6.01531 4.12064 3.54298 7.0805 4.97624 5.57844 2.63665 6.6624 4.55742 3.56918 3.57593 1.89228 7.17835 4.08887 2.95718 5.01377 5.80577 1.97742 6.18295 2.07926 4.64022 4.01446 3.90817 5.88769 6.27835 5.66514 3.56154 3.81324 2.61088 2.13007 2.42347 2.93601 3.35437 4.93856 4.09908 1.58641 3.3092 4.03813 4.15521 4.87256 4.44635 4.37048 2.48782 3.09153 2.70802 5.26674 2.77786 3.30648 1.39343 4.09733 2.603 3.16842 3.0714 2.47606 2.26586 2.9739 2.89945 3.12956 2.95895 4.03126 3.34793 5.16212 2.48396 5.14088 3.50865 5.20877 3.02177 4.15317 2.93176 3.23227 3.18269 3.07795 4.40201 5.73279 4.47147 2.42205 7.80634 2.58092 3.84993 3.47914 4.68981 3.85571 3.81621 3.04758 2.60204 3.81366 5.01228 5.31796 4.99684 4.15775 4.29725 2.84909 4.61235 1.52334 4.44743 2.54073 3.14462 4.95913 2.98847 2.61407 2.38529 3.57952 1.15792 2.17449 5.91538 2.63799 2.69313 4.06613 3.76977 3.45287 2.84141 3.55779 3.50446 4.24282 2.73246 3.03009 1.34331 3.46934 1.8582 3.1008 2.32954 1.81437 7.54608 4.03437 3.38286 0.91221 2.04794 5.52961 3.30682 2.98287 5.48828 9.23869 3.32503 3.2052 6.28573 4.94015 1.94061 2.46625 4.233 3.41315 4.7495 6.45162 5.08211 3.06757 3.65352 5.15931 2.8102 1.4849 4.04074 2.5009 3.09464 2.66527 2.33383 3.29798 3.08771 4.61295 7.25857 3.66962 4.90415 5.92562 4.69817 4.18771 4.73448 3.99038 1.57663 3.59453 4.0469 4.63401 5.47759 4.36321 2.25602 1.21003 2.33501 3.79703 5.17388 5.05832 4.40245 3.57159 4.38172 3.90161 2.77366 3.70295 7.21516 1.36973 8.36672 2.78902 8.11608 3.94146 5.05175 3.01287 2.606 4.36498 4.055 1.74737 5.09002 5.41038 6.02333 5.47731 4.22872 6.33351 4.907 3.1198 2.24699 3.93708 3.14631 5.45706 3.95758 5.68428 2.89898 1.8265 4.98338 3.20396 3.37867 3.09109 4.24353 4.45892 3.52439 6.10451 6.65086 3.6972 8.24596 1.5888 1.77431 2.89922 2.52087 3.64136 4.53787 6.55683 2.63153 4.73615 4.63445 3.19058 6.79017 6.40152 5.56884 4.19864 5.68064 4.81296 5.12162 4.21238 1.78983 2.26734 1.83734 2.71209 6.07635 4.54075 6.99891 2.95739 3.08217 2.28582 4.98281 4.26448 2.62137 2.11322 6.39624 3.45476 4.02977 5.97925 6.33125 5.28998 4.62014 3.15294 4.13386 3.35128 4.12925 5.78606 4.53703 4.2469 2.78987 2.09909 4.32256 3.91355 5.63549 4.10394 4.47141 2.24404 4.09925 5.0367 5.14245 4.48889 2.81389 4.95938 6.15418 7.28775 2.39125 4.09981 2.67008 2.08968 3.65201 5.64451 7.49312 4.22833 5.0622 5.57193 4.64209 4.28534 3.63652 4.7379 4.61518 2.71469 2.98903 3.0669 3.93723 4.23459 4.43486 5.13085 5.3633 4.67357 4.27089 3.68683 2.09808 6.13605 4.09562 3.82268 3.95115 3.31144 3.02656 1.93497 1.83694 4.91334 2.40068 4.64076 3.95293 2.31883 2.03708 2.85747 8.88424 1.26035 7.53678 4.23821 4.55524 5.05776 4.30175 2.87179 9.14939 5.24362 4.27802 2.58456 2.34617 2.66583 1.52243 4.77642 6.74954 3.67004 3.43899 4.17426 9.696 2.42892 2.44171 3.28533 5.22332 5.04812 3.11779 4.47333 4.738 9.43067 4.19191 4.67925 3.63442 5.71819 7.52984 3.51243 7.25754 2.52254 2.12456 4.47381 4.15583 3.00864 4.57 3.6858 6.09942 6.69368 4.22811 5.98857 6.84748 4.22798 4.60051 2.96723 3.62007 3.95675 4.89827 3.06958 2.93509 4.86441 2.58761 4.18021 7.00959 2.19728 1.64452 3.0535 1.84748 2.91657 4.86889 3.20661 6.00645 4.96157 3.47226 5.64134 3.30537 5.07476 3.60532 9.24124 3.79677 4.62732 5.22933 1.64924 3.25149 3.70473 2.4637 4.31406 5.49302 3.4294 5.26729 2.64945 3.55632 2.71772 3.13461 4.31975 6.16364 4.37369 2.81672 2.98911 3.93753 9.0023 3.00442 2.61571 3.15406 6.24724 5.04497 3.73895 6.0009 5.15368 3.75981 7.76598 6.32253 3.80111 4.50572 2.45165 5.03802 7.37525 4.79106 4.91508 4.05523 2.19324 4.83172 3.09292 3.01402 3.17284 2.97688 3.21835 3.14006 3.50823 3.8144 6.0843 5.27898 4.82623 1.96037 3.38114 3.63872 2.94199 6.02473 1.35576 3.14805 5.16714 3.89708 3.04614 2.49948 6.19663 3.69301 3.7693 6.256 3.44357 3.62726 5.09676 4.14167 3.41846 4.91483 7.03831 7.08665 0.553672 2.88388 4.90876 1.30909 3.93474 3.55017 5.1426 2.34334 3.4047 7.35067 2.81973 3.27014 2.46384 3.22984 7.59354 5.26529 5.8877 1.57364 4.40666 3.75904 4.52715 4.36083 2.42043 5.81909 3.98197 3.94721 4.28743 4.21416 5.72709 4.91538 3.62129 4.44471 2.54104 4.19963 5.18633 4.3644 2.60476 6.19729 3.61421 1.50936 2.51677 2.04814 3.01615 3.70558 4.38019 4.60698 4.19135 4.06941 4.68476 5.1417 4.2563 2.7586 3.29831 2.65048 3.4038 8.69249 3.46044 3.65595 2.81646 3.87726 3.88821 5.11976 2.51779 2.7054 3.84834 6.7772 3.0011 3.49518 3.1381 2.75619 4.71507 6.5502 5.78521 2.96451 1.80173 2.70962 3.19824 3.45443 1.94571 2.30849 5.91913 5.20553 3.9707 4.12775 4.00541 3.24601 3.29276 2.6655 7.48608 3.45005 2.64678 5.18105 2.06438 7.44266 2.54488 3.51609 3.6408 4.59353 3.1982 4.80305 4.16736 4.50933 3.62261 7.02669 1.67749 1.83652 2.44635 6.64439 2.11363 2.48287 9.61416 2.93265 4.38481 3.87766 3.32318 3.19756 3.53604 5.48326 3.00965 8.51095 3.10842 4.70932 3.26324 2.39447 2.94346 3.38507 3.71168 3.76441 2.05704 2.2263 4.64757 5.96685 5.09765 3.49182 2.91435 4.24131 5.36701 2.30687 3.73308 3.53372 4.30728 3.83312 3.96594 5.98508 2.89587 4.83966 3.09036 2.08962 5.19162 2.50563 4.90768 2.90185 1.77309 2.5492 4.97972 4.07016 3.12515 5.56162 5.79037 4.40171 3.46416 4.16616 5.17857 6.9191 4.53456 2.05532 2.20381 3.85285 2.27497 3.69313 5.73322 5.27788 4.3033 3.75155 3.46484 4.32637 3.20774 2.08922 4.74155 1.7362 2.41544 3.98251 3.68839 3.42421 3.99193 3.98438 5.14622 3.25744 3.75706 4.22874 3.23691 2.8656 3.09347 3.9038 2.47101 3.19816 2.98712 3.69667 3.59104 2.17524 3.21285 2.93371 3.58071 3.20469 2.73576 3.34404 3.9916 6.34045 3.34643 4.027 3.39088 5.39699 4.93015 3.51894 2.67005 4.06854 3.03365 0.905698 2.64398 3.41699 1.56976 3.28685 2.8134 5.29489 3.42582 4.88222 6.23434 5.91224 3.12573 3.78485 4.76185 2.97304 5.40362 6.56613 3.46367 4.49792 4.59582 5.12868 3.98753 3.90776 3.91673 3.59592 5.74022 1.27883 4.94286 1.69074 4.87528 5.83281 6.12691 4.82717 4.0242 3.26066 1.56651 6.78888 3.11244 4.77102 3.29869 3.24248 5.74455 2.07745 5.66724 4.17535 3.36847 5.12102 2.14788 2.67688 3.41167 6.35591 2.99616 4.22874 2.9627 2.25664 4.03259 3.97189 4.04356 3.2594 5.30809 3.48306 5.36649 4.30484 4.84467 3.95064 2.07274 2.85977 7.09383 6.87019 2.98381 4.85792 4.41781 4.68432 2.81266 3.05033 2.67955 5.52801 3.24542 2.79864 3.66834 5.60462 2.81165 2.48793 3.79868 3.05649 3.58446 2.27886 3.10661 2.72371 2.19187 3.97871 5.1325 6.92535 3.54363 3.30934 4.7351 4.08919 2.74666 5.38377 2.66184 2.234 5.23413 2.75819 3.8507 3.44786 3.66543 3.65399 4.11301 5.86636 4.03143 4.0919 2.87732 4.20757 2.60463 5.04372 2.7458 5.22488 6.55952 4.00561 6.50715 3.02625 1.40307 1.44058 2.43973 3.59949 6.03682 5.08351 4.96625 2.80902 3.31823 1.73859 4.80686 4.47032 3.69099 2.85882 8.25397 3.42031 2.82948 4.28867 5.40295 3.15798 4.83409 2.17534 4.12371 4.43303 2.57466 2.01576 3.02713 4.38806 4.25596 2.21103 5.19395 3.97673 1.90555 1.51319 4.02808 4.68235 3.93801 2.93517 2.19692 2.31738 1.35246 3.24294 3.4439 2.43335 1.47396 3.82545 4.02122 4.85606 4.21346 4.18252 3.15615 4.24445 1.90943 5.98957 4.46244 1.97493 4.21592 3.17149 3.38106 2.18528 3.08527 5.13595 4.11722 3.32782 5.38487 3.91617 3.97193 4.80996 2.55358 5.71578 3.8659 5.3542 3.01359 2.72111 4.46606 2.81919 4.81255 2.16208 2.1699 5.12681 5.19467 4.53542 3.55761 4.56616 6.79222 10.4755 3.27777 3.21214 5.46088 4.57154 3.71312 4.39216 4.43888 4.4086 4.33359 4.2885 1.66928 3.24491 3.59882 3.3226 4.22362 4.82611 5.49819 2.42602 3.1755 6.26688 6.10755 4.09043 2.45465 3.69279 6.9234 3.90073 6.12142 3.48609 6.74825 3.00723 3.87733 4.02031 4.5075 3.12359 4.34773 2.24408 7.6535 2.89016 3.31796 2.43879 5.84914 2.95771 3.01791 4.56873 4.95522 6.06938 4.08093 6.01169 3.05856 3.20957 4.08191 4.28868 5.12046 ]
@@@ Frame-accuracy per-class: [ 81.1589 16.609 5.4902 32.4815 57.6526 0 7.4928 11.0236 31.2318 15.6361 13.7184 3.78251 5.67376 17.4199 0 53.5406 37.5691 21.6594 10.4698 0 30.9362 35.8543 35.122 50.165 57.3262 25.9592 68.6248 0 30.2067 7.25076 17.6904 20.059 16.0207 0 51.6923 9.12981 10.464 0 0 28.0463 62.6065 9.06149 10.5717 23.5724 15.3578 44.0605 62.6909 25.8156 4.21632 25.8977 0 43.853 27.2066 5.05263 0.498753 37.5283 13.9759 36.8034 58.9912 42.9405 10.4854 19.1045 73.6792 86.3648 2.13904 10.4121 3.9604 14.7651 37.8824 17.0426 58.9102 25.7959 41.0091 32.3423 17.2107 0 67.1312 81.7861 39.0606 10.8475 0 26.4674 38.6449 28.7263 11.9595 1.23711 2.89855 30.7692 30.7301 0 7.18409 41.5331 56.0957 2.04778 35.4571 1.22324 31.2413 49.4883 62.6892 8.81273 4.00802 10.0559 0 20.8051 62.2843 17.1728 9.11271 6.19539 3.49515 24.9084 7.1475 4.31267 4.17246 57.8631 1.20846 0 16.4745 0 2.0202 28.8262 4.63768 0 0.851064 0 38.5569 59.9184 0 1.59363 0 32.2275 58.1286 4.83384 41.5712 46.6493 19.2593 17.8151 47.6404 76.1456 28.777 19.0931 2.89855 56.0647 26.3014 44.6826 13.8318 0 38.1665 7.41886 18.2784 58.8727 77.9137 25.5054 3.44615 0 27.1881 63.7122 77.3953 18.799 68.3499 71.2964 6.89655 16.0207 6.91358 0 31.38 25.567 52.3894 4.24028 51.1577 0.524934 46.863 28.9738 18.5792 7.19424 6.52819 3.40426 53.8922 0 9.23077 69.3093 1.86047 8.15348 0 22.4066 45.5411 3.99002 0 19.5965 0 36.2801 34.4498 57.6985 75.8027 50.8108 12.9032 12.5261 16 17.971 0 11.2211 12.708 4.96614 3.32542 0 0 37.2203 8.85312 57.3781 51.363 3.95257 0 22.5949 39.3285 0 29.5626 32.5438 12.2164 47.7852 17.9104 13.6986 38.5882 21.9378 20.7836 8.30189 0 2.65781 0 2.92683 19.5965 28.9855 16.7428 49.238 34.9391 6.72269 32.5866 0 18.7441 0 0.760456 12.0527 0 46.9323 3.28767 30.4895 7.70878 0 7.46667 0 10.1695 20.1097 0 0 26.7769 23.8443 17.983 38.9972 14.7217 6.90979 12.9199 41.1037 53.0753 3.53357 29.4627 7.8853 6.42648 51.7438 19.9536 0 13.289 24.1015 41.3163 12.7333 0 28.4322 5.7554 0 8.14249 0 7.46888 14.8905 39.1662 7.74818 0 33.5495 13.5065 17.0648 0.760456 3.6036 0 9.93228 11.8812 18.2036 18.7416 15.2152 25.1557 12.848 13.9276 27.3504 4.93151 13.4243 65.9189 2.07254 18.9974 51.6899 88.0552 20.1835 0 8.17844 2.85132 5.71429 9.12863 18.1818 15.4015 0 6.61939 3.16206 13.3333 16.804 59.0164 0 0 0 0 44.6834 0 3.8864 15.9292 0 1.67364 1.59363 38.2082 0 10.0264 3.1746 14.0673 62.6914 0 19.656 20.0445 5.01253 0 56.8807 0 37.4656 0 12.1212 8.61678 0 0 0 7.01754 15.3846 20.5647 45.6599 50.1982 25.5572 10.3321 8.54093 9.04977 63.0158 21.5334 11.5007 10.687 5.89681 4.45969 1.31148 41.9865 31.6467 36.3636 1.33779 20.8044 27.6334 65.0261 18.5255 19.6597 17.0397 34.1801 35.1987 49.5939 24.5949 27.1726 16.6311 37.421 8.29187 15.1798 0 40.7231 0 24.4784 0 19.7152 8.31296 28.7561 6.96325 15.5979 20.5934 3.30579 0 9.85222 28.8425 0 50.4931 13.7313 11.8477 3.32226 13.7536 7.19794 22.1093 42.2778 6.47118 5.02857 4.02194 5.61404 7.18816 6.26959 20.7506 4.329 65.2927 3.52645 48.5323 28.1219 0 35.3982 33.8259 47.0229 22.1748 76.3884 54.1353 0 29.5082 33.4448 13.4579 21.8947 12.1593 32.5234 11.6331 3.77804 8.62745 27.2048 32.4324 65.4169 22.2222 40.6881 39.7032 45.1148 64.6305 0 15.0725 23.1214 74.6427 47.0868 0 27.3155 15.1682 1.59363 0 10.2894 16.7247 0 1.28617 52.1498 37.5987 1.64512 10.6048 0.682594 0 0 20.4276 13.4897 0 40.5694 68.2973 6.58579 45.2198 18.4071 22.9102 53.1049 9.36639 12.5894 10.7884 0 22.268 1.47601 0 4.947 11.6592 4.0678 11.1922 72.8921 17.3099 2.28137 2.01005 0.913242 5.24345 43.9955 59.7704 42.0538 19.5212 0 0 3.18725 12.7152 0 15.302 43.604 9.52381 0 69.2377 0 28.877 0 9.05281 0 23.1465 40.4494 7.40741 6.33094 58.0032 0 0 0.682594 2.07254 3.79147 0 1.55039 9.37042 40.2488 10.148 20.5011 2.8436 6.70659 0 24.8154 48.0063 0.766284 34.0479 24.2254 20.8038 3.61011 0 7.6225 0 0 10.2828 0 56.9585 60.0217 13.5642 64.3038 15.8479 1.64609 0 48.8189 0 0 22.8782 0 0 0 11.9929 0 3.48653 1.38408 1.6 57.1715 43.8982 59.9362 35.9897 0 2.64026 0 26.62 14.1636 45.241 1.20846 6.49652 30.4821 45.0161 0 19.5704 3.75747 0 0 2.12014 1.3289 26.8861 0 15.9645 8.19113 0 2.5974 14.0575 35.1648 53.1205 11.7455 6.16016 1.77778 11.6041 16.2162 33.4995 5.41586 0 0 6.47948 23.3807 1.91693 0 0 36.7129 2.44648 51.3438 53.4909 11.3314 0 0 8.25397 3.99429 1.43369 0 4.33925 9.00322 0.985222 8.30189 28.9051 13.6209 22.4961 12.6649 6.14597 5.86319 0.896861 0 9.57592 3.83693 19.554 39.4687 0 10.0358 2.97398 8.13008 17.1429 13.0387 51.3843 35.5346 0 34.1943 1.79372 12.1372 48.9253 45.0095 25.7065 0 68.6166 0 13.5948 3.69515 0 15.0479 19.0476 0 2.20994 5.86667 34.8432 33.9434 26.5372 65.8395 0.808081 0 11.4467 28.1506 5.39374 0 40.9715 32.5678 24.8109 1.78971 5.29801 14.019 5.45906 2.49221 0 12.3167 3.51648 14.841 0 0 9.30233 0 41.7053 50.6143 2.89855 3.83387 21.9178 3.49515 8.46325 0 0 4.29594 0 0 3.10078 9.74212 40.3397 9.22432 7.97342 0 14.5867 40.9178 3.06905 34.4828 2.80702 0 45.4913 58.0645 21.7936 56.2838 39.478 0 27.9769 0 2.51748 13.1903 1.14613 23.8168 0 11.6473 0 16.7979 4.40367 0.829876 57.7406 19.2849 13.0818 36.5405 14.0294 0 27.2597 0 34.315 15.7044 29.2027 20.9567 9.59693 0 7.24234 29.0135 25.1455 7.94045 0 19.6491 53.8012 27.0603 0 3.89105 12.6801 0 3.38983 8.87728 0 0 5.78778 10.0279 37.0815 0 0 0 0.433839 20.0782 38.6909 5.08083 29.9029 25.8165 23.5546 28.024 16.7901 18.5586 18.3942 12.2137 0 0.804829 0 48.2424 22.2222 17.4629 34.2029 1.60643 74.9234 24.4671 1.25786 12.0718 23.1511 27.0059 0 3.42146 12.22 0 28.8493 15.3173 1.52672 19.2208 17.4397 2.83286 0 0 86.0734 30.2449 0 74.7025 13.4737 18.8153 1.12676 32.4952 9.86547 0 28.5714 13.0277 42.0513 21.9178 0 2.80112 0 59.501 1.12045 4.62046 6.43863 6.50096 34.5386 0.790514 9.65795 11.2798 5.67108 6.62526 1.35593 0.907029 12.2186 11.4695 43.6195 4.10023 0 6.8615 36.6287 0 3.11526 62.5734 44.5912 32.5234 27.9202 13.1148 10.0719 0 6.60661 11.0843 8.74036 1.15163 7.20222 32.6267 23.4295 33.4004 13.5281 0 7.21868 13.9194 21.4194 5.95903 16.341 0.57971 39.449 37.861 14.8148 0 29.1545 17.3776 10.3038 34.47 0 0 0 17.7515 57.3901 32.5765 21.4395 7.26073 61.4535 43.3028 0 1.74927 9.48454 9.93789 9.39948 16.7183 31.694 32.1839 0 24.2826 45.2418 0 45.0704 0 47.1042 14.3763 15.8458 3.27198 30.0752 0 8.79541 2.03562 11.7647 0 63.0225 49.7527 39.3027 0 56.1014 35.3982 0 24.5614 5.45906 9.12052 11.5573 13.5412 22.2621 2.14477 36.9668 0 28.8482 8.4507 25.5924 50.4326 24.5487 26.4949 17.0274 6.21359 53.7017 41.9295 0 0 1.90996 21.6652 31.4261 9.91254 0.913242 47.6914 15.8341 16.2528 5.14469 1.24611 3.71517 0 20.8651 0 14.9341 51.0712 0 30.8682 5.7971 33.3722 62.2328 47.5222 4.18848 14.5873 17.1558 0 0 8.73635 17.0378 6.469 0 0 5.82242 59.0863 45.1295 4.43623 47.0953 7.97342 0 0.626959 10.9804 2.86086 17.1161 2.37624 18.1139 43.3083 7.78589 30.1796 37.8801 8.70293 7.86325 6.1745 4.98615 4.65116 5.15222 8.73362 16.2521 7.76699 28.3262 23.0842 34.4234 9.44559 40.1585 27.8994 24.356 15.3846 20.3905 37.2506 21.9709 18.9069 12.0846 26.5023 27.345 17.0446 15.1369 0 19.4254 14.6341 7.53138 0 8.02005 25.3308 26.5448 6.2585 40 72.721 29.0398 15.3718 62.3539 18.7779 20.4473 3.26531 27.027 2.62009 0 0 25.5363 13.1148 1.84615 33.1177 0 0 16.7577 5.06329 3.76648 0 4.6332 5.37313 14.2004 7.0922 0.4662 71.9836 3.14136 56.2626 0.963855 0 0 3.13316 16.1369 29.1372 61.0083 0 9.8434 5.28053 12.983 16.1121 0 50.8559 0.896861 9.44206 18.4697 0 54.0473 47.4923 11.4352 0 39.9287 9.25553 29.8826 37.9072 0.851064 5.6239 12.0444 23.1355 0.772201 19.133 0 10.0448 7.15631 11.9097 39.3742 5.58659 0 0 33.7197 0 3.61174 0 27.5862 27.7635 46.1781 1.27796 18.9147 33.6673 9.36995 0 15.0077 32.6938 15.5676 34.278 10.4348 34.7601 20.999 35.9236 49.7952 15.1067 0.801603 0 18.1456 25.5778 8.94188 10.9409 39.946 0 31.6596 38.3586 1.84332 38.6831 2.72904 10.5012 7.16612 16.0237 1.45278 0 12.2249 6.02151 41.116 2.99252 30.58 3.4384 18.5738 0 0 4.98866 0 39.6761 65.3003 65.1775 27.6083 7.59494 0 0.732601 0 28.5714 17.527 66.6667 0.966184 5.66319 7.97637 21.8557 0 11.9904 33.9499 8.20734 2.07254 31.5609 1.24611 40.8163 7.86974 5.49828 38.8469 49.9455 19.3457 4.69667 9.70043 51.076 0 5.57769 55.2189 63.7572 11.2084 0 12.9032 28.2158 53.3443 47.8617 67.2868 13.4315 14.3426 36.7911 51.2961 18.8912 12.8326 8.51064 7.5388 2.58065 15.6682 2.58481 31.2973 0 9.81595 44.6634 0.849257 24.6777 30.8143 45.4906 24.1584 3.28542 16.7577 14.6718 0 17.7658 15.3005 6.79887 37.9272 0 15.6522 0.711744 13.3333 31.1057 6.72897 31.2139 2.0339 54.4315 38.8524 0 0 4.25532 20.3077 1.4742 0 0 26.3102 18.7013 1.72786 1.88324 10.9692 0 0.623053 0.886918 3.69231 3.57782 50.2241 24.5378 14.4975 30.1486 20.1575 2.01681 1.28617 49.6751 15.9757 0 1.11421 7.79221 37.3043 10.084 0 6.84597 0 16.8514 0 27.027 20.8426 13.8196 4.95575 20.1418 18.2313 56.0364 0 41.0959 21.5865 41.3972 0 30.5433 15.3565 1.53551 0.426439 0 15.1093 0 32.4039 22.6221 9.9844 5.29101 5.53633 ]

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:347) Done 2622 files, 1 with no tgt_mats, 0 with other errors. [TRAINING, 0.328675 min, fps38288.2]
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:353) AvgLoss: 2.08711 (Xent), [AvgXent: 2.08711, AvgTargetEnt: 0]
progress: []
FRAME_ACCURACY >> 46.7197% <<

