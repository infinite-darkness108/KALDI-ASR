speech-HP-Z2-Tower-G9
nnet-train-multistream-perutt --cross-validate=false --randomize=true --verbose=0 --num-streams=10 --max-frames=15000 --learn-rate=0.00004 --momentum=0.9 --l1-penalty=0 --l2-penalty=0 --feature-transform=exp_FG/blstm4i/final.feature_transform 'ark:copy-feats scp:exp_FG/blstm4i/train.scp ark:- | apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_tr90/utt2spk scp:data-fbank/train_tr90/cmvn.scp ark:- ark:- | add-deltas --delta-order=2 ark:- ark:- |' 'ark:ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- |' exp_FG/blstm4i/nnet.init exp_FG/blstm4i/nnet/nnet_iter01 
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:243) Not in compute-exclusive mode.  Suggestion: use 'nvidia-smi -c 3' to set compute exclusive mode
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:438) Selecting from 1 GPUs
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:453) cudaSetDevice(0): NVIDIA RTX A2000 12GB	free:11515M, used:515M, total:12031M, free/total:0.957153
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:501) Device: 0, mem_ratio: 0.957153
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:382) Trying to select device: 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:511) Success selecting device 0 free mem ratio: 0.957153
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:FinalizeActiveGpu():cu-device.cc:338) The active GPU is [0]: NVIDIA RTX A2000 12GB	free:11001M, used:1029M, total:12031M, free/total:0.91443 version 8.6
copy-feats scp:exp_FG/blstm4i/train.scp ark:- 
apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_tr90/utt2spk scp:data-fbank/train_tr90/cmvn.scp ark:- ark:- 
add-deltas --delta-order=2 ark:- ark:- 
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:144) TRAINING STARTED
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Read():nnet/nnet-matrix-buffer.h:191) Read() started... Buffer size in MB: 0, max 3072, having 0 utterances.
ali-to-post ark:- ark:- 
ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl 'ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |' ark:- 
LOG (copy-feats[5.5.1074~1-71f3]:main():copy-feats.cc:143) Copied 2624 feature matrices.
LOG (apply-cmvn[5.5.1074~1-71f3]:main():apply-cmvn.cc:159) Applied cepstral mean and variance normalization to 2624 utterances, errors on 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:303) ### After 0 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:304) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.108179, max 0.105792, mean 8.14022e-05, stddev 0.0578205, skewness -0.00359111, kurtosis -1.20611 ) 
  f_w_gifo_r_   ( min -0.104486, max 0.10561, mean -0.000104259, stddev 0.0576986, skewness 0.00129511, kurtosis -1.20061 ) 
  f_bias_   ( min -0.100496, max 1.10059, mean 0.250223, stddev 0.437581, skewness 1.12304, kurtosis -0.646699 ) 
  f_peephole_i_c_   ( min -0.0996697, max 0.0979258, mean -0.00204559, stddev 0.056738, skewness 0.0645136, kurtosis -1.1159 ) 
  f_peephole_f_c_   ( min -0.0991133, max 0.100656, mean -0.000110465, stddev 0.0587388, skewness -0.072794, kurtosis -1.20934 ) 
  f_peephole_o_c_   ( min -0.10136, max 0.0996185, mean -0.00256945, stddev 0.0565198, skewness -0.056557, kurtosis -1.16111 ) 
  f_w_r_m_   ( min -0.103447, max 0.104455, mean -6.98271e-05, stddev 0.0576989, skewness -0.00031586, kurtosis -1.19511 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -0.107682, max 0.107465, mean 0.000169364, stddev 0.0576952, skewness -0.00279596, kurtosis -1.19752 ) 
  b_w_gifo_r_   ( min -0.104808, max 0.102433, mean -0.000119333, stddev 0.0577553, skewness 0.00185041, kurtosis -1.19979 ) 
  b_bias_   ( min -0.105989, max 1.09878, mean 0.247404, stddev 0.436473, skewness 1.12365, kurtosis -0.644442 ) 
  b_peephole_i_c_   ( min -0.0985702, max 0.10009, mean 0.000864342, stddev 0.0588601, skewness -0.00267858, kurtosis -1.23055 ) 
  b_peephole_f_c_   ( min -0.0996381, max 0.0979905, mean 0.000387941, stddev 0.0602906, skewness 0.0142457, kurtosis -1.32486 ) 
  b_peephole_o_c_   ( min -0.0981826, max 0.100309, mean 0.00129225, stddev 0.0540308, skewness -0.190961, kurtosis -0.976583 ) 
  b_w_r_m_   ( min -0.103946, max 0.104378, mean -0.000334432, stddev 0.0577485, skewness 0.00418233, kurtosis -1.19883 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.49303, max 0.460507, mean -0.000155926, stddev 0.0999982, skewness 0.000777128, kurtosis 0.0159314 ) , lr-coef 1, max-norm 0
  bias ( min -0.00042783, max 0.0355726, mean -2.03727e-11, stddev 0.00110282, skewness 28.0218, kurtosis 863.546 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:305) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -8.67671, max 9.02116, mean 0.0321104, stddev 1.01793, skewness 0.450612, kurtosis 3.20408 ) 
[1] output of <BlstmProjected> ( min -1.48807, max 1.43847, mean 0.00074366, stddev 0.279124, skewness 0.000799188, kurtosis 0.233962 ) 
[2] output of <Tanh> ( min -0.90297, max 0.893389, mean 0.000696214, stddev 0.259417, skewness -0.00299404, kurtosis -0.29501 ) 
[3] output of <AffineTransform> ( min -3.39727, max 3.53679, mean -0.00372672, stddev 0.65808, skewness 0.0055375, kurtosis 0.219006 ) 
[4] output of <Softmax> ( min 1.96184e-05, max 0.0196324, mean 0.000781275, stddev 0.000586189, skewness 3.16656, kurtosis 23.6788 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:307) 
### BACKWARD PROPAGATION BUFFER CONTENT :
[0] diff of <Input>  ( min -0.644541, max 0.571124, mean 0.0016329, stddev 0.113613, skewness 0.00647362, kurtosis 0.974916 ) 
[1] diff-output of <BlstmProjected> ( min -0.456074, max 0.446359, mean -0.00176928, stddev 0.093352, skewness 0.0366998, kurtosis 0.112628 ) 
[2] diff-output of <Tanh> ( min -0.457525, max 0.446753, mean -0.00197683, stddev 0.0996414, skewness 0.0323641, kurtosis 0.0259376 ) 
[3] diff-output of <AffineTransform> ( min -0.999949, max 0.0196324, mean 6.1133e-10, stddev 0.027909, skewness -35.7942, kurtosis 1278.58 ) 
[4] diff-output of <Softmax> ( min -0.999949, max 0.0196324, mean 6.1133e-10, stddev 0.027909, skewness -35.7942, kurtosis 1278.58 ) 
### END BACKWARD


LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:308) 
### GRADIENT STATS :
Component 1 : <BlstmProjected>, ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  ### Gradients 
  f_w_gifo_x_corr_   ( min -250, max 250, mean 0.617164, stddev 19.472, skewness -0.0948115, kurtosis 38.5588 ) 
  f_w_gifo_r_corr_   ( min -181.242, max 229.739, mean -0.0190505, stddev 8.98889, skewness 0.0320462, kurtosis 42.4602 ) 
  f_bias_corr_   ( min -250, max 250, mean -1.29525, stddev 41.6493, skewness 0.273438, kurtosis 11.0887 ) 
  f_peephole_i_c_corr_   ( min -21.1022, max 29.4056, mean -0.839427, stddev 6.41018, skewness 0.401286, kurtosis 2.42236 ) 
  f_peephole_f_c_corr_   ( min -32.1396, max 45.4594, mean -1.35987, stddev 10.7209, skewness 0.391101, kurtosis 1.76998 ) 
  f_peephole_o_c_corr_   ( min -58.5804, max 54.4007, mean -2.26395, stddev 12.7892, skewness -0.229836, kurtosis 3.51696 ) 
  f_w_r_m_corr_   ( min -175.907, max 177.206, mean -0.00790503, stddev 27.9907, skewness 0.00269587, kurtosis 3.02508 ) 
  ---
  b_w_gifo_x_corr_   ( min -250, max 250, mean 0.118702, stddev 21.5741, skewness 0.300819, kurtosis 39.2285 ) 
  b_w_gifo_r_corr_   ( min -165.003, max 189.957, mean -0.00761295, stddev 8.99345, skewness -0.0415471, kurtosis 36.7957 ) 
  b_bias_corr_   ( min -250, max 250, mean 0.218227, stddev 46.364, skewness -0.322563, kurtosis 9.82639 ) 
  b_peephole_i_c_corr_   ( min -17.682, max 33.6301, mean 0.293666, stddev 6.39686, skewness 0.931491, kurtosis 4.2423 ) 
  b_peephole_f_c_corr_   ( min -25.4129, max 47.9114, mean 0.400182, stddev 10.0831, skewness 0.819481, kurtosis 3.19783 ) 
  b_peephole_o_c_corr_   ( min -45.2631, max 75.1838, mean 0.877916, stddev 13.2503, skewness 1.37634, kurtosis 7.28626 ) 
  b_w_r_m_corr_   ( min -194.389, max 206.225, mean -0.166267, stddev 27.0542, skewness -0.0188876, kurtosis 3.56558 ) 

  ### Activations (mostly after non-linearities)
  YI_FW(0..1)^   ( min 0, max 0.994014, mean 0.497318, stddev 0.141165, skewness -0.233927, kurtosis 0.78626 ) 
  YF_FW(0..1)^   ( min 0, max 0.997495, mean 0.713115, stddev 0.128606, skewness -1.75089, kurtosis 6.45029 ) 
  YO_FW(0..1)^   ( min 0, max 0.992317, mean 0.497659, stddev 0.139285, skewness -0.22068, kurtosis 0.764567 ) 
  YG_FW(-1..1)   ( min -0.999741, max 0.999702, mean 0.00270397, stddev 0.454335, skewness 0.00282321, kurtosis -0.789317 ) 
  YC_FW(-R..R)*  ( min -5.04012, max 7.21284, mean -3.93692e-05, stddev 0.751761, skewness 0.055479, kurtosis 0.945591 ) 
  YH_FW(-1..1)   ( min -0.999916, max 0.999999, mean -0.00119885, stddev 0.528956, skewness 0.00492657, kurtosis -1.09639 ) 
  YM_FW(-1..1)   ( min -0.967923, max 0.953432, mean 0.000434768, stddev 0.275988, skewness 0.008913, kurtosis -0.541484 ) 
  YR_FW(-R..R)   ( min -1.35131, max 1.3104, mean -0.000271511, stddev 0.283393, skewness 0.00807983, kurtosis 0.161797 ) 
  ---
  YI_BW(0..1)^   ( min 0, max 0.991707, mean 0.498135, stddev 0.137894, skewness -0.249143, kurtosis 0.923976 ) 
  YF_BW(0..1)^   ( min 0, max 0.997069, mean 0.712659, stddev 0.126531, skewness -1.78842, kurtosis 6.82143 ) 
  YO_BW(0..1)^   ( min 0, max 0.992846, mean 0.496785, stddev 0.141955, skewness -0.182925, kurtosis 0.759864 ) 
  YG_BW(-1..1)   ( min -0.999942, max 0.999907, mean -0.00443688, stddev 0.444426, skewness -0.0235931, kurtosis -0.69311 ) 
  YC_BW(-R..R)*  ( min -5.47414, max 10.9294, mean 0.000145826, stddev 0.730116, skewness 0.0438429, kurtosis 1.67803 ) 
  YH_BW(-1..1)   ( min -0.999965, max 1, mean 0.000142649, stddev 0.513308, skewness -0.00230809, kurtosis -1.01638 ) 
  YM_BW(-1..1)   ( min -0.973665, max 0.952258, mean -0.00350148, stddev 0.269087, skewness -0.0526863, kurtosis -0.330703 ) 
  YR_BW(-R..R)   ( min -1.48807, max 1.43847, mean 0.00175112, stddev 0.273388, skewness -0.00639253, kurtosis 0.344132 ) 

  ### Derivatives (w.r.t. inputs of non-linearities)
  DI_FW^  ( min -0.180061, max 0.230621, mean 8.86412e-05, stddev 0.0122438, skewness 0.0561295, kurtosis 7.34271 ) 
  DF_FW^  ( min -0.315512, max 0.229344, mean 0.000100828, stddev 0.0143057, skewness 0.104876, kurtosis 7.50553 ) 
  DO_FW^  ( min -0.11443, max 0.123433, mean 0.000274319, stddev 0.0151462, skewness 0.0698717, kurtosis 2.90709 ) 
  DG_FW   ( min -0.731871, max 0.882851, mean -0.00169359, stddev 0.0711718, skewness 0.209667, kurtosis 6.91009 ) 
  DC_FW*  ( min -1.47229, max 1.66073, mean -0.00391192, stddev 0.15738, skewness 0.0537399, kurtosis 4.39553 ) 
  DH_FW   ( min -0.386301, max 0.351356, mean -0.000936011, stddev 0.0493255, skewness 0.0331514, kurtosis 2.19561 ) 
  DM_FW   ( min -0.655933, max 0.623613, mean -0.00354963, stddev 0.123605, skewness 0.035569, kurtosis 0.255562 ) 
  DR_FW   ( min -0.577064, max 0.673443, mean -0.0027418, stddev 0.12143, skewness 0.039465, kurtosis 0.432035 ) 
  ---
  DI_BW^  ( min -0.164914, max 0.202091, mean 0.000227779, stddev 0.0122742, skewness 0.115862, kurtosis 6.70246 ) 
  DF_BW^  ( min -0.128973, max 0.165411, mean 0.000336239, stddev 0.0131387, skewness 0.108549, kurtosis 4.34456 ) 
  DO_BW^  ( min -0.137647, max 0.129458, mean 0.000294506, stddev 0.0148001, skewness 0.0573991, kurtosis 3.73221 ) 
  DG_BW   ( min -0.779914, max 0.755603, mean -0.000586112, stddev 0.0739264, skewness -0.0506157, kurtosis 5.21619 ) 
  DC_BW*  ( min -1.19834, max 2.00312, mean -0.000346347, stddev 0.163189, skewness 0.0455767, kurtosis 3.86651 ) 
  DH_BW   ( min -0.357239, max 0.396065, mean -0.000372942, stddev 0.0512778, skewness -0.111647, kurtosis 1.98869 ) 
  DM_BW   ( min -0.702897, max 0.790152, mean 0.000164795, stddev 0.126952, skewness -0.00411716, kurtosis 0.330771 ) 
  DR_BW   ( min -0.619408, max 0.635848, mean -0.00357004, stddev 0.122424, skewness -0.0133633, kurtosis 0.122539 ) 
Component 2 : <Tanh>, 
Component 3 : <AffineTransform>, 
  linearity_grad ( min -511.37, max 431.1, mean 2.64078e-08, stddev 5.54493, skewness -3.63166, kurtosis 2316.26 ) , lr-coef 1, max-norm 0
  bias_grad ( min -889.316, max 10.6957, mean 0, stddev 27.5706, skewness -28.0218, kurtosis 863.547 ) , lr-coef 1
Component 4 : <Softmax>, 
### END GRADIENT

LOG (ali-to-pdf[5.5.1074~1-71f3]:main():ali-to-pdf.cc:68) Converted 2919 alignments to pdf sequences.
LOG (ali-to-post[5.5.1074~1-71f3]:main():ali-to-post.cc:73) Converted 2919 alignments.
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:168) MC05_98, missing targets
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:334) ### After 755062 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:335) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.301148, max 0.331453, mean 0.00338207, stddev 0.0703498, skewness 0.0195238, kurtosis -0.27494 ) 
  f_w_gifo_r_   ( min -0.425459, max 0.408335, mean -0.000551618, stddev 0.0757716, skewness 0.00175504, kurtosis -0.0180304 ) 
  f_bias_   ( min -0.35375, max 1.26852, mean 0.220801, stddev 0.450276, skewness 1.07679, kurtosis -0.654243 ) 
  f_peephole_i_c_   ( min -0.361486, max 0.370627, mean -0.00531675, stddev 0.111826, skewness -0.0606021, kurtosis 0.538335 ) 
  f_peephole_f_c_   ( min -0.391899, max 0.49922, mean 0.00302868, stddev 0.121026, skewness 0.0268733, kurtosis 1.6242 ) 
  f_peephole_o_c_   ( min -0.446902, max 0.406077, mean -0.0172202, stddev 0.149183, skewness 0.245011, kurtosis -0.131909 ) 
  f_w_r_m_   ( min -0.429293, max 0.477106, mean 0.000492045, stddev 0.0937307, skewness -0.0028584, kurtosis -0.0415628 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -0.352832, max 0.322641, mean 0.00543303, stddev 0.0692203, skewness -0.0169963, kurtosis -0.308752 ) 
  b_w_gifo_r_   ( min -0.334211, max 0.300666, mean -0.000337552, stddev 0.0654048, skewness 0.000284943, kurtosis -0.550009 ) 
  b_bias_   ( min -0.250881, max 1.15985, mean 0.215062, stddev 0.443918, skewness 1.07694, kurtosis -0.675578 ) 
  b_peephole_i_c_   ( min -0.377552, max 0.250341, mean 0.00624115, stddev 0.0830032, skewness -0.139292, kurtosis 0.903416 ) 
  b_peephole_f_c_   ( min -0.495802, max 0.389639, mean 0.00757412, stddev 0.111162, skewness 0.30285, kurtosis 3.20524 ) 
  b_peephole_o_c_   ( min -0.374731, max 0.314081, mean -0.0146834, stddev 0.133316, skewness -0.0227591, kurtosis -0.450465 ) 
  b_w_r_m_   ( min -0.331195, max 0.332871, mean -2.75225e-05, stddev 0.0778926, skewness -0.00244976, kurtosis -0.230121 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.679417, max 0.603095, mean -0.00015592, stddev 0.100804, skewness 0.00461174, kurtosis 0.0489564 ) , lr-coef 1, max-norm 0
  bias ( min -0.30808, max 1.84159, mean 3.72529e-10, stddev 0.0580086, skewness 25.1988, kurtosis 792.427 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:336) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -14.8312, max 13.8231, mean 0.00769689, stddev 0.940803, skewness 0.731578, kurtosis 12.6034 ) 
[1] output of <BlstmProjected> ( min -3.91891, max 4.00564, mean -0.00373269, stddev 0.631983, skewness 0.0422207, kurtosis 2.77312 ) 
[2] output of <Tanh> ( min -0.999211, max 0.999337, mean -0.00350039, stddev 0.431453, skewness 0.0029197, kurtosis 0.00797057 ) 
[3] output of <AffineTransform> ( min -14.8823, max 19.62, mean 0.00998912, stddev 1.52611, skewness 0.664559, kurtosis 5.85373 ) 
[4] output of <Softmax> ( min 3.24327e-11, max 0.991061, mean 0.000781089, stddev 0.0122733, skewness 46.8076, kurtosis 2571.82 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:338) 
### BACKWARD PROPAGATION BUFFER CONTENT :
[0] diff of <Input>  ( min -0.797698, max 1.07449, mean 0.0076364, stddev 0.0952299, skewness 0.574152, kurtosis 6.34151 ) 
[1] diff-output of <BlstmProjected> ( min -0.448748, max 0.386641, mean -0.000133001, stddev 0.0537424, skewness -0.0443386, kurtosis 4.05195 ) 
[2] diff-output of <Tanh> ( min -0.488204, max 0.459304, mean -8.54579e-05, stddev 0.0698877, skewness -0.0129168, kurtosis 2.43812 ) 
[3] diff-output of <AffineTransform> ( min -0.999832, max 0.82099, mean -4.64323e-09, stddev 0.0209202, skewness -31.4638, kurtosis 1631.03 ) 
[4] diff-output of <Softmax> ( min -0.999832, max 0.82099, mean -4.64323e-09, stddev 0.0209202, skewness -31.4638, kurtosis 1631.03 ) 
### END BACKWARD


LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:339) 
### GRADIENT STATS :
Component 1 : <BlstmProjected>, ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  ### Gradients 
  f_w_gifo_x_corr_   ( min -33.1494, max 32.9935, mean 0.0118925, stddev 3.74458, skewness 0.0906941, kurtosis 4.56642 ) 
  f_w_gifo_r_corr_   ( min -37.7302, max 39.879, mean 0.00350685, stddev 3.12343, skewness 0.0875468, kurtosis 7.89296 ) 
  f_bias_corr_   ( min -22.9867, max 30.4097, mean -0.0160674, stddev 5.07466, skewness -0.0512488, kurtosis 3.51006 ) 
  f_peephole_i_c_corr_   ( min -21.2071, max 28.9283, mean -0.177104, stddev 5.0636, skewness 0.416479, kurtosis 8.30939 ) 
  f_peephole_f_c_corr_   ( min -100.367, max 79.7794, mean -1.51419, stddev 14.2897, skewness -1.41826, kurtosis 18.0685 ) 
  f_peephole_o_c_corr_   ( min -171.504, max 111.081, mean -0.99072, stddev 22.0603, skewness -1.32157, kurtosis 17.4168 ) 
  f_w_r_m_corr_   ( min -55.7215, max 52.2196, mean -0.00453112, stddev 5.89878, skewness 0.0397034, kurtosis 4.62048 ) 
  ---
  b_w_gifo_x_corr_   ( min -65.2786, max 62.1633, mean 0.156213, stddev 4.96842, skewness -0.487, kurtosis 25.3116 ) 
  b_w_gifo_r_corr_   ( min -49.8497, max 47.6967, mean -0.0102704, stddev 3.08473, skewness 0.000135867, kurtosis 12.3681 ) 
  b_bias_corr_   ( min -71.7016, max 65.6101, mean -0.965347, stddev 9.75236, skewness 0.550195, kurtosis 14.2192 ) 
  b_peephole_i_c_corr_   ( min -23.9192, max 14.3985, mean -0.3162, stddev 4.4761, skewness -1.15665, kurtosis 5.41154 ) 
  b_peephole_f_c_corr_   ( min -107.672, max 36.421, mean -1.05364, stddev 12.3744, skewness -2.72028, kurtosis 20.9984 ) 
  b_peephole_o_c_corr_   ( min -112.098, max 69.2548, mean -0.275115, stddev 19.0672, skewness -1.21777, kurtosis 8.31249 ) 
  b_w_r_m_corr_   ( min -46.3735, max 41.5591, mean 0.018968, stddev 5.99667, skewness -0.0150374, kurtosis 1.54625 ) 

  ### Activations (mostly after non-linearities)
  YI_FW(0..1)^   ( min 0, max 1, mean 0.32934, stddev 0.353649, skewness 0.700451, kurtosis -0.998284 ) 
  YF_FW(0..1)^   ( min 0, max 1, mean 0.452847, stddev 0.39108, skewness 0.110175, kurtosis -1.6051 ) 
  YO_FW(0..1)^   ( min 0, max 1, mean 0.273592, stddev 0.347448, skewness 1.02894, kurtosis -0.492808 ) 
  YG_FW(-1..1)   ( min -1, max 1, mean 0.0209653, stddev 0.769412, skewness -0.0364194, kurtosis -1.4465 ) 
  YC_FW(-R..R)*  ( min -50, max 50, mean 0.28966, stddev 10.5947, skewness 0.300868, kurtosis 15.5361 ) 
  YH_FW(-1..1)   ( min -1, max 1, mean 0.0273002, stddev 0.575228, skewness -0.017393, kurtosis -0.561189 ) 
  YM_FW(-1..1)   ( min -1, max 1, mean -1.69635e-05, stddev 0.284618, skewness -0.148566, kurtosis 4.70779 ) 
  YR_FW(-R..R)   ( min -3.54989, max 4.00564, mean 0.00651723, stddev 0.617564, skewness 0.210773, kurtosis 2.39413 ) 
  ---
  YI_BW(0..1)^   ( min 0, max 1, mean 0.353417, stddev 0.332436, skewness 0.532772, kurtosis -1.0743 ) 
  YF_BW(0..1)^   ( min 0, max 1, mean 0.485867, stddev 0.355776, skewness -0.172316, kurtosis -1.38431 ) 
  YO_BW(0..1)^   ( min 0, max 0.999995, mean 0.289003, stddev 0.333111, skewness 0.925764, kurtosis -0.593029 ) 
  YG_BW(-1..1)   ( min -1, max 1, mean 0.0125413, stddev 0.729037, skewness -0.0190586, kurtosis -1.34432 ) 
  YC_BW(-R..R)*  ( min -50, max 50, mean 0.687626, stddev 8.69043, skewness 1.52509, kurtosis 22.0279 ) 
  YH_BW(-1..1)   ( min -1, max 1, mean 0.0293672, stddev 0.599463, skewness -0.0107698, kurtosis -0.740298 ) 
  YM_BW(-1..1)   ( min -0.999982, max 0.999964, mean 0.00454957, stddev 0.301357, skewness -0.0156943, kurtosis 3.34646 ) 
  YR_BW(-R..R)   ( min -3.91891, max 3.51169, mean -0.0139382, stddev 0.642196, skewness -0.104043, kurtosis 3.13058 ) 

  ### Derivatives (w.r.t. inputs of non-linearities)
  DI_FW^  ( min -0.517969, max 0.453646, mean 3.19596e-05, stddev 0.0130773, skewness -0.577304, kurtosis 137.578 ) 
  DF_FW^  ( min -0.548736, max 0.315874, mean 9.70544e-06, stddev 0.00984277, skewness -3.06427, kurtosis 299.206 ) 
  DO_FW^  ( min -0.477186, max 0.546007, mean 1.02032e-05, stddev 0.0168206, skewness -0.114483, kurtosis 66.0484 ) 
  DG_FW   ( min -1, max 0.715364, mean 1.28055e-05, stddev 0.0174329, skewness -2.56129, kurtosis 469.647 ) 
  DC_FW*  ( min -2.85515, max 2.71866, mean 0.00103245, stddev 0.116536, skewness 0.0414444, kurtosis 57.5944 ) 
  DH_FW   ( min -1.76956, max 2.49582, mean 0.000129501, stddev 0.0790552, skewness -0.0637007, kurtosis 53.5205 ) 
  DM_FW   ( min -2.44461, max 2.7068, mean 0.00122615, stddev 0.21028, skewness 0.068719, kurtosis 11.4185 ) 
  DR_FW   ( min -0.682645, max 0.631244, mean -0.000517158, stddev 0.0645785, skewness -0.0283641, kurtosis 4.75445 ) 
  ---
  DI_BW^  ( min -0.477447, max 0.364519, mean 5.98495e-05, stddev 0.0116744, skewness -0.500966, kurtosis 78.043 ) 
  DF_BW^  ( min -0.266792, max 0.236114, mean 5.94245e-06, stddev 0.0091087, skewness -0.213989, kurtosis 59.3595 ) 
  DO_BW^  ( min -0.268819, max 0.277557, mean 0.000244593, stddev 0.0149783, skewness -0.0760491, kurtosis 25.9969 ) 
  DG_BW   ( min -1, max 0.881431, mean 0.000139935, stddev 0.0219205, skewness 0.848403, kurtosis 182.375 ) 
  DC_BW*  ( min -1.58972, max 2.59782, mean 0.000274366, stddev 0.0978177, skewness 0.590831, kurtosis 37.8911 ) 
  DH_BW   ( min -0.987372, max 1.34596, mean -4.32152e-05, stddev 0.0565895, skewness 0.0231927, kurtosis 32.4634 ) 
  DM_BW   ( min -1.94476, max 1.59233, mean 0.000158817, stddev 0.17318, skewness 0.0246127, kurtosis 4.98806 ) 
  DR_BW   ( min -0.474466, max 0.578226, mean -0.000123714, stddev 0.0658101, skewness -0.00653281, kurtosis 3.16859 ) 
Component 2 : <Tanh>, 
Component 3 : <AffineTransform>, 
  linearity_grad ( min -185.887, max 240.692, mean -3.62e-08, stddev 3.49227, skewness 0.987929, kurtosis 487.689 ) , lr-coef 1, max-norm 0
  bias_grad ( min -377.595, max 358.317, mean 1.78814e-08, stddev 15.5682, skewness -1.8245, kurtosis 487.971 ) , lr-coef 1
Component 4 : <Softmax>, 
### END GRADIENT

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:346) PER-CLASS PERFORMANCE:
@@@ Frames per-class: [ 181212 144 127 606 532 43 173 190 515 703 138 211 211 608 45 289 271 572 372 153 368 892 307 454 467 612 374 177 314 165 203 169 193 120 162 350 506 100 142 40889 89709 154 463 1777 286 231 687 352 545 459 79 394 481 237 200 663 1037 475 2210 428 257 167 4192 1910 93 230 151 223 4363 199 339 612 931 689 168 53 430 425 404 147 72 468 398 184 543 242 103 448 1376 60 779 717 668 146 541 163 358 830 561 408 249 268 103 1105 608 477 208 629 257 136 769 185 359 2082 165 69 303 74 247 336 172 147 117 128 443 612 117 125 52 316 956 165 305 574 202 297 222 1167 208 209 172 556 182 606 267 121 534 323 470 239 776 321 812 179 268 1139 537 191 554 5133 217 193 202 128 264 242 282 141 453 190 390 248 274 208 168 117 250 72 227 832 107 208 74 120 577 200 74 173 93 435 313 308 638 462 201 239 312 172 59 151 330 221 210 68 77 424 248 789 348 126 86 566 208 142 331 485 286 372 234 328 637 273 293 132 41 150 57 102 173 655 328 426 861 178 245 87 533 72 131 265 150 790 182 357 233 69 187 112 206 273 85 62 302 205 411 179 278 260 193 525 723 141 288 139 513 788 215 97 150 236 273 455 166 615 208 63 196 43 120 342 911 206 87 1156 192 146 131 166 31 221 252 417 373 499 401 233 179 409 182 439 579 96 189 251 1812 272 55 134 245 227 120 170 454 69 211 126 232 303 701 89 59 87 90 418 122 334 282 91 119 125 379 59 189 157 163 620 120 203 224 199 71 381 86 544 80 247 220 94 114 76 256 175 1186 420 378 336 135 140 331 666 306 356 196 203 291 152 221 543 214 149 360 365 670 264 264 692 216 616 923 524 408 234 395 301 375 105 525 80 359 98 456 204 309 258 288 286 181 90 101 263 53 253 167 354 150 174 194 393 579 494 437 273 142 236 159 226 115 606 198 255 508 136 282 304 327 234 1467 199 113 213 747 267 237 238 267 223 423 127 334 351 773 400 668 269 762 507 73 172 259 944 420 60 318 817 125 150 155 430 99 155 360 316 425 603 146 124 301 210 170 102 140 531 288 716 282 161 233 181 349 120 133 242 135 51 141 111 147 205 551 427 131 99 109 133 445 7274 204 271 135 128 125 377 120 372 449 808 108 557 45 280 47 596 57 276 400 121 347 315 193 77 146 96 105 216 193 341 1125 236 219 105 417 109 338 639 130 396 177 211 138 121 275 91 104 194 51 542 461 346 197 315 121 109 444 210 64 135 84 52 96 283 268 315 144 187 498 569 470 194 79 151 71 285 409 404 165 215 321 466 69 209 585 117 83 141 150 364 478 225 439 81 192 156 773 344 306 243 112 146 203 501 258 143 117 231 316 156 83 53 468 163 539 465 176 116 79 157 350 139 86 253 155 101 132 342 440 324 189 390 153 111 192 365 208 291 263 103 139 134 307 402 452 451 5943 88 406 111 189 488 786 548 86 632 99 382 216 141 365 388 65 90 187 430 300 463 604 247 79 314 305 463 45 473 718 330 223 226 1312 201 160 62 170 227 141 90 90 193 68 539 610 241 156 182 257 224 114 99 209 178 32 193 174 235 238 150 123 308 261 195 275 142 111 371 821 518 831 306 124 432 87 357 932 174 327 225 300 36 190 272 120 358 461 397 462 306 105 348 166 390 216 520 219 260 71 179 258 429 201 53 142 256 406 60 128 173 26 88 191 83 60 155 179 1000 125 44 191 230 383 687 216 257 321 233 417 202 1061 342 327 163 248 165 412 229 303 517 124 815 539 79 306 155 255 89 321 245 73 308 228 196 192 269 176 228 101 8591 469 337 966 237 430 177 258 334 85 304 414 487 401 57 178 99 260 178 151 248 261 926 126 248 230 264 241 147 220 155 139 646 219 209 393 1097 85 160 936 605 267 175 213 208 155 166 207 194 260 180 542 294 745 650 41 235 409 387 268 281 172 798 467 256 138 171 316 378 542 197 77 199 253 693 506 326 151 543 272 142 171 242 241 191 161 274 304 47 226 320 127 248 43 388 236 233 244 332 111 261 196 246 68 466 707 315 146 315 282 30 427 201 153 510 1026 278 186 316 45 464 106 316 404 415 543 346 257 452 1347 138 153 366 558 585 171 109 855 530 221 155 160 161 111 196 131 341 863 116 777 103 428 210 393 95 260 221 295 129 320 516 185 97 126 343 492 405 270 645 150 141 159 382 1153 426 252 535 332 205 417 575 597 292 372 180 236 213 343 301 154 349 528 290 243 378 616 213 266 230 676 309 338 165 324 314 639 310 148 365 184 119 116 199 264 1367 367 467 1442 213 598 641 335 156 122 351 114 129 98 489 213 162 386 116 74 274 197 265 173 129 167 753 211 214 1711 286 746 207 83 123 191 204 353 783 57 223 151 323 285 99 1197 111 116 189 130 401 488 428 96 280 248 468 506 117 284 315 328 129 334 124 557 265 243 383 268 66 97 474 153 221 139 391 194 634 156 322 748 309 195 326 651 462 370 172 385 490 392 610 304 249 70 501 324 335 228 370 206 587 560 108 850 256 209 153 168 206 131 204 232 340 200 284 174 301 124 107 220 67 370 1390 1253 311 434 127 136 104 493 416 778 103 335 338 242 66 208 459 231 96 291 160 220 368 145 728 458 351 255 350 348 137 125 1039 1466 285 221 232 361 605 549 4192 543 376 361 3105 243 319 117 225 232 542 309 859 113 244 913 235 271 853 881 252 243 274 129 90 371 274 176 453 107 172 140 172 411 267 778 147 558 2509 132 95 258 162 203 87 21 467 192 231 265 665 552 160 225 162 279 3235 297 303 235 317 297 155 692 494 82 179 192 734 178 103 204 87 225 63 351 225 260 282 352 367 219 77 182 384 386 147 340 273 260 234 61 251 141 663 194 320 94 144 ]
@@@ Loss per-class: [ 1.20963 6.76494 6.41872 5.80156 5.33144 8.87869 6.85929 5.58928 4.83006 5.56452 6.59664 6.50551 8.31256 5.05492 9.80743 5.76806 5.79396 5.85474 5.93 6.89456 5.31158 4.19739 5.96917 5.28844 4.83495 4.89182 5.11387 6.96077 6.23125 6.95341 5.94507 6.62041 5.67943 7.54598 6.65287 5.56115 6.39199 7.71943 7.58531 5.33704 3.11872 6.49526 4.75038 5.57306 5.81569 5.65722 3.76173 5.64489 5.58568 5.34351 7.86441 5.58032 5.13816 7.54143 7.93131 4.74653 5.43796 4.9305 4.15626 5.04825 6.58413 7.50161 2.86963 2.36771 6.95539 5.13792 6.98307 5.96841 4.77545 5.95935 4.45942 5.59124 4.47118 4.72134 6.36411 9.23302 4.75917 3.93661 4.73566 6.64713 9.59818 5.68596 6.28975 6.36193 6.17756 6.43217 8.49348 5.71211 5.15416 8.68186 5.59921 4.76843 4.11879 7.52449 5.3176 7.35907 5.83421 4.23761 3.38166 6.04047 6.73189 6.93302 7.86102 5.15572 4.73516 6.34376 7.14599 5.86904 5.27932 7.60225 5.14204 7.58679 6.45975 3.85885 6.68813 7.71718 5.83622 7.46901 7.18709 6.57315 7.14052 7.86685 8.52314 6.67863 4.75614 4.12112 8.6982 7.36015 9.19662 5.3406 4.84875 6.59726 4.74404 4.59588 6.44101 6.28819 5.0214 3.72232 6.12911 6.67055 7.66146 3.89174 6.29777 4.30024 6.45028 8.2502 5.82052 6.0997 5.24183 5.48957 3.28797 6.04861 5.89849 7.08129 6.40841 3.81602 4.367 5.83685 4.18292 2.65301 6.3139 6.39816 7.03491 7.32762 5.79643 5.94204 5.09491 6.891 4.26458 7.67512 5.56286 5.11384 6.00068 7.30443 6.89353 7.41932 5.29512 7.73979 7.85818 3.94861 6.61465 7.11303 8.52045 6.87431 4.96527 6.45746 9.97639 6.37451 8.28913 5.23838 6.1079 5.01377 4.49793 5.24285 6.92264 5.49423 5.59843 6.79143 8.28457 6.99948 5.70752 6.87524 6.60929 8.3834 7.62631 5.90183 6.60155 4.01056 5.41668 7.30739 8.38979 5.13573 5.94296 11.43 5.60819 4.76034 6.21577 4.82116 5.97905 5.64166 4.26173 4.96382 6.31832 6.69639 9.39059 7.12046 9.46596 6.69692 7.40283 5.0403 6.39054 4.33558 4.72493 6.66314 5.75056 8.17054 5.47362 8.16234 6.88688 6.88399 7.01619 3.80444 6.71284 5.28531 5.64359 8.53971 7.06929 8.70034 6.7153 6.05131 9.12615 7.16084 6.14185 6.42007 5.40564 5.26847 6.00491 6.80263 5.99841 5.5929 4.08356 7.74945 5.78209 7.53115 5.86671 4.86015 6.62311 9.39382 6.21315 6.25655 6.71536 5.32367 6.9477 5.77525 6.9052 8.68636 7.80609 8.23509 6.79813 6.34855 4.90303 6.37931 8.13398 4.42187 6.02584 7.20272 7.70192 7.52449 9.76494 7.07569 7.27782 6.38576 5.73582 6.13895 5.84056 7.46028 7.5262 5.01887 6.8777 6.16636 3.77213 8.10811 7.31817 5.71915 2.81874 6.44346 8.80637 7.46903 6.51057 6.87901 7.77846 6.12771 6.08861 7.73638 6.48847 7.57767 6.90776 5.86122 5.12 7.54395 8.5489 7.72954 7.58803 4.92727 8.33652 6.00264 6.11506 9.17798 7.30513 7.78204 4.88352 8.68514 6.96504 5.83492 6.35597 4.80711 7.84121 6.38068 5.44651 6.96742 7.48061 4.89794 8.65074 4.77282 7.84753 6.2578 6.62921 7.97837 7.84486 7.94393 6.70107 7.86311 4.62003 6.06045 5.02545 5.80115 5.82341 7.07612 6.20761 4.58191 6.45328 5.99546 6.38948 6.75569 6.34549 6.46449 5.81908 5.16205 5.62745 7.65414 5.47331 5.98374 3.95825 6.603 5.6073 5.28732 6.28366 4.55111 4.86775 5.49142 5.30995 5.76982 6.34483 6.69068 5.46683 6.98965 4.55462 6.92631 6.5165 7.57746 5.34113 6.39256 6.14826 6.73229 5.28333 5.83189 6.75442 8.1562 6.83687 5.6558 8.12587 5.90599 6.27314 5.33571 6.6066 6.95718 6.60297 5.85877 5.32299 5.74249 7.17796 6.9924 6.94044 6.37181 6.93545 5.12121 8.01575 4.43949 7.12635 6.271 5.29842 7.62356 6.63788 5.05242 5.21568 6.41141 4.29523 6.47518 8.83182 6.41266 5.3784 6.35042 6.88658 7.00086 5.60478 6.77231 5.6065 7.57055 4.81875 4.65602 3.80106 6.32122 3.97286 5.83407 4.39195 4.32774 9.49005 6.33128 6.28186 3.60588 5.32281 8.26376 6.15533 5.67374 7.43741 11.7173 6.68915 5.33019 6.82958 6.97783 5.20908 5.15449 6.80366 5.47002 7.20076 8.36882 7.08866 6.13576 6.24266 7.0986 5.60013 4.07835 6.83565 5.1639 6.14735 6.17883 5.55759 6.29108 5.48964 6.6686 9.02691 6.64172 7.17301 8.41071 7.66594 8.4122 7.27316 6.5519 3.88285 5.50479 7.32518 7.81947 7.0403 7.01239 5.0372 3.97173 5.86159 6.12523 7.66687 7.94883 7.29795 5.94622 8.07578 6.36572 5.57405 5.6748 8.67893 3.70758 8.6808 6.03635 9.36298 5.79119 8.52378 5.72619 4.65028 6.92659 6.44225 5.24543 7.04831 7.63268 7.79629 7.24273 6.60461 7.75851 7.2435 6.56703 4.67868 6.59756 6.0949 6.93315 5.41916 7.28443 6.08358 3.75748 7.26706 5.52715 6.99537 6.04792 7.1371 6.88064 5.72789 8.65494 7.95882 6.93909 8.33289 5.26065 4.13584 6.8488 6.37223 6.52933 7.46917 8.00617 5.3421 7.03712 7.03055 6.9244 8.27069 8.38403 8.12105 6.1698 8.19941 7.68867 7.17628 6.39806 4.21448 4.98001 4.62178 5.36265 7.8763 7.14974 9.44075 6.15491 5.30508 4.92086 7.48591 6.25617 4.38044 4.91032 7.70961 7.09728 7.85922 8.38186 7.65994 6.96648 7.03835 5.69915 6.95963 6.2544 6.30139 7.60769 6.93299 7.34703 5.23534 5.58483 6.52912 6.42353 6.91068 6.34416 7.03305 5.30117 6.05982 6.97053 7.66867 7.62437 5.20644 7.64216 7.65605 8.10915 4.9836 6.89451 5.16893 4.31688 6.25657 8.36606 8.7849 7.44349 7.25432 7.4469 8.02752 7.12505 7.11408 8.10984 8.02547 6.53459 5.04648 5.68756 6.83377 6.54307 7.17996 7.23239 7.61319 6.26493 5.95181 5.68664 5.02323 7.77133 7.21843 7.07283 6.60108 5.65862 5.67832 4.95656 4.06386 7.52399 5.26671 7.93369 6.71803 5.06476 4.05099 5.45862 9.14038 3.65528 8.98968 7.00993 6.42615 7.8062 6.16383 5.43607 8.68219 8.25591 7.69407 4.65382 4.5192 4.62535 3.74748 6.94187 8.05765 6.41158 6.16674 6.68328 9.46511 4.9495 4.78252 5.89897 7.61054 6.45285 5.05786 6.87187 6.94104 9.8074 6.76474 6.5805 5.53025 7.20655 8.3962 6.43652 6.88625 4.63627 4.50117 6.66077 6.8524 5.74314 6.84515 6.37452 7.51738 8.51778 6.7069 7.70879 7.54623 7.10845 7.05483 6.48281 6.75452 7.25716 6.99138 5.29057 5.83621 7.70575 5.76118 6.1697 7.66741 5.25387 4.7569 4.72468 4.00219 5.03787 6.98109 5.72453 9.06856 6.71134 5.42107 8.12358 5.58296 7.18485 7.05806 10.2494 6.98676 7.27546 7.06173 4.50438 5.77842 6.20239 4.23302 5.8982 7.86743 6.10659 7.04245 5.22224 7.11731 5.30298 5.48798 6.63444 8.85464 7.73531 5.46055 4.82233 6.50678 9.56641 5.65819 6.05595 5.53661 8.36862 7.4055 6.85973 7.86732 7.23295 6.73309 8.62353 7.60025 6.19244 7.01516 4.54764 6.55808 8.21087 6.57211 7.09013 7.05679 4.50675 7.41281 6.32566 6.14784 5.83991 5.81628 6.6302 5.55809 6.45868 6.10852 7.76501 7.04402 7.5347 4.48424 5.16787 6.08983 5.90952 7.34232 4.16735 5.72227 8.43931 7.12313 7.05133 5.16868 8.31505 5.93815 6.30215 7.22085 6.41249 5.603 7.52242 7.12039 6.09591 7.19884 7.89585 9.01483 1.72891 5.11895 7.43474 3.66815 6.01283 5.42889 7.4123 5.1988 5.32182 9.02687 6.12147 5.34857 4.53383 5.47184 7.87303 7.15844 8.26251 5.52071 6.78991 7.31972 6.93371 6.76319 4.15395 7.82928 6.60496 6.65614 7.2924 6.33907 7.72673 7.08813 6.52435 7.03218 4.86767 7.10763 6.9054 7.7849 4.71912 8.81556 7.04804 3.489 4.80357 4.86467 5.81339 6.1472 6.87248 6.48993 6.51338 7.05541 7.4583 6.98674 6.51719 4.61017 6.18469 5.18142 5.87671 8.60801 5.9747 6.53671 5.2938 6.21103 7.27611 7.5822 4.90226 5.17744 6.56773 8.54465 6.13778 6.04963 5.62666 5.19161 7.13731 7.65497 7.37086 6.01105 3.96621 4.71538 5.74652 6.10975 4.47104 5.33623 7.28986 7.48432 6.62838 6.76279 6.08524 7.83429 6.57015 5.43781 8.49238 6.59041 5.71376 7.00331 5.34304 8.53799 5.1085 6.2508 6.72452 6.92369 5.34338 7.71139 6.61903 6.7362 5.73465 7.32906 4.11723 4.17397 5.7555 7.96541 5.63873 5.58726 9.29425 5.20103 6.5787 6.33618 5.3951 5.50475 6.52251 7.68063 5.86026 8.92124 5.63703 7.16821 6.71055 5.21214 5.24359 5.38372 6.33691 6.3228 5.31799 4.60951 6.93433 7.77226 7.13646 6.26871 4.96371 6.71835 7.91844 4.96458 6.02687 5.79354 7.11529 6.74359 6.47959 7.25974 6.12273 6.60655 5.20883 3.78848 7.76294 4.62463 7.67835 5.9642 5.02723 5.04917 6.92782 6.87314 5.78687 7.20892 7.44807 6.26608 5.33782 6.70056 7.53894 7.96146 7.74673 5.27259 4.84506 6.06015 4.71998 7.75691 7.44814 7.69337 5.99736 5.35496 5.75982 6.89714 5.32126 5.13174 7.41323 4.91477 4.96461 6.48356 6.78879 6.30436 6.47257 6.23525 7.70043 5.06445 6.45607 6.79503 6.19028 5.23778 6.46965 6.55288 5.9094 5.01372 6.41317 6.22278 6.24266 4.4754 6.04221 5.50557 5.76823 4.97533 5.81123 6.19248 6.33062 8.04791 5.80218 7.28993 5.97184 7.58862 7.1756 5.35214 4.95164 6.70259 4.94926 2.46061 6.09933 5.39889 4.04901 5.59131 6.1615 7.60184 5.81633 6.81291 7.95326 7.38032 5.24788 6.5103 6.98136 5.44722 7.06631 8.37898 5.82268 6.86914 6.416 7.67359 6.90758 6.46825 5.38669 6.77279 7.19735 2.81352 6.60925 3.55718 6.64835 7.094 8.34086 7.54379 7.15186 6.04577 4.21468 8.96517 5.38488 6.9443 5.69863 6.08015 7.92025 3.97936 7.77488 7.57038 6.04315 7.93106 5.45292 5.98503 5.43369 8.02716 6.01657 6.95307 5.76214 4.98187 6.48132 6.4972 6.46118 6.25132 7.72067 6.11095 7.85181 6.50677 6.96689 6.96453 5.08403 6.40421 7.66104 7.81364 5.98198 6.92116 7.32084 6.84443 5.81754 5.88771 5.01991 7.34975 5.55629 4.98292 6.29133 7.68636 5.21475 4.5281 6.02981 6.00936 6.50306 5.01611 5.49527 5.44701 4.70612 6.76439 7.03881 8.0881 5.71996 6.04931 7.42539 6.1059 5.92384 7.26636 4.67788 4.61703 7.73025 4.87782 6.23045 5.93756 6.56595 7.37618 7.63129 8.19444 7.15848 6.6739 6.27692 6.70469 5.0896 6.89229 5.31979 7.55189 8.57544 6.15315 7.85098 6.0331 4.02371 4.84905 5.15023 5.84904 8.21268 6.46354 6.87095 4.88127 5.31425 4.0888 8.11587 6.84246 6.49953 6.00526 9.09477 5.96981 5.63388 6.82362 8.13564 5.54748 7.24095 5.43672 6.16289 7.38733 4.36517 4.28256 6.4604 7.02826 6.38352 5.68299 7.99448 6.96084 4.63414 3.48431 7.00317 7.30931 6.48411 6.39464 4.28288 4.42398 3.12828 5.75035 5.91003 5.01088 3.43805 6.6303 6.29188 7.53758 6.66558 6.39115 5.15497 6.51541 4.63154 7.8913 6.86484 3.95667 6.91602 6.25475 5.78543 4.55173 5.42527 7.21781 6.91213 6.42826 7.01572 6.14872 6.90577 6.88595 5.06267 7.88436 6.79167 7.03674 6.11093 5.91901 7.04585 4.96747 6.84611 4.61878 3.74364 7.78937 7.23896 7.18051 6.95806 6.73116 8.02821 10.3602 6.08441 6.33021 7.31303 6.97229 5.6834 6.75195 6.99871 7.43043 6.7863 6.6616 3.16397 6.04829 5.55878 5.90861 6.35248 6.53867 7.50127 4.84016 5.05907 7.2606 6.87142 6.45059 4.5942 7.08853 8.44059 6.39299 7.50122 5.83926 7.65878 5.85682 6.29788 7.04992 6.84795 6.23731 6.60277 5.09678 8.72582 6.69386 5.05186 5.69123 6.85549 5.10013 6.06544 7.19949 7.73772 7.50454 6.94947 8.13208 5.88345 6.71214 6.496 7.12207 7.40314 ]
@@@ Frame-accuracy per-class: [ 67.614 0 0 4.28689 15.0235 0 0 2.62467 4.84966 5.828 0 0 0 2.95809 0 2.41796 6.26151 2.27074 0.536913 0 0.27137 18.1513 5.52846 4.62046 16.0428 5.38776 23.231 0 0.63593 0 1.4742 1.17994 0 0 1.23077 0.285307 0.987167 0 0 21.4285 45.0989 0 2.1575 0.618847 1.39616 4.31965 25.7455 5.10638 0 8.70511 0 10.3929 5.81516 0 0 13.1123 0.289157 7.78128 29.2242 14.7025 0 0 48.9207 62.1303 0 1.30152 1.32013 0 8.80028 0.501253 22.3859 0.816327 11.9163 6.6715 4.74777 0 16.0279 35.9577 18.5414 0 0 2.77481 6.52447 0 0.183993 0 0 3.79041 5.66655 0 0.128287 12.2648 19.1473 0 2.40074 0 2.51046 16.4961 26.0018 1.95838 0 0 0 0 14.2975 0.418848 0 0.158856 0 0 0.779727 0 0.278164 11.8607 0 0 0.658979 0 0 2.97177 0 0 0 0 8.34273 20.898 0 0 0 4.73934 7.52744 0.60423 10.1473 12.8808 0 0.672269 16.1798 42.9122 1.43885 0.477327 0 23.1806 0 8.24402 0.747664 0 2.99345 2.47295 0.21254 4.17537 40.0515 2.17729 0.492308 0 2.60708 30.803 28.6512 0.522193 30.8386 45.1739 0 1.03359 0 0 2.6465 0.824742 12.3894 0 13.6714 0 6.40205 14.0845 3.64299 0 0 0 9.98004 0 0 18.6186 0 0 0 0 8.13853 0 0 1.15274 0 3.44432 1.5949 17.8282 26.6249 10.3784 0 0 4.8 0 0 0 3.02572 0 0 0 0 6.83157 0.804829 32.2989 8.32138 0 0 2.29479 1.43885 0 3.31825 2.88363 0 7.24832 0 1.52207 11.2941 4.02194 0 0 0 0 0 0 0 5.7971 2.1309 17.8195 17.1793 1.68067 5.29532 0 2.06186 0 0 0 0 11.3852 0 3.07692 0.428266 0 0 0 0.484262 3.65631 0 0 5.28926 0.486618 3.6452 1.67131 1.43627 0 0 9.89534 27.367 0 1.03986 0 0.584226 14.331 0 0 0 0 7.67824 0.219539 0 7.4736 0 0 1.52672 0 0 0 19.3088 0 0 4.58279 0 0 0 0 0 0.451467 1.58416 1.91617 3.48059 2.4024 1.74346 0 0 11.7216 0 1.82025 31.0613 0 0 6.36183 58.8138 0.366972 0 0 0 0 0 0 0.440044 0 0 0 0 1.97694 13.2573 0 0 0 0 6.69056 0 0.298954 1.41593 0 0 0 8.95916 0 0 0 0 16.4384 0 1.9656 0.890869 0 0 12.3198 0 13.5904 0 0 0 0 0 0 0 0 3.70839 0 13.7384 2.37741 0.738007 0 0.301659 10.9527 0 0.841515 0 0.4914 0 0 9.02935 6.99172 10.2564 0 2.21914 1.91518 28.4862 1.13422 4.53686 1.01083 1.84758 16.545 16.1343 2.47855 5.38556 0.852878 2.2756 0 0.798935 0 17.1265 0 0.556328 0 5.69551 0 1.61551 2.70793 6.58579 2.44328 0 0 0 0.379507 0 2.76134 0 5.92384 0 0 0 0.762389 10.0086 0.404449 0.228571 0.731261 0 0 0 0 0 20.4452 0 0.391389 11.2094 0 0 11.1658 3.35878 1.70576 21.4651 1.00251 0 0 4.28094 0.747664 0 0 2.24299 0 0 0 5.97907 13.9403 29.8643 0 16.6043 3.71058 20.0656 31.5271 0 1.73913 0 29.5394 5.9453 0 3.13972 0.489297 0 0 5.14469 2.32288 0 0 10.5409 12.0063 0.235018 0.6628 0 0 0 1.42518 1.17302 0 13.5231 25.3998 0 11.8632 0.707965 1.23839 9.42184 0 2.28898 0 0 5.7732 0 0 0 0 0 0 37.8966 4.21053 0 0 0 0 9.20314 26.5173 1.95599 0.736648 0 0 0 0.794702 0 2.14765 9.12125 2.47372 0 32.8251 0 0.713012 0 0 0 0.361664 9.48814 0 0 13.3122 0 0 0 0 0 0 0 0 7.7299 0 0.455581 0 0.239521 0 3.84047 18.6083 0 5.04414 0.56338 0.945626 0 0 0.725953 0 0 0 0 1.84332 29.9025 0 0.506329 1.90174 0 0 6.52418 0 0 0 0 0 0 0 0 0 0.692042 0 20.662 10.1844 12.7524 12.3393 0 0 0 1.40105 3.1746 7.41656 0 0 5.28771 7.07395 0 0 0 0 0 0 0 0.823045 0 0 0.455063 0 0 0 10.7304 8.418 0 0 0 0 0 4.78564 0.386847 0 0 0 5.6872 0 0 0 7.6841 0 16.126 16.971 0 0 0 0 0 0 0 0 0.643087 0 0 0 3.63224 1.849 0 0.256082 1.30293 0 0 2.46238 0.479616 3.43053 4.93359 0 0 0 0 1.98758 0.441989 13.5105 13.9817 0 4.67405 0 0 11.6684 19.8347 8.02188 0 38.419 0 0 0 0 2.18878 1.8018 0 0 0 13.9373 11.3145 10.5717 32.5889 0 0 0.63593 0.654664 0 0 7.81415 3.61865 2.42057 0 0 0.685714 3.97022 0 0 0.58651 0 0 0 0 0 0 15.0139 11.6298 0 0 0.547945 0 0 0 0 0.477327 0.560224 0 0 0 0.849257 0 0 0 1.62075 3.44168 0 6.53358 0 0 7.80619 8.27754 11.5718 29.9459 9.78793 0 5.78035 0 0 2.14477 0 6.10687 0 0.998336 0 0.524934 0.366972 0 18.689 3.46696 0.754717 17.5135 2.61011 0 4.5911 0 6.65813 0 4.03458 7.28929 0 0 0 5.80271 10.7101 0 0 5.61404 8.577 5.41205 0 0 0.576369 0 0 0 0 0 0 0 18.5907 0 0 0 0 0.521512 9.89091 0 0.776699 0 4.28266 2.15569 0 3.67405 0.291971 0.305344 0 0 0 10.4242 3.48584 0 3.09179 0 29.1845 1.11214 0 0.326264 0 7.82779 0 0 1.222 0 1.2966 3.93873 0 0.519481 0 0 0 0 70.0925 5.53781 0 41.076 0.842105 2.55517 0 5.02901 6.27803 0 0.656814 0.241255 15.5897 2.49066 0 0 0 13.4357 0 0 0 0 15.9741 0 2.01207 0.433839 0 0 0 0 0 0 8.66203 0.455581 0 0 7.92711 0 0 36.3054 12.8819 5.23364 1.7094 0 0 0 0 0.481928 0 0 1.66205 24.5161 3.39559 1.87793 0 0 1.27389 2.442 4.64516 0.372439 0 0 4.88416 8.98396 0.779727 0 10.4956 1.89573 1.0568 8.47926 0 0 0 1.18343 24.9459 10.0691 2.75651 0 19.8712 3.30275 0 0 0.412371 0 0.522193 0 0.728597 6.89655 0 0.441501 4.05616 2.35294 8.4507 0 9.78121 0.422833 0.428266 0 3.30827 0 0 0 1.21704 0 27.224 18.3746 4.75436 0 3.80349 2.12389 0 1.40351 0 0 1.17532 0.876766 0 0 0.947867 0 7.10441 0 0.315956 8.89988 2.40674 6.80773 0.865801 0 11.7127 15.2876 0 0 0 1.25336 13.4927 0 0 11.2215 2.82752 2.25734 0 0 0.619195 0 2.54453 0 3.22108 30.3416 0 5.9164 0 2.33372 16.6271 17.0267 0 0 2.7088 0 0 0.624025 4.45305 0 0 0 0 9.74619 22.9346 0 22.3083 0 0 0 1.30719 0.346771 4.68933 0 2.05415 4.51128 0 1.91617 5.73414 0 0 0.536913 0 0.422833 0 1.74672 1.3267 0 0.572246 4.16272 0.344234 0 2.11361 6.65045 3.27869 0 3.47072 10.3474 1.29241 1.77253 1.81269 9.24499 3.81558 0.781861 0.322061 0 3.00958 0 0 0 0 8.31758 1.24314 0.272109 15.4011 55.1127 0.468384 3.50877 25.8769 1.49031 0 0 3.69844 0 0 0 7.35444 0.468384 0 5.95084 0 0 2.91439 0 0.376648 0 0 0 5.57399 0 0 48.2033 0 30.0067 0 0 0 0 0 2.54597 15.5712 0 4.02685 0 4.01855 0.350263 0 24.3841 0 0 1.58311 0 14.6949 8.59775 0 0 1.06952 0 1.28068 5.3307 0 0.702988 1.26783 1.21766 0 3.28849 0 0.358744 0 0 3.12907 0 0 0 0.210748 0 0 0 2.29885 2.05656 15.2876 0 7.13178 5.07682 0 0 1.53139 16.4236 1.72973 4.04858 0 10.8949 6.32008 6.36943 14.0868 0.328407 0 0 1.59521 0.924499 0 1.75055 7.55735 0 11.4043 5.8876 0 19.8707 0.389864 0 0 0 0 0 0 0 0.881057 0 10.5448 0 4.31177 0 0 0 0 3.23887 22.0784 11.4878 1.60514 0.920598 0 0 1.91388 13.5765 2.40096 28.3879 0 0 1.4771 0 0 1.43885 1.08814 0 0 6.51801 0 11.7914 0.814111 0 16.884 13.3043 0.56899 1.95695 3.13837 5.73888 0 0 14.5262 30.5489 0 0 0 0.829876 19.3229 18.9263 34.2755 1.47194 2.12483 6.639 15.231 0 0.938967 0 0.443459 0 7.74194 0 9.19139 0 0 29.3377 0 3.31492 3.28061 17.924 9.50495 0 0.728597 0 0 0.269179 3.27869 0 13.4509 0 0 0 1.15942 7.77643 0 9.37701 0 20.949 9.9223 0 0 0.386847 0 0 0 0 1.06952 1.03896 0 0 0.601052 0 0 0 0 0 24.3239 1.68067 2.30643 4.67091 0.944882 0 0 16.6065 9.50455 0 0 0 10.7556 0 0 1.95599 0 2.21729 0 3.41394 2.21729 0 0 0.851064 0 12.3007 0 0 3.90117 3.88098 0 7.04846 0 0 0 0 0.397614 0 6.33007 0 0.312012 0 0 ]

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:347) Done 2622 files, 1 with no tgt_mats, 0 with other errors. [TRAINING, 0.326505 min, fps38542.6]
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:353) AvgLoss: 4.13094 (Xent), [AvgXent: 4.13094, AvgTargetEnt: 0]
progress: []
FRAME_ACCURACY >> 28.8447% <<

