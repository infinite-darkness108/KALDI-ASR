speech-HP-Z2-Tower-G9
nnet-train-multistream-perutt --cross-validate=false --randomize=true --verbose=0 --num-streams=10 --max-frames=15000 --learn-rate=0.00004 --momentum=0.9 --l1-penalty=0 --l2-penalty=0 --feature-transform=exp_FG/blstm4i/final.feature_transform 'ark:copy-feats scp:exp_FG/blstm4i/train.scp ark:- | apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_tr90/utt2spk scp:data-fbank/train_tr90/cmvn.scp ark:- ark:- | add-deltas --delta-order=2 ark:- ark:- |' 'ark:ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- |' exp_FG/blstm4i/nnet/nnet_iter04_learnrate0.00004_tr1.2117_cv2.0752 exp_FG/blstm4i/nnet/nnet_iter05 
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:243) Not in compute-exclusive mode.  Suggestion: use 'nvidia-smi -c 3' to set compute exclusive mode
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:438) Selecting from 1 GPUs
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:453) cudaSetDevice(0): NVIDIA RTX A2000 12GB	free:11515M, used:515M, total:12031M, free/total:0.957153
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:501) Device: 0, mem_ratio: 0.957153
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:382) Trying to select device: 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:511) Success selecting device 0 free mem ratio: 0.957153
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:FinalizeActiveGpu():cu-device.cc:338) The active GPU is [0]: NVIDIA RTX A2000 12GB	free:11001M, used:1029M, total:12031M, free/total:0.91443 version 8.6
copy-feats scp:exp_FG/blstm4i/train.scp ark:- 
apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_tr90/utt2spk scp:data-fbank/train_tr90/cmvn.scp ark:- ark:- 
add-deltas --delta-order=2 ark:- ark:- 
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:144) TRAINING STARTED
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Read():nnet/nnet-matrix-buffer.h:191) Read() started... Buffer size in MB: 0, max 3072, having 0 utterances.
ali-to-post ark:- ark:- 
ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl 'ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |' ark:- 
LOG (copy-feats[5.5.1074~1-71f3]:main():copy-feats.cc:143) Copied 2624 feature matrices.
LOG (apply-cmvn[5.5.1074~1-71f3]:main():apply-cmvn.cc:159) Applied cepstral mean and variance normalization to 2624 utterances, errors on 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:303) ### After 0 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:304) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.464232, max 0.504043, mean 0.00397508, stddev 0.0801098, skewness 0.0100524, kurtosis 0.256708 ) 
  f_w_gifo_r_   ( min -0.421381, max 0.399543, mean -0.000603535, stddev 0.0771581, skewness -0.000462893, kurtosis 0.000594378 ) 
  f_bias_   ( min -0.351704, max 1.31875, mean 0.213917, stddev 0.458923, skewness 1.0678, kurtosis -0.655865 ) 
  f_peephole_i_c_   ( min -0.464282, max 0.442141, mean -0.00220347, stddev 0.124587, skewness 0.12539, kurtosis 1.11499 ) 
  f_peephole_f_c_   ( min -0.705293, max 0.785375, mean 0.0031955, stddev 0.164794, skewness 0.257592, kurtosis 4.41684 ) 
  f_peephole_o_c_   ( min -0.5152, max 0.415614, mean -0.00973333, stddev 0.175149, skewness 0.218131, kurtosis -0.211005 ) 
  f_w_r_m_   ( min -0.558262, max 0.468445, mean 0.000596909, stddev 0.0996311, skewness 0.000318856, kurtosis -0.0202014 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -0.860018, max 0.762354, mean 0.00627528, stddev 0.0848185, skewness -0.0539922, kurtosis 1.65195 ) 
  b_w_gifo_r_   ( min -0.36629, max 0.293288, mean -0.000218277, stddev 0.0692158, skewness 0.00106993, kurtosis -0.351698 ) 
  b_bias_   ( min -0.318605, max 1.1816, mean 0.208538, stddev 0.449854, skewness 1.05418, kurtosis -0.686026 ) 
  b_peephole_i_c_   ( min -0.355517, max 0.271561, mean 0.00534048, stddev 0.0905407, skewness -0.10323, kurtosis 0.886218 ) 
  b_peephole_f_c_   ( min -0.601484, max 0.661454, mean 0.0116864, stddev 0.15537, skewness 0.547112, kurtosis 3.77661 ) 
  b_peephole_o_c_   ( min -0.541427, max 0.463611, mean -0.0167332, stddev 0.18009, skewness -0.119687, kurtosis 0.232522 ) 
  b_w_r_m_   ( min -0.38341, max 0.359479, mean -0.000204442, stddev 0.0875874, skewness 0.000801449, kurtosis -0.102928 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.910532, max 0.721945, mean -0.000155918, stddev 0.105338, skewness 0.00618222, kurtosis 0.0665326 ) , lr-coef 1, max-norm 0
  bias ( min -0.0776042, max 2.22965, mean 1.02445e-09, stddev 0.0725238, skewness 23.6695, kurtosis 702.583 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:305) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -8.67671, max 9.02116, mean 0.0321104, stddev 1.01793, skewness 0.450612, kurtosis 3.20408 ) 
[1] output of <BlstmProjected> ( min -4.96364, max 4.51912, mean 0.00114377, stddev 0.761333, skewness -0.0152082, kurtosis 1.35048 ) 
[2] output of <Tanh> ( min -0.999902, max 0.999762, mean 0.00156745, stddev 0.505714, skewness -0.00922761, kurtosis -0.723819 ) 
[3] output of <AffineTransform> ( min -15.4005, max 20.5961, mean 0.00888625, stddev 2.28701, skewness 0.802941, kurtosis 3.12363 ) 
[4] output of <Softmax> ( min 7.33432e-14, max 0.99991, mean 0.000779825, stddev 0.0185689, skewness 36.802, kurtosis 1529.12 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:307) 
### BACKWARD PROPAGATION BUFFER CONTENT :
[0] diff of <Input>  ( min -2.93817, max 2.77534, mean -0.00685085, stddev 0.215291, skewness 0.433786, kurtosis 11.1499 ) 
[1] diff-output of <BlstmProjected> ( min -0.689195, max 0.73693, mean -9.38405e-05, stddev 0.0529595, skewness -0.0016878, kurtosis 5.0124 ) 
[2] diff-output of <Tanh> ( min -0.970066, max 0.814493, mean -0.000129221, stddev 0.0693385, skewness 0.00290961, kurtosis 3.12037 ) 
[3] diff-output of <AffineTransform> ( min -0.999998, max 0.988794, mean -4.03817e-07, stddev 0.0203039, skewness -22.6437, kurtosis 1366.84 ) 
[4] diff-output of <Softmax> ( min -0.999998, max 0.988794, mean -4.03817e-07, stddev 0.0203039, skewness -22.6437, kurtosis 1366.84 ) 
### END BACKWARD


LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:308) 
### GRADIENT STATS :
Component 1 : <BlstmProjected>, ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  ### Gradients 
  f_w_gifo_x_corr_   ( min -49.2306, max 51.5157, mean 0.03924, stddev 3.07918, skewness -0.690135, kurtosis 33.5924 ) 
  f_w_gifo_r_corr_   ( min -54.5044, max 75.639, mean -0.000518601, stddev 2.2473, skewness 0.412362, kurtosis 37.0351 ) 
  f_bias_corr_   ( min -29.6241, max 50.2043, mean 0.237905, stddev 4.06998, skewness 3.07692, kurtosis 38.6826 ) 
  f_peephole_i_c_corr_   ( min -25.1969, max 107.279, mean 0.34068, stddev 7.33546, skewness 9.66646, kurtosis 139.87 ) 
  f_peephole_f_c_corr_   ( min -154.444, max 50.3698, mean -0.250168, stddev 12.6638, skewness -5.61455, kurtosis 69.5251 ) 
  f_peephole_o_c_corr_   ( min -94.9925, max 250, mean -0.0789193, stddev 19.4159, skewness 6.43198, kurtosis 86.921 ) 
  f_w_r_m_corr_   ( min -35.0297, max 40.4875, mean -0.0031374, stddev 3.80063, skewness 0.0446057, kurtosis 4.9016 ) 
  ---
  b_w_gifo_x_corr_   ( min -78.1248, max 74.5593, mean -0.0016149, stddev 3.76235, skewness -0.96369, kurtosis 26.0442 ) 
  b_w_gifo_r_corr_   ( min -32.845, max 32.1778, mean -0.004766, stddev 2.47778, skewness -0.0938255, kurtosis 8.23894 ) 
  b_bias_corr_   ( min -38.2055, max 48.7217, mean -0.191273, stddev 5.74615, skewness 0.972311, kurtosis 17.7403 ) 
  b_peephole_i_c_corr_   ( min -75.0043, max 28.2807, mean -0.0431453, stddev 5.84938, skewness -6.12522, kurtosis 85.7722 ) 
  b_peephole_f_c_corr_   ( min -33.2978, max 39.7612, mean 0.178721, stddev 7.83713, skewness 0.195353, kurtosis 3.54375 ) 
  b_peephole_o_c_corr_   ( min -46.1792, max 53.638, mean 0.278881, stddev 10.9094, skewness 0.315222, kurtosis 4.69009 ) 
  b_w_r_m_corr_   ( min -31.3062, max 34.3461, mean -0.0152406, stddev 3.88852, skewness -0.0107113, kurtosis 1.5369 ) 

  ### Activations (mostly after non-linearities)
  YI_FW(0..1)^   ( min 0, max 1, mean 0.43228, stddev 0.34363, skewness 0.359394, kurtosis -1.2941 ) 
  YF_FW(0..1)^   ( min 0, max 1, mean 0.635144, stddev 0.317263, skewness -0.503382, kurtosis -1.01989 ) 
  YO_FW(0..1)^   ( min 0, max 1, mean 0.363524, stddev 0.349889, skewness 0.656111, kurtosis -1.09248 ) 
  YG_FW(-1..1)   ( min -1, max 1, mean 0.0238449, stddev 0.868595, skewness -0.0464409, kurtosis -1.80308 ) 
  YC_FW(-R..R)*  ( min -50, max 50, mean 0.360011, stddev 13.2663, skewness 0.14017, kurtosis 9.414 ) 
  YH_FW(-1..1)   ( min -1, max 1, mean 0.035732, stddev 0.680824, skewness -0.0645318, kurtosis -1.22448 ) 
  YM_FW(-1..1)   ( min -1, max 1, mean 0.00332494, stddev 0.326349, skewness -0.0677362, kurtosis 2.86352 ) 
  YR_FW(-R..R)   ( min -4.05694, max 4.44906, mean 0.0199593, stddev 0.742118, skewness 0.0479827, kurtosis 1.22603 ) 
  ---
  YI_BW(0..1)^   ( min 0, max 1, mean 0.458097, stddev 0.327903, skewness 0.251966, kurtosis -1.32166 ) 
  YF_BW(0..1)^   ( min 0, max 1, mean 0.64564, stddev 0.279032, skewness -0.537881, kurtosis -0.666857 ) 
  YO_BW(0..1)^   ( min 0, max 1, mean 0.38255, stddev 0.353279, skewness 0.558868, kurtosis -1.22096 ) 
  YG_BW(-1..1)   ( min -1, max 1, mean 0.0120908, stddev 0.852368, skewness -0.0230752, kurtosis -1.77452 ) 
  YC_BW(-R..R)*  ( min -50, max 50, mean 1.17781, stddev 11.6224, skewness 1.04762, kurtosis 11.4486 ) 
  YH_BW(-1..1)   ( min -1, max 1, mean 0.0452888, stddev 0.693035, skewness -0.0621875, kurtosis -1.29409 ) 
  YM_BW(-1..1)   ( min -0.999996, max 1, mean 0.0054983, stddev 0.336302, skewness 0.022055, kurtosis 2.27546 ) 
  YR_BW(-R..R)   ( min -4.96364, max 4.51912, mean -0.0176836, stddev 0.776081, skewness -0.0642471, kurtosis 1.46912 ) 

  ### Derivatives (w.r.t. inputs of non-linearities)
  DI_FW^  ( min -1, max 1, mean -7.34315e-06, stddev 0.0219766, skewness -0.524707, kurtosis 305.803 ) 
  DF_FW^  ( min -1, max 1, mean 0.000124112, stddev 0.0185607, skewness -0.919623, kurtosis 594.648 ) 
  DO_FW^  ( min -1, max 1, mean 0.000101137, stddev 0.0255903, skewness 0.0954474, kurtosis 64.9765 ) 
  DG_FW   ( min -1, max 1, mean 2.4855e-05, stddev 0.0291815, skewness 1.06438, kurtosis 398.097 ) 
  DC_FW*  ( min -22.8082, max 25.1469, mean 2.6885e-05, stddev 0.339132, skewness 0.335905, kurtosis 1296.73 ) 
  DH_FW   ( min -4.22728, max 4.66016, mean 2.71489e-05, stddev 0.115323, skewness 0.0943128, kurtosis 56.0183 ) 
  DM_FW   ( min -5.50749, max 5.35626, mean -0.000166511, stddev 0.301208, skewness -0.0254975, kurtosis 11.1838 ) 
  DR_FW   ( min -1.22508, max 1.52458, mean -4.87325e-05, stddev 0.0849784, skewness 0.00147078, kurtosis 6.27584 ) 
  ---
  DI_BW^  ( min -1, max 1, mean -8.91463e-05, stddev 0.0223703, skewness 0.0659781, kurtosis 292.201 ) 
  DF_BW^  ( min -1, max 1, mean -0.0001186, stddev 0.0158616, skewness 0.187562, kurtosis 392.746 ) 
  DO_BW^  ( min -0.592889, max 0.554471, mean -0.000200416, stddev 0.0204101, skewness -0.0678384, kurtosis 44.3714 ) 
  DG_BW   ( min -1, max 1, mean 0.000212987, stddev 0.0345034, skewness 0.18527, kurtosis 215.368 ) 
  DC_BW*  ( min -14.7438, max 15.7429, mean 0.000192936, stddev 0.269369, skewness 1.03318, kurtosis 586.328 ) 
  DH_BW   ( min -2.41979, max 2.0554, mean 0.000409238, stddev 0.0950552, skewness 0.0159339, kurtosis 35.5387 ) 
  DM_BW   ( min -3.73396, max 3.1796, mean 0.00218011, stddev 0.250233, skewness 0.0180667, kurtosis 6.76846 ) 
  DR_BW   ( min -1.03574, max 1.01548, mean -0.000332394, stddev 0.0877884, skewness 0.000377683, kurtosis 4.62655 ) 
Component 2 : <Tanh>, 
Component 3 : <AffineTransform>, 
  linearity_grad ( min -104.326, max 122.486, mean -4.31325e-08, stddev 1.88605, skewness 0.433902, kurtosis 461.691 ) , lr-coef 1, max-norm 0
  bias_grad ( min -244.992, max 260.071, mean 2.14577e-07, stddev 10.3147, skewness 2.01444, kurtosis 561.451 ) , lr-coef 1
Component 4 : <Softmax>, 
### END GRADIENT

LOG (ali-to-pdf[5.5.1074~1-71f3]:main():ali-to-pdf.cc:68) Converted 2919 alignments to pdf sequences.
LOG (ali-to-post[5.5.1074~1-71f3]:main():ali-to-post.cc:73) Converted 2919 alignments.
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:168) MC05_98, missing targets
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:334) ### After 755062 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:335) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.534412, max 0.575167, mean 0.00392646, stddev 0.0828441, skewness 0.0129826, kurtosis 0.451696 ) 
  f_w_gifo_r_   ( min -0.396732, max 0.402796, mean -0.000584577, stddev 0.0776614, skewness -0.00137377, kurtosis 0.00726247 ) 
  f_bias_   ( min -0.351988, max 1.33522, mean 0.213059, stddev 0.460789, skewness 1.06584, kurtosis -0.65511 ) 
  f_peephole_i_c_   ( min -0.481604, max 0.485605, mean -0.00317217, stddev 0.126939, skewness 0.119733, kurtosis 1.24239 ) 
  f_peephole_f_c_   ( min -0.697312, max 0.899064, mean 0.00409983, stddev 0.167501, skewness 0.433853, kurtosis 5.04795 ) 
  f_peephole_o_c_   ( min -0.499965, max 0.483306, mean -0.0107627, stddev 0.182714, skewness 0.172178, kurtosis -0.27957 ) 
  f_w_r_m_   ( min -0.544066, max 0.502354, mean 0.000607742, stddev 0.101407, skewness 0.00228731, kurtosis -0.00437164 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -1.27296, max 0.911836, mean 0.00641885, stddev 0.0886868, skewness -0.099603, kurtosis 2.71066 ) 
  b_w_gifo_r_   ( min -0.36186, max 0.308144, mean -0.000210136, stddev 0.0702765, skewness 8.17872e-05, kurtosis -0.310479 ) 
  b_bias_   ( min -0.342501, max 1.19224, mean 0.206842, stddev 0.451399, skewness 1.04729, kurtosis -0.689132 ) 
  b_peephole_i_c_   ( min -0.351871, max 0.292619, mean 0.00521996, stddev 0.0943552, skewness -0.111132, kurtosis 0.931633 ) 
  b_peephole_f_c_   ( min -0.668495, max 0.697546, mean 0.0114531, stddev 0.164035, skewness 0.464016, kurtosis 3.8556 ) 
  b_peephole_o_c_   ( min -0.549219, max 0.494779, mean -0.0164459, stddev 0.18734, skewness -0.131944, kurtosis 0.207577 ) 
  b_w_r_m_   ( min -0.405403, max 0.363496, mean -0.00025552, stddev 0.0899801, skewness -0.000330038, kurtosis -0.0753531 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.925742, max 0.71254, mean -0.000155919, stddev 0.106477, skewness 0.00606121, kurtosis 0.0643852 ) , lr-coef 1, max-norm 0
  bias ( min -0.0827118, max 2.31625, mean 9.31323e-10, stddev 0.0762138, skewness 23.0727, kurtosis 672.905 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:336) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -14.8312, max 13.8231, mean 0.00769689, stddev 0.940803, skewness 0.731578, kurtosis 12.6034 ) 
[1] output of <BlstmProjected> ( min -4.1126, max 4.09823, mean 0.000504753, stddev 0.655574, skewness -0.0162529, kurtosis 3.18411 ) 
[2] output of <Tanh> ( min -0.999465, max 0.999449, mean 0.000624024, stddev 0.430108, skewness -0.00167999, kurtosis 0.187543 ) 
[3] output of <AffineTransform> ( min -13.7587, max 20.1036, mean 0.00908579, stddev 1.98983, skewness 0.970412, kurtosis 5.66848 ) 
[4] output of <Softmax> ( min 4.16116e-12, max 0.999712, mean 0.000781028, stddev 0.0179492, skewness 43.7031, kurtosis 2061.17 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:338) 
### BACKWARD PROPAGATION BUFFER CONTENT :
[0] diff of <Input>  ( min -2.38989, max 2.62085, mean 0.000632057, stddev 0.191823, skewness -0.387063, kurtosis 17.6963 ) 
[1] diff-output of <BlstmProjected> ( min -0.458011, max 0.48104, mean -0.000121387, stddev 0.0394078, skewness -0.0254497, kurtosis 11.8981 ) 
[2] diff-output of <Tanh> ( min -0.478898, max 0.484735, mean -9.75582e-05, stddev 0.053733, skewness 0.0161515, kurtosis 7.31255 ) 
[3] diff-output of <AffineTransform> ( min -0.998499, max 0.891813, mean -7.43273e-09, stddev 0.0149568, skewness -32.2329, kurtosis 2637.86 ) 
[4] diff-output of <Softmax> ( min -0.998499, max 0.891813, mean -7.43273e-09, stddev 0.0149568, skewness -32.2329, kurtosis 2637.86 ) 
### END BACKWARD


LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:339) 
### GRADIENT STATS :
Component 1 : <BlstmProjected>, ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  ### Gradients 
  f_w_gifo_x_corr_   ( min -35.5224, max 31.6662, mean 0.117427, stddev 3.73917, skewness 0.00238177, kurtosis 7.2187 ) 
  f_w_gifo_r_corr_   ( min -34.9796, max 39.4292, mean -0.000123644, stddev 3.16642, skewness 0.0152136, kurtosis 7.68382 ) 
  f_bias_corr_   ( min -23.1166, max 27.6413, mean -0.360499, stddev 4.63805, skewness 0.0612611, kurtosis 4.91379 ) 
  f_peephole_i_c_corr_   ( min -59.959, max 84.7144, mean 0.0160799, stddev 8.69497, skewness 1.9915, kurtosis 36.7991 ) 
  f_peephole_f_c_corr_   ( min -101.132, max 145.366, mean -1.25998, stddev 17.4287, skewness 0.703106, kurtosis 20.9999 ) 
  f_peephole_o_c_corr_   ( min -162.544, max 114.672, mean -2.38341, stddev 20.8709, skewness -1.01402, kurtosis 15.5992 ) 
  f_w_r_m_corr_   ( min -56.6646, max 57.121, mean -0.0179241, stddev 4.73244, skewness 0.0525884, kurtosis 4.86106 ) 
  ---
  b_w_gifo_x_corr_   ( min -47.9816, max 82.737, mean 0.156522, stddev 4.32835, skewness 0.448707, kurtosis 16.498 ) 
  b_w_gifo_r_corr_   ( min -49.7801, max 50.8026, mean -0.0133699, stddev 3.24577, skewness -0.0939832, kurtosis 8.03512 ) 
  b_bias_corr_   ( min -48.5361, max 60.7665, mean -0.498095, stddev 6.81958, skewness 0.869267, kurtosis 15.5392 ) 
  b_peephole_i_c_corr_   ( min -52.4388, max 44.6849, mean 0.380036, stddev 6.92046, skewness -0.261175, kurtosis 21.7092 ) 
  b_peephole_f_c_corr_   ( min -118.186, max 84.5498, mean -0.0779939, stddev 14.7926, skewness -0.659572, kurtosis 17.4503 ) 
  b_peephole_o_c_corr_   ( min -77.8819, max 75.6128, mean -0.0876184, stddev 14.9161, skewness -0.151735, kurtosis 5.65279 ) 
  b_w_r_m_corr_   ( min -44.5642, max 34.3486, mean 0.0111433, stddev 4.76243, skewness -0.00570487, kurtosis 1.34305 ) 

  ### Activations (mostly after non-linearities)
  YI_FW(0..1)^   ( min 0, max 1, mean 0.319804, stddev 0.350738, skewness 0.753329, kurtosis -0.911844 ) 
  YF_FW(0..1)^   ( min 0, max 1, mean 0.472828, stddev 0.387823, skewness 0.0085543, kurtosis -1.58635 ) 
  YO_FW(0..1)^   ( min 0, max 1, mean 0.269612, stddev 0.340489, skewness 1.06341, kurtosis -0.381172 ) 
  YG_FW(-1..1)   ( min -1, max 1, mean 0.0166229, stddev 0.747435, skewness -0.0277681, kurtosis -1.38703 ) 
  YC_FW(-R..R)*  ( min -50, max 50, mean 0.19801, stddev 10.2244, skewness 0.214316, kurtosis 17.0064 ) 
  YH_FW(-1..1)   ( min -1, max 1, mean 0.0252854, stddev 0.5871, skewness -0.0394064, kurtosis -0.621534 ) 
  YM_FW(-1..1)   ( min -1, max 1, mean 0.00520194, stddev 0.278638, skewness -0.0779426, kurtosis 5.03604 ) 
  YR_FW(-R..R)   ( min -3.93694, max 4.09823, mean 0.00708102, stddev 0.636806, skewness 0.0911545, kurtosis 3.03897 ) 
  ---
  YI_BW(0..1)^   ( min 0, max 1, mean 0.339972, stddev 0.349545, skewness 0.632102, kurtosis -1.08381 ) 
  YF_BW(0..1)^   ( min 0, max 1, mean 0.477217, stddev 0.370732, skewness -0.0700135, kurtosis -1.48888 ) 
  YO_BW(0..1)^   ( min 0, max 1, mean 0.28278, stddev 0.348576, skewness 0.981581, kurtosis -0.583774 ) 
  YG_BW(-1..1)   ( min -1, max 1, mean 0.0115915, stddev 0.744546, skewness -0.0205815, kurtosis -1.38368 ) 
  YC_BW(-R..R)*  ( min -50, max 50, mean 0.681667, stddev 9.07209, skewness 1.33636, kurtosis 19.2276 ) 
  YH_BW(-1..1)   ( min -1, max 1, mean 0.0293295, stddev 0.593412, skewness -0.0196175, kurtosis -0.680281 ) 
  YM_BW(-1..1)   ( min -0.999992, max 0.999998, mean 0.00266608, stddev 0.291494, skewness -0.0288953, kurtosis 4.16174 ) 
  YR_BW(-R..R)   ( min -4.1126, max 3.8001, mean -0.00607746, stddev 0.669941, skewness -0.105698, kurtosis 3.33542 ) 

  ### Derivatives (w.r.t. inputs of non-linearities)
  DI_FW^  ( min -1, max 1, mean -0.000134186, stddev 0.0240851, skewness -3.54692, kurtosis 528.67 ) 
  DF_FW^  ( min -1, max 1, mean 0.000150383, stddev 0.0185959, skewness 9.3459, kurtosis 744.882 ) 
  DO_FW^  ( min -0.903657, max 0.749025, mean -0.000154508, stddev 0.0255611, skewness -1.14003, kurtosis 143.165 ) 
  DG_FW   ( min -1, max 1, mean 0.000119043, stddev 0.0295258, skewness 1.10036, kurtosis 522.902 ) 
  DC_FW*  ( min -10.0447, max 6.06586, mean 0.00336143, stddev 0.259774, skewness -4.28042, kurtosis 271.966 ) 
  DH_FW   ( min -3.97782, max 3.14763, mean 0.000898735, stddev 0.123133, skewness 0.545668, kurtosis 95.3353 ) 
  DM_FW   ( min -5.39721, max 5.71243, mean 0.00061529, stddev 0.308212, skewness 0.282481, kurtosis 31.7047 ) 
  DR_FW   ( min -1.11579, max 1.42857, mean -0.00016224, stddev 0.0824144, skewness 0.0783517, kurtosis 20.0132 ) 
  ---
  DI_BW^  ( min -0.802811, max 1, mean -0.000236608, stddev 0.0178742, skewness -0.570819, kurtosis 250.664 ) 
  DF_BW^  ( min -0.555862, max 0.739935, mean -0.000141654, stddev 0.0129142, skewness -0.41537, kurtosis 231.432 ) 
  DO_BW^  ( min -0.400336, max 0.612166, mean -8.49283e-05, stddev 0.0171052, skewness 0.403251, kurtosis 78.3485 ) 
  DG_BW   ( min -1, max 1, mean 0.000136899, stddev 0.02939, skewness 3.93465, kurtosis 330.677 ) 
  DC_BW*  ( min -8.36117, max 10.5376, mean -0.000772245, stddev 0.217685, skewness 1.91141, kurtosis 491.803 ) 
  DH_BW   ( min -2.30319, max 2.27207, mean 0.000139175, stddev 0.0881508, skewness -0.501286, kurtosis 65.9435 ) 
  DM_BW   ( min -2.87456, max 2.85321, mean 0.000339635, stddev 0.204764, skewness -0.0684751, kurtosis 13.1038 ) 
  DR_BW   ( min -0.983297, max 0.996896, mean -0.000233124, stddev 0.0717813, skewness -0.0774433, kurtosis 9.94965 ) 
Component 2 : <Tanh>, 
Component 3 : <AffineTransform>, 
  linearity_grad ( min -84.7166, max 67.2081, mean -4.6753e-08, stddev 1.85989, skewness -0.243427, kurtosis 99.924 ) , lr-coef 1, max-norm 0
  bias_grad ( min -150.199, max 99.5619, mean 2.68221e-08, stddev 6.25864, skewness -9.47675, kurtosis 331.149 ) , lr-coef 1
Component 4 : <Softmax>, 
### END GRADIENT

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:346) PER-CLASS PERFORMANCE:
@@@ Frames per-class: [ 181212 144 127 606 532 43 173 190 515 703 138 211 211 608 45 289 271 572 372 153 368 892 307 454 467 612 374 177 314 165 203 169 193 120 162 350 506 100 142 40889 89709 154 463 1777 286 231 687 352 545 459 79 394 481 237 200 663 1037 475 2210 428 257 167 4192 1910 93 230 151 223 4363 199 339 612 931 689 168 53 430 425 404 147 72 468 398 184 543 242 103 448 1376 60 779 717 668 146 541 163 358 830 561 408 249 268 103 1105 608 477 208 629 257 136 769 185 359 2082 165 69 303 74 247 336 172 147 117 128 443 612 117 125 52 316 956 165 305 574 202 297 222 1167 208 209 172 556 182 606 267 121 534 323 470 239 776 321 812 179 268 1139 537 191 554 5133 217 193 202 128 264 242 282 141 453 190 390 248 274 208 168 117 250 72 227 832 107 208 74 120 577 200 74 173 93 435 313 308 638 462 201 239 312 172 59 151 330 221 210 68 77 424 248 789 348 126 86 566 208 142 331 485 286 372 234 328 637 273 293 132 41 150 57 102 173 655 328 426 861 178 245 87 533 72 131 265 150 790 182 357 233 69 187 112 206 273 85 62 302 205 411 179 278 260 193 525 723 141 288 139 513 788 215 97 150 236 273 455 166 615 208 63 196 43 120 342 911 206 87 1156 192 146 131 166 31 221 252 417 373 499 401 233 179 409 182 439 579 96 189 251 1812 272 55 134 245 227 120 170 454 69 211 126 232 303 701 89 59 87 90 418 122 334 282 91 119 125 379 59 189 157 163 620 120 203 224 199 71 381 86 544 80 247 220 94 114 76 256 175 1186 420 378 336 135 140 331 666 306 356 196 203 291 152 221 543 214 149 360 365 670 264 264 692 216 616 923 524 408 234 395 301 375 105 525 80 359 98 456 204 309 258 288 286 181 90 101 263 53 253 167 354 150 174 194 393 579 494 437 273 142 236 159 226 115 606 198 255 508 136 282 304 327 234 1467 199 113 213 747 267 237 238 267 223 423 127 334 351 773 400 668 269 762 507 73 172 259 944 420 60 318 817 125 150 155 430 99 155 360 316 425 603 146 124 301 210 170 102 140 531 288 716 282 161 233 181 349 120 133 242 135 51 141 111 147 205 551 427 131 99 109 133 445 7274 204 271 135 128 125 377 120 372 449 808 108 557 45 280 47 596 57 276 400 121 347 315 193 77 146 96 105 216 193 341 1125 236 219 105 417 109 338 639 130 396 177 211 138 121 275 91 104 194 51 542 461 346 197 315 121 109 444 210 64 135 84 52 96 283 268 315 144 187 498 569 470 194 79 151 71 285 409 404 165 215 321 466 69 209 585 117 83 141 150 364 478 225 439 81 192 156 773 344 306 243 112 146 203 501 258 143 117 231 316 156 83 53 468 163 539 465 176 116 79 157 350 139 86 253 155 101 132 342 440 324 189 390 153 111 192 365 208 291 263 103 139 134 307 402 452 451 5943 88 406 111 189 488 786 548 86 632 99 382 216 141 365 388 65 90 187 430 300 463 604 247 79 314 305 463 45 473 718 330 223 226 1312 201 160 62 170 227 141 90 90 193 68 539 610 241 156 182 257 224 114 99 209 178 32 193 174 235 238 150 123 308 261 195 275 142 111 371 821 518 831 306 124 432 87 357 932 174 327 225 300 36 190 272 120 358 461 397 462 306 105 348 166 390 216 520 219 260 71 179 258 429 201 53 142 256 406 60 128 173 26 88 191 83 60 155 179 1000 125 44 191 230 383 687 216 257 321 233 417 202 1061 342 327 163 248 165 412 229 303 517 124 815 539 79 306 155 255 89 321 245 73 308 228 196 192 269 176 228 101 8591 469 337 966 237 430 177 258 334 85 304 414 487 401 57 178 99 260 178 151 248 261 926 126 248 230 264 241 147 220 155 139 646 219 209 393 1097 85 160 936 605 267 175 213 208 155 166 207 194 260 180 542 294 745 650 41 235 409 387 268 281 172 798 467 256 138 171 316 378 542 197 77 199 253 693 506 326 151 543 272 142 171 242 241 191 161 274 304 47 226 320 127 248 43 388 236 233 244 332 111 261 196 246 68 466 707 315 146 315 282 30 427 201 153 510 1026 278 186 316 45 464 106 316 404 415 543 346 257 452 1347 138 153 366 558 585 171 109 855 530 221 155 160 161 111 196 131 341 863 116 777 103 428 210 393 95 260 221 295 129 320 516 185 97 126 343 492 405 270 645 150 141 159 382 1153 426 252 535 332 205 417 575 597 292 372 180 236 213 343 301 154 349 528 290 243 378 616 213 266 230 676 309 338 165 324 314 639 310 148 365 184 119 116 199 264 1367 367 467 1442 213 598 641 335 156 122 351 114 129 98 489 213 162 386 116 74 274 197 265 173 129 167 753 211 214 1711 286 746 207 83 123 191 204 353 783 57 223 151 323 285 99 1197 111 116 189 130 401 488 428 96 280 248 468 506 117 284 315 328 129 334 124 557 265 243 383 268 66 97 474 153 221 139 391 194 634 156 322 748 309 195 326 651 462 370 172 385 490 392 610 304 249 70 501 324 335 228 370 206 587 560 108 850 256 209 153 168 206 131 204 232 340 200 284 174 301 124 107 220 67 370 1390 1253 311 434 127 136 104 493 416 778 103 335 338 242 66 208 459 231 96 291 160 220 368 145 728 458 351 255 350 348 137 125 1039 1466 285 221 232 361 605 549 4192 543 376 361 3105 243 319 117 225 232 542 309 859 113 244 913 235 271 853 881 252 243 274 129 90 371 274 176 453 107 172 140 172 411 267 778 147 558 2509 132 95 258 162 203 87 21 467 192 231 265 665 552 160 225 162 279 3235 297 303 235 317 297 155 692 494 82 179 192 734 178 103 204 87 225 63 351 225 260 282 352 367 219 77 182 384 386 147 340 273 260 234 61 251 141 663 194 320 94 144 ]
@@@ Loss per-class: [ 0.338724 0.993772 1.43092 0.883641 1.0351 1.72343 1.63922 1.01251 1.27164 1.84274 1.00711 1.80441 1.40293 1.09871 3.77992 0.96301 1.21274 1.52951 1.64999 1.33854 0.979613 1.1922 1.33767 0.840857 0.763074 1.32391 0.766067 2.61878 1.06442 1.97161 1.20485 0.639548 1.4798 1.75219 0.661343 1.1607 1.83884 1.81312 1.58069 1.36276 0.532532 1.35549 1.24843 1.08501 1.57039 1.12275 0.607894 1.79876 1.47353 0.989873 1.7378 1.47541 1.35237 1.70721 1.25241 1.24834 0.96344 0.723616 0.801209 1.31176 1.57521 1.03758 0.399695 0.379744 2.18003 1.6256 1.98899 1.51066 1.08793 0.840891 0.806459 0.781881 0.822834 1.28533 1.25404 1.8253 0.473262 0.31882 0.813697 1.06615 1.34431 1.47291 0.500471 0.981715 1.21052 1.43341 2.00902 1.36679 1.31064 2.00582 0.917607 1.02534 0.925178 2.61597 1.44466 2.19118 1.04725 0.57581 0.760847 0.966855 2.22571 1.12871 1.88875 0.817944 0.708302 0.877543 1.81562 1.49112 1.32089 1.21729 1.75184 1.89043 2.13094 0.68044 1.86298 3.48858 1.58939 1.67976 2.41786 1.41068 1.81051 3.08454 1.76298 1.88407 0.881393 0.682751 1.69746 2.12512 1.93864 0.975134 0.511454 1.68867 0.847844 1.05067 2.15015 0.816991 1.11755 0.516089 1.44216 1.57462 1.75473 0.906936 1.20679 0.905063 0.927464 1.52385 1.55331 1.87747 1.05215 0.713561 0.334357 1.34501 1.59991 2.0674 1.46361 0.713341 0.487684 0.854626 0.834533 0.539943 1.05533 1.58509 1.99953 4.61575 1.23775 1.50387 1.04968 1.57507 0.753451 1.73369 0.676859 1.48036 1.31277 1.32027 1.96705 0.887758 1.09404 1.04766 2.20478 0.419507 1.78272 1.71941 1.76666 1.38898 0.353091 1.37124 3.39193 1.75815 2.06816 1.04106 0.992046 1.22788 0.500097 0.967991 1.23907 1.26768 1.65298 1.91473 2.14586 0.960312 2.2911 1.96599 1.52116 2.71031 2.42301 1.10169 2.23663 1.07297 1.22147 1.89145 2.01502 1.38396 0.821042 12.7262 1.68528 0.863701 1.59659 0.823424 0.854827 1.81449 1.14728 0.888171 2.06159 1.27724 1.88427 1.86036 2.8913 1.54862 1.17323 1.09604 1.62573 1.01983 1.10294 2.77507 1.11595 2.79759 0.910481 2.72649 1.74438 1.36392 1.59737 0.599717 1.38126 1.23479 1.60027 3.00296 1.34724 4.21061 2.11922 1.80062 3.10882 1.76374 1.63253 1.62218 1.57481 0.953173 2.3717 1.08076 0.949776 1.07628 1.09867 1.84028 1.56744 2.71228 1.23627 0.930005 1.83987 1.90423 1.74724 1.70941 1.10328 1.36602 2.43213 2.02901 2.87921 2.43767 1.49084 3.98449 1.13443 1.95202 1.482 2.07589 1.78115 0.873837 1.25936 1.91582 2.48408 1.30492 8.0798 2.59056 1.73367 2.31167 0.924695 0.779832 2.15841 1.46732 1.18369 1.15325 1.73824 1.89012 0.745855 1.55147 1.24186 1.01171 0.394622 1.0946 2.92832 1.80597 2.17827 2.20952 1.4729 1.5326 1.23753 1.85988 1.93857 2.46936 0.754018 1.53943 0.574896 2.0487 2.86599 0.976053 2.84192 0.628829 2.10348 2.52916 1.90544 4.10399 1.9756 1.37122 1.11139 2.0229 2.2634 1.48293 1.7723 0.778504 2.1754 1.89345 1.67928 1.2164 1.3692 1.12906 1.44837 1.10575 1.57205 1.28466 1.42476 2.04893 1.59676 1.76515 1.57746 1.46233 0.565737 0.339531 1.5065 1.21615 1.30576 1.5598 1.9908 0.529917 1.03578 2.41901 1.62969 2.801 1.08851 2.05675 0.701227 1.40846 0.892336 1.73643 1.57873 1.883 0.782653 1.64458 1.2368 0.815245 1.08334 0.907714 0.93532 1.41627 1.43578 1.44332 1.01586 1.1744 1.40488 1.7223 1.13508 1.74785 1.46172 1.8466 1.5311 1.91533 1.469 1.07378 0.900359 1.32008 2.03893 1.32671 1.83069 0.887494 4.01184 0.942495 1.40429 1.12863 2.30836 1.45676 1.33413 1.49321 1.31661 1.85436 2.97856 2.72797 1.75939 1.45999 1.54052 0.93019 1.63999 0.660394 2.78124 1.09312 0.731721 1.09581 0.820582 0.862866 1.1137 1.64408 0.379717 0.656466 2.44788 0.890239 1.33065 2.14552 1.79305 1.72358 1.21319 0.79119 1.39332 1.23391 1.20734 1.24488 0.600571 1.69336 0.61248 1.68995 1.42466 0.886172 2.93373 1.12716 1.47169 0.39652 1.18168 1.61033 1.36889 0.672832 1.31181 5.48008 0.844041 1.58954 1.80175 1.62625 0.757409 1.17513 2.10383 1.24039 1.82023 2.80782 1.65715 1.31878 1.63037 1.87909 0.703353 0.593383 1.34026 0.971346 1.2424 0.647903 0.857103 1.02093 1.37207 1.49119 4.31079 2.0597 1.2578 1.848 1.45557 0.958717 1.84937 1.42796 0.597867 1.74262 1.61271 1.73246 3.52947 2.12178 0.89555 0.734702 0.886995 2.279 2.85727 1.11044 1.7163 1.59828 1.58117 2.20934 0.79495 1.6267 2.76144 0.774645 3.61257 1.36109 3.53586 0.998828 1.98297 1.1871 1.25386 1.89925 2.02026 0.938812 2.77103 1.86642 3.00281 1.92174 1.31768 4.16414 2.03436 1.20412 0.838396 2.00876 1.68818 2.99666 2.23639 2.9299 1.51188 1.09959 2.12302 2.21043 1.28018 1.40809 1.47232 2.30481 1.47338 1.83147 1.95533 1.79895 3.01214 0.295718 0.851165 1.35475 1.00383 1.55889 2.2751 3.35767 1.22195 1.88063 2.25211 1.1489 2.24283 2.87249 2.15427 2.24714 3.32911 2.20675 2.0653 1.63716 0.68551 0.928139 0.696128 1.08948 2.17406 1.8877 2.0357 1.2529 1.17331 0.8658 2.06337 2.25848 1.09512 0.991555 2.81442 1.41808 1.09496 2.69546 1.85952 3.43687 1.37724 1.42063 1.0635 2.00845 1.70534 2.27671 1.12733 1.50697 1.11792 0.724493 1.98659 2.19206 3.30006 1.85862 2.4778 1.19484 2.40932 2.62305 2.43815 2.53823 1.52829 1.54249 1.20892 2.68716 0.490017 1.85084 1.09038 1.03246 1.43452 1.66852 1.45916 2.31722 2.87243 1.84923 1.32234 2.54175 2.01662 1.42056 1.81951 1.09637 1.4413 1.22313 1.4199 2.22272 1.32137 1.9639 2.97193 3.08257 1.51845 1.95156 1.04825 1.85446 1.27895 1.66642 1.59311 1.53645 0.96157 0.998434 0.878027 1.10757 1.04336 0.989953 2.19505 1.18914 0.923256 1.24997 5.01796 0.581359 3.08189 1.26619 2.09231 1.95614 1.39539 1.3868 5.80859 2.04349 1.5725 1.41853 0.822164 0.927995 0.912581 2.22734 2.34414 1.20119 1.75369 1.43462 6.96401 1.19166 0.880822 1.44954 2.86865 2.87178 1.12084 1.48599 2.32245 3.97993 2.06717 1.39501 1.60195 2.00384 1.67915 1.81705 2.98578 1.09499 0.912758 2.37189 2.34725 1.2488 2.13864 1.01891 2.4405 1.99168 2.07571 2.84464 3.45979 1.46969 1.24706 0.800498 1.3237 1.74998 1.74138 1.48101 1.38926 1.745 0.80976 1.57818 4.10215 0.861831 0.525351 1.1913 0.699265 1.33197 1.75589 1.58749 2.89865 3.05882 2.27967 1.96132 1.5564 1.97723 1.7591 4.64919 1.79051 2.27654 2.01257 0.805522 1.42119 1.01883 1.36594 2.31139 1.92123 1.50953 2.11315 1.46347 1.86054 1.00426 1.71435 1.39763 1.79988 1.40712 1.3938 1.17315 1.3019 4.07654 1.49046 1.56275 1.29074 2.34365 2.14584 1.52122 3.01141 2.70704 1.89666 3.6023 2.65283 1.48362 1.9369 0.726301 2.78496 3.35279 2.61765 2.23844 1.77504 0.753744 1.95412 1.29488 1.27861 1.36778 1.43175 1.15263 1.69748 1.63435 2.29334 3.38348 1.37032 1.46789 0.956211 1.02013 1.88418 1.13093 3.35273 0.499041 0.989389 1.76023 1.675 1.08001 1.37132 3.34555 1.06032 1.37926 3.31896 0.894154 1.90219 2.50944 1.6508 1.59074 2.39799 4.60007 2.27003 0.270485 0.856902 2.16774 0.701389 2.28522 1.59105 2.66915 1.24003 1.30233 5.2935 1.13039 1.46484 1.40466 1.84473 3.82442 2.36254 2.55 0.777043 1.41768 1.85844 2.29008 1.52665 0.974328 3.09482 2.01538 1.86709 1.85124 1.39769 3.13188 1.98571 1.50252 2.01291 1.0443 1.55306 2.69089 1.73007 0.970581 3.10356 1.70135 0.800448 1.19809 1.20119 1.37055 2.02481 2.10471 1.89471 1.76649 2.20268 2.03453 2.65797 2.02846 1.28346 1.40605 0.888428 1.57336 3.1066 1.80017 1.85138 1.08316 1.45326 1.26333 2.46972 1.05059 1.29536 1.94509 2.52112 0.712201 1.69666 1.77931 1.04745 1.78106 3.11669 2.80147 1.24334 0.899713 1.19757 1.59362 1.02463 0.812928 1.11913 3.02124 2.17157 1.80645 1.80465 2.20407 0.879117 1.38051 1.1904 1.37618 1.51724 1.14218 2.10378 0.853775 2.99724 1.09477 1.85376 1.67223 2.23525 1.38861 2.27898 1.37076 2.16916 1.3061 2.06269 0.769064 0.85032 1.38626 3.26185 1.03082 1.1196 6.2779 1.39752 2.52768 1.85306 1.23187 1.65408 1.59203 1.81821 1.83868 4.67892 1.91617 1.73325 1.40375 1.23297 1.51125 2.02422 2.15529 1.10511 0.938767 1.32446 2.25563 2.62746 1.80166 1.95181 1.56112 2.26913 2.45969 1.26443 1.51075 1.74201 2.32077 1.44684 1.77849 2.5459 1.16935 2.47233 1.3175 0.985875 1.07173 1.14341 1.94767 1.0784 0.84202 1.31613 1.96274 2.05189 1.4918 2.81028 2.1968 2.7125 1.46032 2.00072 1.69592 4.60489 1.99526 0.93266 1.07824 1.71281 1.1714 1.73064 2.42383 1.98306 2.19414 1.82705 1.54543 2.20271 1.33207 1.00843 1.93556 0.698186 0.894629 1.88596 1.51266 1.1598 1.4106 1.66821 1.62416 1.53147 1.2704 1.97246 1.68173 1.56974 0.922211 1.44705 1.14114 1.41385 1.42296 2.06476 1.5831 0.975165 1.08018 1.18481 1.49328 1.33226 1.42868 1.54093 2.33421 2.66137 1.62972 1.39288 1.00273 2.24193 1.7771 1.91891 1.38439 2.23577 1.53402 0.530103 0.959116 1.77204 0.688313 1.50371 1.02371 2.66929 2.00109 1.79807 2.37839 2.70315 1.49563 1.8011 1.42122 1.71775 2.71556 2.72524 1.91988 2.22657 2.24736 2.18777 1.82229 1.90152 2.38486 0.884513 2.74944 0.778526 2.61686 0.921477 2.89318 1.82069 3.45477 2.14978 1.85131 1.72025 0.610019 2.85975 1.13734 2.2595 1.83338 1.73447 3.51282 0.930546 2.6224 1.85935 1.32967 2.00857 0.994522 1.2479 0.973692 3.0073 1.34276 1.67333 1.74105 0.923221 1.92962 1.46763 1.86143 1.43773 2.65119 1.88347 2.48107 2.31315 2.34958 1.5742 0.801601 0.870156 2.63348 3.17978 1.06381 2.31862 2.57932 1.9689 1.32005 1.6042 1.49919 2.24396 1.05095 1.3556 1.10908 2.27567 1.50695 1.38348 1.98615 1.2067 1.94452 1.24984 1.88468 0.982899 0.810545 1.42297 2.74953 3.57335 2.03754 1.8325 2.43902 1.82472 1.2306 2.77395 1.07052 1.19186 1.70006 1.5649 2.28196 1.451 1.38553 0.957395 1.82722 2.61734 1.64132 2.01292 1.33006 2.30306 1.28515 2.32398 1.32166 2.7064 3.79682 1.95837 1.8817 1.70953 0.392102 0.420424 0.851227 1.82693 2.76034 2.01785 2.26611 1.5262 1.66383 0.91243 1.78151 2.06117 1.46842 0.980322 5.46669 1.57779 1.08821 1.27984 1.99309 1.81748 2.03356 1.03308 2.59659 2.07015 1.02465 0.90757 0.702719 1.5855 1.9474 0.795775 0.922289 1.43526 0.91744 0.794315 2.02478 1.44646 1.8188 1.19393 1.05636 1.12024 0.677599 1.92555 1.61799 1.10666 0.646945 1.93616 1.83856 1.60573 1.99333 1.87075 1.7972 2.313 0.709947 2.65567 1.11878 1.04345 1.83961 1.60697 1.69222 0.744131 1.23375 2.42385 2.16225 1.81396 2.85273 2.15807 1.6864 1.89316 1.66492 3.00452 1.69136 2.97414 1.14913 1.8543 2.722 1.75691 1.72008 1.22809 0.903728 1.85527 2.08435 2.45122 1.18832 2.08083 3.27067 10.0245 1.35161 1.18115 2.03441 2.24303 1.99355 2.67855 2.381 1.85797 1.27844 2.22633 0.839617 1.37325 1.90636 1.44188 2.09966 3.12259 2.30766 1.44949 1.88832 3.11154 3.2517 2.15893 0.876081 1.51112 1.66322 1.9197 2.80721 1.38947 4.44408 1.67918 1.93484 1.88851 1.94503 1.07411 1.80961 0.95267 3.23351 0.898966 1.18483 1.4418 2.36796 1.4703 1.21069 2.30663 2.13826 2.26654 1.59654 2.27569 1.12237 1.34028 2.35307 1.62585 2.16793 ]
@@@ Frame-accuracy per-class: [ 87.4802 74.7405 50.1961 78.1533 74.7418 39.0805 57.6369 67.7165 57.42 43.2125 75.0903 54.3735 60.9929 66.7214 6.59341 74.9568 62.6151 52.4017 47.2483 64.4951 73.8128 60.056 59.187 80.308 83.2086 61.7143 83.3111 18.5915 69.9523 48.9426 59.9509 82.5959 58.9147 36.5145 82.4615 71.3267 42.4482 40.796 56.8421 45.8064 80.1442 56.9579 57.6052 64.9226 47.8185 65.2268 82.4727 46.5248 47.846 68.7704 46.5409 55.5133 60.0208 47.5789 65.3367 60.8892 61.2048 79.7056 76.8152 55.0758 52.0388 64.4776 87.5611 87.7257 35.2941 40.3471 38.9439 57.2707 68.8897 75.188 78.056 78.3673 78.1535 58.5932 65.2819 56.0748 86.8757 93.067 76.6378 86.1017 63.4483 60.1921 87.0765 75.8808 68.0773 47.0103 42.5121 66.6667 63.0585 51.2397 71.9692 74.8432 73.7472 30.0341 62.4192 34.8624 75.5927 81.5172 79.0739 75.153 33.2665 74.4879 33.8164 76.436 79.7042 73.5079 46.5228 58.7768 57.4757 69.5971 43.1449 44.2049 39.7775 79.5678 45.9215 5.7554 48.7644 64.4295 29.899 59.7325 53.913 24.4068 54.4681 31.9066 75.31 77.3878 52.766 43.0279 43.8095 73.6177 82.8019 50.7553 73.3224 68.4073 39.0123 80 71.4607 85.2248 64.2686 55.3699 42.8986 73.1357 66.3014 72.3825 76.6355 60.9053 55.0047 52.5502 65.6748 76.4092 92.0798 55.9876 56.6154 35.0975 57.7281 77.2269 86.8837 77.8068 74.4815 81.9714 63.4483 54.2636 49.8765 0.77821 63.5161 63.0928 73.2743 61.4841 78.0595 45.1444 79.8976 56.338 55.7377 67.1463 43.9169 74.0426 68.6627 70.3448 38.6813 87.5676 38.1395 50.3597 49.6644 62.2407 93.5065 54.3641 6.71141 46.6859 31.016 71.1825 77.193 64.1815 88.0188 71.1351 61.0422 62.2129 42.88 46.9565 38.6555 79.868 26.6263 37.4718 61.7577 16.0584 23.2258 70.2002 28.169 72.5776 63.7016 37.9447 45.0867 56.4872 74.3405 0 50.6787 77.034 50.6108 78.1208 71.6418 52.3592 64.4706 75.3199 45.6559 63.3962 40.9639 42.5249 10.4348 46.8293 73.1988 60.2593 56.621 68.4642 70.1103 30.2521 72.9124 26.2857 73.477 16.5517 31.9392 61.3936 51.1628 79.9494 64.1096 64.6154 55.6745 7.19424 62.4 11.5556 38.2567 51.9196 18.7135 49.6 57.1901 55.4745 53.949 75.2089 40.5745 72.1689 71.3178 74.215 71.1818 51.5901 58.5789 46.595 63.2911 76.728 46.8677 36.9231 29.2359 65.5391 65.8135 51.1526 34.8348 32.3314 25.4197 25.1969 62.5954 2.29885 76.3485 45.2555 59.4624 38.7409 40 73.757 59.7403 48.4642 36.5019 58.2583 0 40.6321 56.2376 37.8443 75.2343 76.0761 40.5978 57.8158 67.4095 67.6435 50.9589 49.8294 78.6885 58.0311 60.686 69.1849 89.4897 74.8624 19.8198 43.8662 35.4379 38.2418 61.4108 60.4106 63.8064 38.8489 38.2979 27.668 86.0215 52.3888 83.1076 48.0447 11.7647 83.4286 20.9945 81.0036 35.102 25.4111 38.9381 7.65027 45.1883 63.745 66.1397 21.8487 42.2164 55.2381 57.4924 75.9065 35.6846 55.5283 47.216 62.6566 67.1329 65.5308 67.052 67.7686 49.6894 65.8586 57.5964 42.328 55.0218 49.6732 50.6823 56.9801 83.7758 91.7955 61.0304 67.7563 50.1845 67.6157 52.187 86.5716 69.4943 36.1851 56.4885 28.0098 73.7564 36.0656 85.7788 64.9494 74.5921 54.1806 54.0915 53.3516 75.9135 59.7353 55.5766 83.1769 75.7506 74.2903 73.4164 57.388 59.9755 52.0256 71.8078 65.6716 62.3169 49.2891 70.4091 42.236 62.3088 57.868 51.9168 47.9218 64.6204 65.764 65.5113 62.8272 46.281 64.0884 52.2167 72.1063 0 80.0789 57.3134 68.8293 43.8538 63.0372 59.126 57.1792 64.8835 46.7139 26.5143 34.0037 54.7368 57.9281 57.0533 77.7042 52.8139 80.7914 22.67 68.1018 82.0059 58.6081 77.5221 79.1461 71.145 63.5394 92.1976 81.7043 25.5507 73.0679 61.5385 37.757 49.6842 52.4109 67.6636 77.8523 57.6151 64.3137 68.1614 69.4168 81.9651 55.9301 82.1242 56.7718 63.082 77.0443 12.2449 70.1449 62.8131 90.1006 65.874 39.6694 67.8179 78.6544 75.6972 0 73.3119 53.4262 33.1658 54.6624 77.6699 66.9826 39.0129 59.4863 56.6553 24.8996 48.4245 61.7577 60.4106 44.878 86.121 84.666 58.9255 74.529 65.8407 85.4489 78.8009 71.0744 54.3634 58.0913 10.4869 43.299 70.1107 46.6019 57.9505 84.3049 44.0678 69.5864 85.2221 50.5263 47.9087 50.2513 8.21918 39.7004 73.4007 73.792 74.3276 31.3076 21.4022 63.035 47.012 50.8609 44.8133 40.5369 80.089 53.4323 11.0599 76.9507 8.79121 57.041 14.7368 72.5901 40 61.4828 63.9201 53.4979 42.5899 70.0475 18.0879 47.7419 24.5734 56.9948 63.5071 17.0901 48.062 59.7365 74.1892 41.8605 59.2255 52.1327 31.6168 10.0457 62.0384 62.7052 39.0805 54.9811 64.2254 59.5745 58.4838 16.4609 46.824 46.9945 45.933 54.4987 23.301 92.9032 75.1896 58.2973 76.962 55.4675 37.037 19.1781 69.9663 42.7553 31.0078 66.4207 29.5858 38.0952 31.0881 38.448 8.56611 43.1062 38.7543 57.0667 81.4443 73.9245 82.678 70.437 28.9308 47.5248 27.972 62.697 65.2015 77.8739 37.4622 39.4432 68.7403 73.3119 18.705 61.5752 65.2434 23.8298 56.2874 10.6007 56.4784 58.9849 64.1588 47.4501 48.6917 24.5399 76.8831 62.6198 69.8125 78.0842 44.6982 39.0144 32 51.1945 36.855 65.4038 32.4952 26.4808 22.9787 28.5097 50.5529 56.869 59.8802 14.9533 87.2999 49.5413 72.4745 75.4028 60.6232 54.0773 52.8302 36.1905 24.2511 49.4624 58.9595 33.1361 35.3698 60.0985 56.6038 68.3212 53.5755 67.1803 64.3799 38.1562 55.3746 41.2556 15.5844 27.0862 61.3909 48.0274 65.2751 48.3092 66.6667 34.2007 56.2602 57.1429 73.3702 73.9756 70.615 81.3559 65.6827 70.852 41.1609 66.3255 74.3802 57.4294 0 83.7945 20.1005 72.9412 44.8037 33.2155 71.409 58.1725 0 47.5138 46.9333 55.7491 79.2013 72.2762 75.1034 32.7273 27.673 67.7266 51.0638 55.2319 0 67.3706 73.3473 60.2118 22.3714 22.5166 68.1143 58.0645 34.891 0 48.0938 58.4615 60.7774 46.4088 49.7238 37.2093 10.219 70.0649 75.6757 21.118 32.5879 63.5616 40 74.3875 24.4541 34.1709 36.7542 21.8487 0 57.8811 62.4642 80.2548 63.3124 55.814 52.6316 52.188 65.7744 54.2199 77.677 48.4211 8.07175 73.4859 86.0621 64.4166 82.6218 65.2529 57.8313 54.104 14.8571 26.014 32.815 44.1261 58.9313 41.2417 36.2729 0 54.5932 38.8991 55.6017 76.9874 63.0553 70.6918 64 40.4568 37.9147 60.5452 39.6396 58.6428 46.1894 75.3122 54.2141 61.0365 39.1608 64.624 57.6402 65.4249 62.0347 1.86916 54.7368 60.4288 67.6507 34.7107 38.1323 59.366 7.54717 35.0282 39.6867 4.79042 21.4876 48.8746 57.3816 82.6587 11.1554 11.236 19.3211 29.0672 51.6297 72.5818 46.1894 61.7476 65.0078 62.0985 59.6407 61.7284 48.0452 64.2336 27.7863 9.17431 65.996 58.006 72.7273 71.8954 48.4349 72.2705 18.4739 88.1668 75.0695 47.7987 55.1387 67.5241 58.7084 25.6983 68.4292 63.5438 6.80272 78.4441 50.3282 35.6234 60.2597 53.4323 30.5949 26.2582 32.5123 91.0318 76.6773 32.2963 82.8764 43.3684 60.3949 35.493 68.0851 50.8221 11.6959 70.6076 55.7298 59.2821 47.5716 19.1304 32.493 21.1055 73.3205 53.2213 40.264 40.2414 61.5679 72.639 33.2016 41.0463 48.1562 47.259 63.7681 25.7627 53.0612 54.0193 43.7276 68.5228 46.0137 24.821 41.9314 68.1549 11.6959 57.3209 75.1735 68.3732 57.5701 62.6781 38.8759 45.5635 36.6559 45.6456 42.8916 44.2159 23.4165 37.1191 68.3871 59.0832 73.6419 55.1883 12.0482 39.4904 49.3284 68.129 48.7896 61.8117 31.8841 68.7539 66.738 45.614 41.8773 77.551 54.3444 48.0845 65.2535 44.0506 14.1935 38.5965 59.5661 76.8565 67.5222 56.049 66.0066 83.1647 66.422 23.1579 28.5714 41.2371 44.3064 38.1201 74.3034 62.6594 67.9803 56.8421 63.1347 71.1388 36.0784 74.8491 11.4943 68.9833 49.4715 47.5375 40.4908 68.2707 29.5964 64.2447 35.6234 60.8519 27.7372 81.0289 75.477 61.8067 8.87372 69.0967 70.4425 0 58.7135 26.799 54.0717 59.5495 52.4111 52.7828 48.7936 58.7678 2.1978 42.4112 53.5211 60.9795 70.9518 52.4669 44.5262 43.29 74.5631 70.0552 56.475 31.769 24.7557 48.8404 52.2829 52.9462 36.7347 34.7032 61.4845 57.6814 51.0158 30.8682 59.19 48.2972 35.8744 73.2824 13.6882 60.3221 74.2328 76.3948 64.1801 50.2415 69.7783 75.0594 69.8856 52.356 46.833 58.6907 15.5668 42.471 32.1373 57.3088 45.283 49.2308 0 38.4279 73.7056 67.8175 51.3863 70.488 49.8339 31.0954 36.3636 40.5229 43.0863 55.0996 38.8119 54.3417 73.3835 52.0681 72.8144 70.1998 46.6946 51.2821 65.7718 62.6039 53.277 57.1429 52.4017 64.0133 44.6602 53.505 50.1419 75.7315 59.1376 66.8428 56.7721 62.2951 39.7749 57.2668 73.4664 73.6672 67.356 59.8187 64.0986 55.9618 61.4543 36.715 24.9158 52.2572 60.1626 67.7824 36.0515 48.1203 45.7467 56.0146 45.7143 61.6043 80.9012 75.8782 43.7761 81.06 56.6319 65.8147 35.9184 52.6316 53.2751 34.749 11.1675 52.5026 59.0164 68.3077 50.9702 20.6009 17.4497 41.5301 45.0633 33.8983 33.4294 50.9653 40.597 35.8328 75.6501 29.3706 74.7882 29.6684 71.5338 24.0964 50.2994 12.9555 34.9869 52.8117 51.768 83.344 8.69565 68.0089 37.6238 45.4405 53.9405 6.03015 73.3194 33.1839 44.6352 65.4354 47.5096 66.2516 73.2856 67.9113 18.6528 69.1622 57.1429 48.666 72.458 41.7021 52.0211 51.0301 63.6225 24.7104 42.7504 26.506 34.0807 34.275 59.5483 79.7914 73.743 24.0602 33.8462 74.3941 32.5733 32.0542 44.4444 59.5147 58.6118 62.8842 31.9489 72.2481 60.7882 66.8821 32.7366 48.6983 54.6431 45.1892 70.1754 40 59.144 47.9103 73.8854 78.9517 64.0394 31.6633 0 41.8744 53.621 34.2772 50.3282 66.1269 10.1695 70.4681 60.4817 47.9263 56.555 28.0702 55.3699 51.4658 75.9644 42.1308 22.8137 58.6797 43.871 64.3172 38.404 61.1599 36.1032 62.0232 23.2932 13.9535 42.6304 44.4444 55.0607 89.2485 89.988 73.5152 44.4189 23.5294 41.7582 44.9761 51.4691 51.6206 76.1721 54.1063 36.9598 53.1758 73.8144 0 53.2374 70.7291 67.3866 48.7047 58.6621 48.5981 69.3878 34.1927 39.8625 73.9876 74.5911 83.6415 58.317 52.4964 79.1966 72.7273 47.012 77.1525 75.2813 49.387 53.2731 55.914 74.1355 71.346 68.9718 80.2862 41.3983 50.7304 67.4965 78.6347 48.46 56.651 66.383 46.5632 42.5806 42.212 33.2795 75.8581 19.3833 78.5276 67.5424 45.0106 51.5654 55.6532 81.2252 65.7426 37.7823 42.2587 40.9266 23.2044 39.3001 47.7231 45.8924 45.645 19.5349 55.0725 29.1815 66.087 37.4241 26.9159 53.0507 44.0678 66.2489 73.2018 42.2642 46.0733 36.7505 65.8462 37.3464 9.14286 0 60.5348 67.5325 51.8359 38.7947 41.6228 30.2262 30.5296 37.694 65.2308 35.4204 74.3316 64.2017 43.8221 68.3652 47.5591 30.5882 35.3698 62.0939 39.4338 16.9697 25.6267 31.6883 78.5568 57.1429 50.2415 46.4548 20.5714 57.2062 0 56.33 57.6497 51.0557 44.6018 70.0709 54.6939 77.9043 15.4839 74.5205 63.199 57.4386 29.1525 58.4435 58.8665 27.6392 35.8209 32.5203 52.0875 35.3357 69.9322 61.1825 41.8097 56.0847 40.1384 ]

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:347) Done 2622 files, 1 with no tgt_mats, 0 with other errors. [TRAINING, 0.336907 min, fps37352.7]
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:353) AvgLoss: 0.997224 (Xent), [AvgXent: 0.997224, AvgTargetEnt: 0]
progress: []
FRAME_ACCURACY >> 69.7393% <<

