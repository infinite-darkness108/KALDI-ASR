speech-HP-Z2-Tower-G9
nnet-train-multistream-perutt --cross-validate=true --randomize=false --verbose=0 --num-streams=10 --max-frames=15000 --feature-transform=exp_FG/blstm4i/final.feature_transform 'ark:copy-feats scp:exp_FG/blstm4i/cv.scp ark:- | apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_cv10/utt2spk scp:data-fbank/train_cv10/cmvn.scp ark:- ark:- | add-deltas --delta-order=2 ark:- ark:- |' 'ark:ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- |' exp_FG/blstm4i/nnet/nnet_iter03 
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:243) Not in compute-exclusive mode.  Suggestion: use 'nvidia-smi -c 3' to set compute exclusive mode
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:438) Selecting from 1 GPUs
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:453) cudaSetDevice(0): NVIDIA RTX A2000 12GB	free:11515M, used:515M, total:12031M, free/total:0.957153
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:501) Device: 0, mem_ratio: 0.957153
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:382) Trying to select device: 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:511) Success selecting device 0 free mem ratio: 0.957153
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:FinalizeActiveGpu():cu-device.cc:338) The active GPU is [0]: NVIDIA RTX A2000 12GB	free:11001M, used:1029M, total:12031M, free/total:0.91443 version 8.6
copy-feats scp:exp_FG/blstm4i/cv.scp ark:- 
apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_cv10/utt2spk scp:data-fbank/train_cv10/cmvn.scp ark:- ark:- 
add-deltas --delta-order=2 ark:- ark:- 
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:144) CROSS-VALIDATION STARTED
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Read():nnet/nnet-matrix-buffer.h:191) Read() started... Buffer size in MB: 0, max 3072, having 0 utterances.
ali-to-post ark:- ark:- 
ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl 'ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |' ark:- 
LOG (copy-feats[5.5.1074~1-71f3]:main():copy-feats.cc:143) Copied 296 feature matrices.
LOG (apply-cmvn[5.5.1074~1-71f3]:main():apply-cmvn.cc:159) Applied cepstral mean and variance normalization to 296 utterances, errors on 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:303) ### After 0 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:304) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.408399, max 0.419209, mean 0.00376648, stddev 0.0769677, skewness 0.0150702, kurtosis 0.0456359 ) 
  f_w_gifo_r_   ( min -0.445386, max 0.400828, mean -0.000575929, stddev 0.0766374, skewness 0.000994921, kurtosis -0.00418234 ) 
  f_bias_   ( min -0.351857, max 1.32066, mean 0.215869, stddev 0.456511, skewness 1.07022, kurtosis -0.655579 ) 
  f_peephole_i_c_   ( min -0.389615, max 0.413185, mean -0.00394479, stddev 0.12002, skewness 0.0245806, kurtosis 0.741832 ) 
  f_peephole_f_c_   ( min -0.692214, max 0.710466, mean 0.00228086, stddev 0.157115, skewness 0.0824935, kurtosis 4.18139 ) 
  f_peephole_o_c_   ( min -0.529181, max 0.458228, mean -0.0111839, stddev 0.167957, skewness 0.240416, kurtosis -0.0548913 ) 
  f_w_r_m_   ( min -0.53702, max 0.471381, mean 0.000575746, stddev 0.0976943, skewness -0.00156689, kurtosis -0.0299637 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -0.5708, max 0.581206, mean 0.00620702, stddev 0.0802532, skewness -0.0292889, kurtosis 0.937816 ) 
  b_w_gifo_r_   ( min -0.345889, max 0.304109, mean -0.00024402, stddev 0.0680733, skewness 0.00161323, kurtosis -0.403567 ) 
  b_bias_   ( min -0.298454, max 1.16775, mean 0.210175, stddev 0.448265, skewness 1.06132, kurtosis -0.683011 ) 
  b_peephole_i_c_   ( min -0.369135, max 0.261902, mean 0.00537656, stddev 0.0887678, skewness -0.172002, kurtosis 1.10869 ) 
  b_peephole_f_c_   ( min -0.543131, max 0.596355, mean 0.0111786, stddev 0.143537, skewness 0.474962, kurtosis 3.56313 ) 
  b_peephole_o_c_   ( min -0.538749, max 0.384202, mean -0.0169933, stddev 0.169263, skewness -0.184098, kurtosis 0.157635 ) 
  b_w_r_m_   ( min -0.360523, max 0.35442, mean -0.000146544, stddev 0.0849152, skewness -0.00220094, kurtosis -0.131654 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.855551, max 0.712344, mean -0.000155919, stddev 0.104017, skewness 0.00653466, kurtosis 0.0661545 ) , lr-coef 1, max-norm 0
  bias ( min -0.0701939, max 2.08181, mean 7.45058e-10, stddev 0.0669258, skewness 24.2658, kurtosis 733.809 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:305) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -11.004, max 12.2048, mean 0.00682255, stddev 0.969149, skewness 0.287893, kurtosis 3.83541 ) 
[1] output of <BlstmProjected> ( min -4.2972, max 4.27807, mean -6.32971e-05, stddev 0.736734, skewness 0.0218082, kurtosis 0.952137 ) 
[2] output of <Tanh> ( min -0.99963, max 0.999615, mean -0.000505907, stddev 0.507598, skewness 0.000924803, kurtosis -0.822324 ) 
[3] output of <AffineTransform> ( min -14.9186, max 19.3523, mean 0.0120423, stddev 2.20298, skewness 0.626056, kurtosis 2.01391 ) 
[4] output of <Softmax> ( min 1.82774e-12, max 0.998613, mean 0.000779896, stddev 0.0143717, skewness 39.6096, kurtosis 1872.97 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:334) ### After 79212 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:335) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.408399, max 0.419209, mean 0.00376648, stddev 0.0769677, skewness 0.0150702, kurtosis 0.0456359 ) 
  f_w_gifo_r_   ( min -0.445386, max 0.400828, mean -0.000575929, stddev 0.0766374, skewness 0.000994921, kurtosis -0.00418234 ) 
  f_bias_   ( min -0.351857, max 1.32066, mean 0.215869, stddev 0.456511, skewness 1.07022, kurtosis -0.655579 ) 
  f_peephole_i_c_   ( min -0.389615, max 0.413185, mean -0.00394479, stddev 0.12002, skewness 0.0245806, kurtosis 0.741832 ) 
  f_peephole_f_c_   ( min -0.692214, max 0.710466, mean 0.00228086, stddev 0.157115, skewness 0.0824935, kurtosis 4.18139 ) 
  f_peephole_o_c_   ( min -0.529181, max 0.458228, mean -0.0111839, stddev 0.167957, skewness 0.240416, kurtosis -0.0548913 ) 
  f_w_r_m_   ( min -0.53702, max 0.471381, mean 0.000575746, stddev 0.0976943, skewness -0.00156689, kurtosis -0.0299637 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -0.5708, max 0.581206, mean 0.00620702, stddev 0.0802532, skewness -0.0292889, kurtosis 0.937816 ) 
  b_w_gifo_r_   ( min -0.345889, max 0.304109, mean -0.00024402, stddev 0.0680733, skewness 0.00161323, kurtosis -0.403567 ) 
  b_bias_   ( min -0.298454, max 1.16775, mean 0.210175, stddev 0.448265, skewness 1.06132, kurtosis -0.683011 ) 
  b_peephole_i_c_   ( min -0.369135, max 0.261902, mean 0.00537656, stddev 0.0887678, skewness -0.172002, kurtosis 1.10869 ) 
  b_peephole_f_c_   ( min -0.543131, max 0.596355, mean 0.0111786, stddev 0.143537, skewness 0.474962, kurtosis 3.56313 ) 
  b_peephole_o_c_   ( min -0.538749, max 0.384202, mean -0.0169933, stddev 0.169263, skewness -0.184098, kurtosis 0.157635 ) 
  b_w_r_m_   ( min -0.360523, max 0.35442, mean -0.000146544, stddev 0.0849152, skewness -0.00220094, kurtosis -0.131654 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.855551, max 0.712344, mean -0.000155919, stddev 0.104017, skewness 0.00653466, kurtosis 0.0661545 ) , lr-coef 1, max-norm 0
  bias ( min -0.0701939, max 2.08181, mean 7.45058e-10, stddev 0.0669258, skewness 24.2658, kurtosis 733.809 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:336) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -7.66434, max 7.26584, mean -0.0147177, stddev 0.961893, skewness 0.577064, kurtosis 2.62679 ) 
[1] output of <BlstmProjected> ( min -3.78721, max 4.29782, mean 0.000277644, stddev 0.709312, skewness -0.0121615, kurtosis 1.66062 ) 
[2] output of <Tanh> ( min -0.998974, max 0.99963, mean 0.000929024, stddev 0.481371, skewness -0.0113556, kurtosis -0.576561 ) 
[3] output of <AffineTransform> ( min -14.1297, max 18.2537, mean 0.0144749, stddev 2.13066, skewness 0.794076, kurtosis 3.25984 ) 
[4] output of <Softmax> ( min 3.73877e-12, max 0.990811, mean 0.000780804, stddev 0.0168812, skewness 35.2202, kurtosis 1420.29 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:346) PER-CLASS PERFORMANCE:
@@@ Frames per-class: [ 1670 30 8 79 45 6 24 17 70 220 22 36 34 39 10 53 43 120 22 20 34 66 38 35 87 73 52 35 43 17 86 33 19 32 16 75 106 10 5 1873 21251 22 63 135 22 30 80 98 44 43 9 67 66 27 41 51 107 54 285 40 48 24 308 222 21 35 16 33 248 15 54 49 139 81 14 4 45 35 47 26 17 66 4 14 44 25 18 49 111 11 94 64 72 34 66 27 32 33 78 42 15 34 9 135 37 103 31 64 37 21 67 0 93 69 23 2 34 10 32 37 25 34 18 19 28 43 46 13 4 26 88 25 22 43 33 62 12 98 32 25 22 78 11 62 24 13 88 37 45 30 117 35 60 38 37 129 16 23 64 982 29 24 10 20 31 33 34 16 52 44 64 25 22 25 3 9 30 11 42 35 13 12 11 8 32 32 9 20 32 48 32 42 56 36 35 41 27 16 5 29 35 31 12 7 9 45 27 81 21 16 19 72 29 0 46 70 68 37 26 33 50 37 32 9 5 26 23 15 14 128 52 51 59 20 20 8 73 5 17 32 22 16 4 39 60 11 16 7 19 20 8 7 52 22 37 15 32 12 39 94 72 13 59 17 69 35 25 11 13 18 58 26 24 119 16 14 9 5 13 35 18 40 17 83 10 20 20 32 21 19 33 104 48 60 79 47 20 14 27 49 56 7 20 34 199 37 4 3 21 22 15 27 39 6 10 11 22 25 47 21 6 14 6 60 3 43 26 14 8 10 70 4 15 29 11 133 11 35 22 51 21 57 14 109 15 36 29 5 21 20 18 19 114 55 69 35 21 21 64 44 39 43 15 21 37 31 46 70 32 25 50 48 101 37 32 62 34 53 184 59 33 24 47 16 35 9 79 14 45 12 20 16 36 46 8 52 28 13 12 31 17 29 40 53 13 20 9 73 85 128 108 21 7 44 30 32 12 46 15 32 90 31 20 32 52 21 237 45 11 21 215 49 25 30 31 29 71 13 57 48 98 46 27 41 19 60 21 4 38 48 50 5 63 71 0 0 25 47 11 16 42 44 49 71 27 15 49 22 22 11 30 67 41 56 15 28 24 35 7 14 15 34 7 8 38 0 9 28 57 50 12 13 39 28 40 746 35 13 19 19 18 27 12 16 52 60 21 82 1 33 49 89 6 38 42 18 51 39 26 11 19 7 23 28 18 51 98 35 37 10 34 11 56 273 7 39 31 18 21 15 24 22 15 25 6 21 53 59 24 25 15 4 49 28 14 9 8 3 8 29 34 0 8 18 51 52 48 23 3 60 16 24 35 46 16 52 37 72 3 38 59 15 5 40 14 54 52 32 51 4 14 21 187 40 33 16 15 18 19 52 32 28 37 29 41 52 10 6 95 18 46 53 17 8 5 18 31 15 14 27 7 8 17 7 93 28 14 64 22 15 8 48 40 29 32 15 10 26 38 92 13 54 242 3 84 0 22 55 226 137 6 121 19 57 20 16 55 85 4 17 33 53 42 81 83 26 30 36 55 64 9 66 84 47 14 13 91 43 19 13 23 37 17 19 27 24 0 59 88 26 15 20 111 16 50 7 32 22 5 17 15 28 38 22 27 28 48 52 41 28 16 114 105 37 112 43 4 69 17 87 95 11 45 15 22 24 24 65 3 27 29 73 63 47 17 34 18 53 17 37 17 20 6 14 36 78 20 8 14 23 47 6 6 22 3 11 89 3 5 18 20 146 25 8 25 44 29 28 22 26 34 24 73 34 85 42 37 35 79 40 39 34 32 23 18 104 65 9 45 19 21 17 48 42 21 40 28 25 27 73 23 34 2 186 119 39 90 27 23 23 38 48 0 28 66 71 45 13 24 7 25 19 16 23 24 129 50 34 21 69 32 14 26 16 14 79 10 15 35 83 24 21 61 28 128 42 66 19 12 25 28 26 60 26 36 30 128 60 8 15 24 59 47 2 19 63 42 22 4 29 30 49 65 12 8 20 32 71 48 32 17 59 28 24 21 20 22 26 24 15 35 0 17 35 13 29 14 47 49 7 13 6 17 31 19 38 15 53 143 34 68 37 23 6 34 29 7 46 92 30 48 31 3 30 16 40 24 44 69 43 19 65 170 29 5 28 134 73 20 11 82 86 29 13 21 12 10 24 12 81 163 12 193 6 28 39 45 31 23 36 65 11 39 113 30 11 28 125 45 52 34 87 20 29 16 97 42 97 26 45 39 15 109 53 18 59 52 33 29 52 31 45 25 45 136 11 24 33 61 26 28 29 106 30 33 18 48 56 86 92 23 37 23 14 2 14 36 346 138 60 357 25 129 51 39 29 11 53 15 14 13 44 38 41 41 8 18 32 16 20 21 17 8 96 16 23 156 45 91 21 6 43 48 25 45 29 8 22 23 31 42 3 198 10 14 29 25 39 64 39 11 26 28 39 96 13 53 5 34 9 11 21 67 28 24 50 19 10 11 22 23 15 13 30 30 176 37 84 80 124 21 74 32 81 43 23 25 68 52 83 27 20 0 33 56 14 23 30 22 132 113 9 71 18 15 16 5 38 61 32 70 35 44 36 82 12 19 25 56 12 24 164 178 24 42 24 19 11 41 67 112 11 27 62 39 5 26 82 58 18 39 25 15 60 20 122 54 35 24 51 28 24 13 180 152 39 59 14 42 123 70 441 14 35 36 425 22 33 13 41 32 48 41 161 16 18 110 56 36 262 66 33 47 21 15 29 84 58 48 47 2 22 14 22 35 120 98 20 59 274 17 15 22 15 59 16 0 27 31 40 47 63 69 16 68 32 39 426 46 55 29 28 47 14 42 42 14 17 28 110 23 16 27 12 36 25 38 21 57 45 37 40 25 7 21 46 54 29 44 49 16 46 13 48 16 103 37 35 12 34 ]
@@@ Loss per-class: [ 0.906078 1.80623 2.21251 1.78425 1.31383 4.44586 2.84652 1.36756 2.60263 3.18957 3.25837 3.23961 1.05699 2.11905 3.75568 2.5155 2.01766 2.7968 4.3013 1.00098 1.1603 2.06949 3.7949 1.44948 2.50677 1.97886 1.95061 4.72048 1.84572 1.8518 4.45076 4.40924 3.60981 6.03309 0.368763 0.610539 3.03818 2.17151 5.0242 0.788378 1.19008 3.3385 4.65488 2.24961 1.67261 2.41746 1.76037 6.83809 2.99747 2.54833 3.58871 2.60438 3.49222 3.51212 2.34726 2.41767 2.24255 1.45188 2.67797 1.69359 4.27017 1.94546 1.35082 1.54039 4.45349 1.8166 2.2113 3.29686 1.482 2.54126 1.09654 1.60278 1.67903 2.90069 2.19412 3.72799 2.38929 0.684847 2.31531 1.75049 2.67951 2.723 3.92634 1.59813 2.31603 3.33679 3.92481 4.61377 1.47326 4.70482 3.44617 2.27724 2.40865 4.54241 4.69921 3.72848 1.56839 1.79268 1.63385 0.808171 5.24711 2.11673 4.11397 1.62401 0.995524 4.59257 2.31088 2.74674 2.64986 1.9559 2.9584 0 3.79584 2.0019 2.631 3.51728 2.41019 2.97853 2.15113 2.78682 2.52872 4.481 1.89883 2.74889 1.20424 2.16613 4.57612 3.40928 2.74537 2.50265 1.18441 2.88939 1.11058 2.86428 2.91846 1.31989 2.32489 0.557961 2.50675 2.58533 3.84875 1.67373 1.21716 2.18444 3.62889 1.98119 2.1826 3.4937 2.60622 1.68861 1.29126 2.14636 2.82732 2.84977 1.72278 0.598053 0.907192 2.43142 1.54903 3.28381 3.27818 1.97811 2.93854 6.32103 2.33682 2.5914 2.76432 2.70748 1.05163 3.29641 2.13529 3.93245 2.36067 1.73493 4.5298 0.746327 2.0421 2.33286 3.08116 3.53666 2.85589 2.89698 8.03116 1.54307 1.6704 2.77318 5.19336 2.51813 2.89394 2.95021 1.67806 3.05594 2.2635 2.89127 2.15557 1.91097 1.6739 2.98187 2.67156 2.04322 3.06954 3.17758 2.32216 3.26999 2.97747 1.65073 2.44005 1.61153 2.6578 2.6834 3.09284 2.01641 0.892187 0 2.31026 1.17234 2.66739 1.67597 1.58498 1.95215 1.85704 3.60345 3.55909 1.36235 3.76492 3.16622 4.65854 3.7981 1.26091 3.7442 2.68595 1.75814 2.28192 2.14392 4.96864 3.54433 1.67724 3.66673 2.57626 1.56354 5.47413 1.77986 3.30699 1.33385 4.47244 4.36649 2.3156 6.21155 2.74765 2.88041 5.0378 1.82381 3.67513 1.23009 2.98801 1.19066 1.63802 1.77922 2.22203 1.25797 1.66562 2.17516 2.62574 1.86459 2.62812 0.287419 2.89423 3.62898 0.669438 1.69589 2.43129 0.613109 5.08028 3.44213 2.78087 4.78072 1.13422 5.55745 1.83924 2.63719 1.63608 2.83737 3.12379 4.42135 2.12299 2.45215 1.0906 2.00647 9.65472 1.77738 2.70206 2.72894 1.91181 0.489028 4.12924 3.12366 1.74832 2.89312 6.19035 4.9906 2.27891 2.06365 1.90752 2.9104 0.583411 1.71104 5.24403 2.69282 3.26618 3.82541 2.74706 2.94081 2.55333 2.53307 3.45446 3.20445 1.31065 1.55297 2.04455 6.56185 4.92511 2.34166 3.78275 1.03145 4.175 2.92322 3.68781 4.74287 3.00182 2.99018 2.47954 2.66225 3.21011 2.94351 1.55498 3.17553 4.16787 2.31884 2.30829 2.95932 2.10968 1.424 1.86931 1.8443 2.6601 3.6399 3.18059 3.2721 2.80088 2.57519 2.96501 2.55705 0.901746 1.27983 2.89809 2.08743 2.31428 3.43358 3.09168 2.66574 1.57793 2.88417 2.06167 2.85547 2.13358 4.84418 1.19224 2.46918 2.78288 3.28675 3.8924 2.5668 1.66779 2.97235 0.669329 1.14788 1.57504 1.53841 2.4039 2.92595 2.70572 1.9413 1.05982 1.81576 2.38088 1.77338 2.89453 3.66068 2.38252 4.38678 2.8964 3.01582 3.41651 2.54199 2.94926 1.23609 3.37582 4.67408 2.30833 2.6758 6.21065 2.84704 1.76884 1.06409 2.50964 0.801457 1.95751 3.10868 2.4973 3.38757 4.80469 3.56702 2.07554 3.35244 3.18902 1.56413 1.86045 0.928753 3.64476 2.49416 1.31623 1.69793 0.826551 0.679936 4.0474 2.34145 1.16961 1.95806 5.0661 1.7232 3.94481 2.30267 2.66022 2.89442 2.94672 1.22178 2.79872 2.37118 3.14287 1.94023 0.927132 2.52027 2.73682 2.57523 6.8185 1.25392 5.54959 3.71643 1.86146 2.05764 2.25836 2.58795 2.84646 3.47504 0 0 0.396898 1.81308 2.84569 3.57933 0.874702 1.84412 2.42492 2.52401 4.62268 4.37009 3.40923 0.926603 1.75279 2.71251 1.41439 1.57061 1.32511 1.95128 2.67033 1.61707 0.943402 2.97808 5.16356 2.7296 4.45394 2.07523 3.29696 2.60842 3.89898 0 2.77153 3.13 1.83296 2.79194 3.08345 4.01108 3.79251 7.49697 0.885046 1.98059 2.46942 5.04122 4.63805 1.82169 2.44073 1.86742 2.16444 5.22353 1.38803 2.62501 3.15178 1.21027 5.4758 1.64752 5.97078 3.60316 3.94985 1.96512 2.12802 3.12534 2.73458 3.1609 3.47738 3.9052 4.5158 3.19812 2.85342 4.7373 2.81549 1.18218 2.55311 3.27368 3.9091 2.00238 5.27925 3.77484 3.7763 4.49135 3.54283 1.90279 2.3532 1.63039 2.3783 3.03284 1.76472 5.62499 2.95875 2.0464 6.4339 2.16751 1.42731 3.1692 1.60945 2.32526 2.7065 4.67103 2.17915 2.5709 3.95846 1.19106 2.74457 3.37211 2.0942 2.88835 4.12207 0 3.61493 3.75146 1.85466 1.53944 1.84212 1.6801 2.67906 6.69747 3.47703 2.22704 2.63171 1.17175 4.22663 6.30862 3.89691 1.4046 2.88923 4.53536 3.73868 4.22144 3.25707 4.87173 2.84409 1.34253 5.35871 4.12054 1.65211 2.71695 1.70642 2.9381 1.92452 1.72673 3.53407 2.97136 2.80496 2.33572 3.85885 1.69513 1.92768 4.45072 2.98712 2.76079 1.81619 2.81651 4.01571 3.99712 1.1886 2.25305 2.56528 1.86297 4.94435 0.999911 3.65367 2.96173 5.10312 4.32094 2.58768 4.93391 2.68896 1.8512 2.32587 1.78374 5.26899 2.07589 2.25043 3.41459 1.99493 3.14955 3.24687 5.0478 2.68617 3.79765 1.48457 4.3329 1.54842 1.75151 2.52833 4.37967 3.54247 2.95859 0.873306 1.76677 1.43377 0 2.8258 2.41097 5.93804 1.77673 4.53099 1.93888 3.93236 2.14517 2.85943 4.49193 2.72162 6.13385 5.67452 2.63788 3.34382 2.04093 3.53427 3.1268 1.81441 4.68125 5.21946 1.75766 3.97064 2.18538 7.69164 1.77832 1.89249 2.28024 3.83798 6.44207 4.76989 5.37161 4.3475 4.9503 3.24529 3.84976 2.29544 4.54875 6.71228 3.17133 0 2.41725 2.15233 3.72527 2.53412 3.44342 5.85179 1.28273 5.35851 3.97849 0.812304 5.03877 4.67508 2.29354 1.83851 2.92409 2.33244 3.98037 4.27662 2.70767 3.03775 3.89617 1.10525 3.56521 7.51575 2.14258 0.701541 1.35835 0.871672 1.42057 3.15709 4.01889 3.34232 4.15845 2.46327 7.17047 1.93929 3.27921 3.95293 7.81606 3.1999 4.54323 3.24249 1.3762 3.64904 1.61831 2.34608 5.26686 2.25907 0.930697 4.13219 2.07743 2.12007 2.44388 2.37703 3.3177 3.49128 2.70473 1.95775 2.53705 1.2897 5.43609 1.54465 1.91794 2.77164 2.68716 3.05109 2.20373 4.18687 2.52478 6.78217 5.99967 3.77454 2.388 2.04254 1.07595 3.60597 2.92595 3.1918 4.66555 2.40585 4.81064 2.66654 1.62109 1.31862 1.77122 4.55217 3.47174 1.88862 1.65651 3.37069 4.51615 3.14135 3.49466 2.59459 3.31191 3.36637 3.07319 5.29455 1.50152 2.68266 3.11073 2.3657 1.98613 2.14307 5.41389 3.07714 4.21723 4.6199 2.71673 2.75609 3.00202 2.79097 0.895348 5.11239 4.56508 4.15398 1.29458 1.97752 4.56806 1.94294 2.06426 2.87787 3.36583 2.46777 3.8107 0 3.32171 1.63231 3.08772 2.66793 4.90838 4.14151 1.99822 1.84268 3.09796 1.98366 3.16498 3.92208 2.53706 6.21452 3.54269 2.82236 2.84469 3.01882 3.60424 4.07826 5.14808 3.60934 1.40701 3.80333 4.58837 1.40574 0.65323 3.56519 1.61081 1.27391 1.99242 5.41194 3.69516 5.98589 1.68747 2.74563 1.5888 4.42922 4.85737 4.21713 2.4175 1.38338 1.93478 2.38117 1.69908 5.14123 2.433 3.03352 2.44827 3.82495 2.64593 4.10132 2.03989 2.10917 3.28879 4.36713 3.65752 3.59263 2.15905 1.91924 2.24231 5.51109 5.63713 2.41525 1.64095 2.24975 1.90259 1.65351 1.25578 1.92888 4.12676 4.35579 4.32939 4.42458 2.17543 1.20968 2.99056 1.5869 0 2.68837 1.6717 3.38076 1.29703 4.75092 2.45108 2.72381 2.71549 3.26923 3.00272 2.95162 2.64891 3.79233 2.37097 3.92203 1.48026 1.41943 2.08439 6.62753 2.08228 2.41471 8.22022 2.40628 4.1131 2.6584 2.43226 3.38797 2.30746 3.17698 3.45836 5.546 3.18382 3.99511 2.44898 1.87088 3.75711 3.54724 3.11145 1.94972 3.86949 2.34334 2.9081 2.80181 3.76751 4.13066 3.15622 3.10084 3.16865 4.28404 3.59663 3.05917 2.88796 3.58083 3.11642 3.74813 3.55182 3.02005 2.85767 2.19745 1.98952 5.20804 1.11236 2.29158 1.34662 3.4698 4.46496 1.68372 1.86484 4.65801 4.5757 4.35018 3.93555 4.1527 2.37956 6.03662 3.0218 0.849502 2.38032 4.03227 1.49314 2.62213 5.4264 2.52125 3.99265 2.86174 3.68116 5.1705 1.29888 1.12199 2.4696 3.85659 1.47292 3.83658 2.17573 1.90386 4.14792 2.35198 1.67507 3.52608 1.7176 2.60247 2.55106 4.5331 2.6517 2.47801 3.28431 2.33965 1.49073 2.49931 1.76076 2.02818 2.00897 3.55253 1.19686 1.76072 4.30739 1.27504 4.67216 4.74613 2.15942 2.23849 1.88206 3.09544 3.58629 3.63718 3.16446 4.67182 2.19495 3.00464 1.44182 3.1145 2.39856 2.26892 1.50685 4.50813 2.65553 2.72365 4.38544 5.02389 0.724139 3.28663 4.04804 2.31459 3.21894 4.66977 2.50796 3.35863 5.13696 4.48404 1.85486 2.81512 3.98925 1.62493 3.23835 3.08047 3.59207 2.06639 4.51059 2.34478 6.14381 4.14538 3.7762 2.96072 2.28348 4.40071 1.98895 3.52853 0.864145 2.68253 3.61437 2.56409 2.82441 3.9002 2.29788 3.07873 2.44568 2.98208 1.67943 3.93222 1.32138 3.04106 3.5418 1.62824 3.33552 3.3923 3.28039 2.19976 2.68869 3.78266 4.18111 3.70101 2.90097 4.39006 1.03815 1.07548 5.26474 3.54124 1.98282 3.8067 3.09786 3.08876 2.54936 2.6419 3.75137 4.3151 1.77069 2.33322 3.1672 4.8442 3.10938 3.94968 2.70711 1.57886 2.6964 1.53035 3.78657 2.17565 0.885769 2.19453 4.73722 0 3.38781 3.87965 2.96998 2.09483 2.81269 5.22042 1.88836 3.22612 5.05592 3.42136 3.53016 1.3859 4.88567 1.98158 3.02841 5.43714 2.82597 3.87999 2.32959 3.54287 2.32676 7.47035 1.54919 4.04354 5.86005 5.61591 4.54966 1.35535 1.83376 2.13038 1.12052 2.87554 4.8187 3.23423 3.42364 1.4572 4.10748 1.16109 3.87477 1.98158 3.28048 2.75457 7.7577 2.69145 1.90993 5.01529 5.62705 3.22194 3.21611 2.53962 3.85385 3.4008 1.23863 1.1956 1.17703 3.51633 3.06076 3.15862 1.79897 2.63926 1.50222 2.15794 4.75648 3.6272 4.71658 1.67857 4.69998 1.69111 3.06168 4.89815 1.68513 1.06954 1.0741 3.39066 2.67051 2.17045 2.79394 3.3302 2.60065 4.54197 2.75559 4.36644 2.66868 2.24505 3.28877 2.90826 6.07782 2.19925 1.80264 5.16005 2.54881 2.63991 3.98874 3.15693 3.00169 3.96043 1.81749 3.47409 2.69675 4.25561 1.6777 1.90158 3.57445 4.18935 2.6247 1.01728 3.31038 3.95009 3.4077 3.3984 1.98649 5.36133 4.7482 0 3.5728 1.91919 5.48562 3.16039 3.09433 3.75185 2.34883 4.57166 3.19028 3.26313 2.1082 3.15927 2.30156 3.57381 5.732 3.77311 4.49246 2.20827 1.79933 3.95383 4.13728 4.72463 1.80558 2.39725 5.00114 2.80555 3.97449 2.00842 6.82922 1.85532 2.76721 4.44322 4.29792 1.78747 2.49742 1.35018 6.05908 1.39023 1.09303 1.7996 3.34047 1.75148 2.28655 2.55183 2.62054 3.39338 3.56204 3.88836 2.19637 1.65655 3.38929 3.47176 3.64964 ]
@@@ Frame-accuracy per-class: [ 73.3313 52.459 23.5294 47.7987 74.7253 0 16.3265 51.4286 36.8794 8.61678 13.3333 30.137 72.4638 32.9114 0 26.1682 43.6782 31.5353 8.88889 68.2927 78.2609 40.6015 15.5844 67.6056 30.8571 50.3401 59.0476 0 45.977 51.4286 6.93642 20.8955 0 0 96.9697 94.0397 20.6573 19.0476 0 80.9714 49.8271 17.7778 25.1969 50.9225 57.7778 42.623 36.0248 0 6.74157 25.2874 10.5263 22.2222 22.5564 3.63636 26.506 40.7767 50.2326 67.8899 19.2644 64.1975 6.18557 24.4898 58.671 64.2697 0 39.4366 30.303 20.8955 50.7042 32.2581 64.2202 48.4848 63.0824 31.9018 48.2759 0 39.5604 84.507 37.8947 79.2453 34.2857 28.5714 22.2222 68.9655 20.2247 3.92157 5.40541 10.101 63.6771 0 8.46561 13.9535 20.6897 5.7971 39.0977 0 61.5385 47.7612 54.7771 89.4118 6.45161 55.0725 0 58.3026 77.3333 11.5942 63.4921 21.7054 26.6667 60.4651 17.7778 0 13.9037 64.7482 8.51064 0 40.5797 0 33.8462 34.6667 39.2157 5.7971 48.6486 15.3846 56.1404 43.6782 0 7.40741 0 37.7358 75.7062 15.6863 66.6667 22.9885 20.8955 70.4 64 84.264 33.8462 43.1373 0 57.3248 69.5652 38.4 0 0 57.6271 0 21.978 59.0164 66.383 33.8028 21.4876 20.7792 64 89.5753 66.6667 34.0426 62.0155 22.799 20.339 53.0612 9.52381 0 44.4444 35.8209 23.1884 30.303 70.4762 6.74157 13.9535 3.92157 66.6667 47.0588 0 73.6842 62.2951 52.1739 14.1176 25.3521 14.8148 32 0 82.3529 49.2308 21.5385 0 24.3902 0 12.3711 55.3846 21.1765 44.2478 35.6164 50.7042 43.3735 61.8182 12.1212 18.1818 13.5593 16.9014 9.52381 48 0 10.5263 57.1429 40 72.3926 41.8605 0 30.7692 45.5172 77.9661 0 47.3118 62.4113 18.9781 66.6667 52.8302 59.7015 51.4851 21.3333 15.3846 63.1579 0 7.54717 0 0 75.8621 13.2296 24.7619 56.3107 35.2941 58.5366 4.87805 0 48.9796 0 11.4286 61.5385 0 24.2424 0 63.2911 11.5702 0 30.303 0 10.2564 24.3902 0 66.6667 5.71429 62.2222 34.6667 83.871 64.6154 40 25.3165 76.1905 68.9655 37.037 33.6134 57.1429 24.4604 95.7746 19.6078 0 88.8889 54.0541 34.188 79.2453 12.2449 20.9205 24.2424 0 84.2105 0 59.2593 14.0845 59.4595 14.8148 5.71429 5.98802 47.619 34.1463 73.1707 46.1538 0 41.0256 5.97015 29.6651 45.3608 84.2975 1.25786 29.4737 48.7805 0 0 8.08081 38.9381 40 48.7805 23.1884 83.208 77.3333 0 0 37.2093 4.44444 12.9032 10.9091 32.9114 0 0 17.3913 75.5556 47.0588 52.6316 0 0 41.3793 0 76.0331 0 18.3908 18.8679 0 23.5294 9.52381 42.5532 0 19.3548 16.9492 52.1739 12.7341 0 42.2535 31.1111 19.4175 27.907 59.1304 89.6552 51.1416 12.9032 8.21918 10.1695 0 37.2093 29.2683 32.4324 30.7692 80.3493 72.0721 41.7266 47.8873 27.907 4.65116 31.0078 35.9551 45.5696 16.092 64.5161 23.2558 58.6667 0 68.8172 46.8085 33.8462 23.5294 11.8812 37.1134 62.069 21.3333 86.1538 72 72.4638 50.4673 35.7724 33.6134 38.806 36.7347 77.8947 42.4242 28.169 52.6316 51.5723 0 43.956 16 9.7561 18.1818 21.9178 19.3548 0 66.6667 14.0351 0 48 19.0476 0 30.5085 61.7284 82.243 7.40741 78.0488 31.5789 31.2925 32.7485 14.786 5.52995 13.9535 0 13.4831 22.9508 64.6154 48 70.9677 0 46.1538 70.7182 41.2698 63.4146 92.3077 17.1429 65.1163 76.2105 41.7582 0 60.4651 8.35267 42.4242 23.5294 42.623 22.2222 57.6271 16.7832 59.2593 20.8696 51.5464 72.0812 45.1613 36.3636 45.7831 0 69.4215 0 0 64.9351 39.1753 53.4653 0 34.6457 22.3776 0 0 98.0392 50.5263 0 0 84.7059 51.6854 30.303 11.1888 0 0 6.06061 80 48.8889 26.087 75.4098 53.3333 65.0602 58.4071 19.3548 45.614 85.7143 39.4366 0 20.6897 0 43.4783 40 23.5294 15.5844 0 21.0526 17.5439 59.1304 1.9802 24 7.40741 0 3.50877 79.0123 43.9384 30.9859 0 0 20.5128 27.027 43.6364 32 6.06061 78.0952 11.5702 13.9535 65.4545 0 68.6567 0 7.82123 15.3846 44.1558 58.8235 32.4324 23.301 25.3165 3.77358 0 0 0 21.2766 0 27.027 54.3689 28.4264 22.5352 5.33333 76.1905 8.69565 0 8.84956 12.4314 0 60.7595 28.5714 54.0541 32.5581 12.9032 44.898 13.3333 32.2581 31.3725 0 46.5116 67.2897 15.1261 73.4694 15.6863 19.3548 0 62.6263 10.5263 0 63.1579 23.5294 0 35.2941 33.8983 0 0 0 10.8108 60.1942 62.8571 51.5464 55.3191 0 4.95868 0 36.7347 2.8169 73.1183 0 3.80952 18.6667 60.6897 0 10.3896 13.4454 0 18.1818 0 34.4828 53.211 20.9524 9.23077 42.7184 44.4444 48.2759 0 60.2667 49.3827 23.8806 12.1212 51.6129 37.8378 0 51.4286 43.0769 3.50877 18.6667 30.5085 50.6024 19.0476 0 0 63.8743 21.6216 38.7097 59.8131 0 94.1176 18.1818 5.40541 3.1746 0 13.7931 3.63636 0 23.5294 51.4286 26.6667 21.3904 38.5965 34.4828 17.0543 53.3333 12.9032 58.8235 12.3711 39.5062 13.5593 43.0769 0 57.1429 26.4151 31.1688 11.8919 14.8148 22.0183 78.7629 28.5714 53.2544 0 40 46.8468 28.2561 50.9091 0 37.8601 5.12821 41.7391 19.5122 0 19.8198 2.33918 0 22.8571 0 31.7757 21.1765 41.7178 46.7066 0 0 57.5342 9.00901 17.0543 0 60.1504 53.2544 31.5789 0 0 6.55738 0 5.12821 0 25.5319 2.66667 40 5.12821 0 16.3265 0 40.3361 48.5876 11.3208 25.8065 39.0244 0 66.6667 0 0 89.2308 0 0 34.2857 58.0645 35.0877 46.7532 8.88889 0 31.5789 30.9278 5.71429 72.2892 10.5263 0 27.9476 80.5687 64 86.2222 64.3678 22.2222 2.8777 0 8 34.555 0 54.9451 0 0 0 24.4898 21.374 28.5714 50.9091 16.9492 54.4218 50.3937 10.5263 22.8571 81.1594 5.40541 50.4673 51.4286 34.6667 45.7143 9.7561 0 6.89655 35.6164 40.7643 68.2927 0 68.9655 51.0638 50.5263 0 15.3846 62.2222 0 8.69565 5.58659 0 0 5.40541 48.7805 84.6416 7.84314 0 11.7647 0 40.678 17.5439 62.2222 37.7358 66.6667 48.9796 8.16327 2.89855 33.9181 51.7647 24 0 16.3522 0 35.443 31.8841 12.3077 25.5319 0 62.201 18.3206 21.0526 30.7692 46.1538 27.907 0 4.12371 7.05882 0 49.3827 28.0702 23.5294 40 89.7959 8.51064 0 0 67.0241 42.6778 0 44.1989 58.1818 38.2979 17.0213 28.5714 0 0 7.01754 58.6466 16.7832 35.1648 0 12.2449 13.3333 50.9804 5.12821 36.3636 34.0426 8.16327 31.6602 5.94059 34.7826 0 23.0216 9.23077 0 3.77358 6.06061 0 66.6667 9.52381 0 45.0704 89.8204 4.08163 51.1628 66.6667 38.5965 9.33852 9.41176 16.5414 56.4103 0 50.9804 0 15.0943 6.61157 41.5094 49.3151 49.1803 32.6848 44.6281 0 25.8065 32.6531 21.8487 4.21053 40 5.12821 53.5433 56.4706 4.44444 0 0 9.83607 32.3232 42.7481 24 0 19.5122 36.9231 58.7413 39.1753 46.1538 51.4286 72.2689 42.1053 0 0 29.2683 0 41.5094 73.4694 12.9032 64.7887 0 51.4286 56.338 7.40741 67.7966 0 40 22.2222 0 7.40741 0 22.8571 19.0476 20.5128 33.7662 0 69.1589 67.5958 52.1739 0 34.6667 8.51064 0 26.087 20.339 0 4.30108 14.0541 42.623 10.3093 28.5714 0 22.9508 0 39.5062 48.9796 22.4719 20.1439 25.2874 61.5385 9.16031 42.2287 13.5593 0 35.0877 6.69145 10.8844 19.5122 34.7826 3.63636 17.341 27.1186 29.6296 18.6047 40 0 8.16327 0 18.4049 43.4251 24 0.516796 92.3077 42.1053 63.2911 50.5495 9.52381 68.0851 54.7945 7.63359 0 10.1266 5.28634 3.27869 8.69565 0 0 74.7253 30.4762 2.89855 61.7143 9.7561 0 12.1212 15.3846 30.5882 10.2564 0 57.1429 75.9494 58.0645 21.0046 57.9439 10.8108 38.6555 49.5238 8.95522 37.2881 57.1429 9.52381 61.5385 39.2157 26.3736 3.663 43.4783 48.9796 23.8806 42.2764 60.3774 24.5614 40.678 46.0094 36.0656 11.9403 64.8649 55.6701 8.84956 77.4566 3.24324 0 53.3333 51.0638 34.4828 0 13.7931 10.9589 21.3564 3.61011 47.9339 22.0979 70.5882 20.8494 38.835 37.9747 54.2373 0 44.8598 12.9032 13.7931 0 89.8876 7.79221 21.6867 50.6024 11.7647 0 18.4615 6.06061 19.5122 0 45.7143 11.7647 17.6166 36.3636 34.0426 47.2843 10.989 41.5301 0 46.1538 0 4.12371 3.92157 21.978 54.2373 0 44.4444 0 88.8889 23.5294 0 31.738 28.5714 6.89655 33.8983 15.6863 32.9114 32.5581 55.6962 8.69565 79.2453 28.0702 27.8481 52.8497 14.8148 13.0841 0 23.1884 10.5263 8.69565 9.30233 2.96296 14.0351 4.08163 79.2079 82.0513 0 8.69565 31.1111 0 6.45161 7.40741 42.623 9.83607 30.5949 10.6667 63.9053 43.4783 9.63855 0 17.4497 9.23077 45.3988 73.5632 25.5319 47.0588 2.91971 57.1429 92.2156 47.2727 14.6341 0 23.8806 1.76991 48.2759 59.5745 9.83607 0 45.283 33.4802 0 34.965 0 64.5161 0 18.1818 23.3766 1.62602 18.4615 7.0922 33.8028 24.7191 35.6164 2.42424 40 0 0 14.1593 0 77.551 53.4954 39.2157 69.3878 21.1765 0 30.7692 0 55.4217 14.8148 77.3333 0 47.2727 8 27.8481 0 22.6415 49.697 1.7094 0 30.3797 31.3725 38.7097 18.1818 19.5122 69.3878 71.5596 53.5211 44.898 7.76699 28.0702 57.1429 14.8148 69.8061 51.1475 0 6.72269 0 51.7647 12.1457 63.8298 35.3341 6.89655 45.0704 87.6712 66.51 8.88889 50.7463 59.2593 28.9157 0 20.6186 16.8675 19.195 0 32.4324 48.8688 8.84956 30.137 11.0476 27.0677 65.6716 0 32.5581 12.9032 0 20.1183 20.5128 0 42.1053 0 35.5556 6.89655 48.8889 42.2535 16.5975 10.1523 34.1463 75.6303 13.4791 0 12.9032 13.3333 58.0645 0 0 0 3.63636 53.9683 0 25.2632 23.622 1.43885 36.3636 0 0 17.7215 49.0035 36.5591 41.4414 16.9492 3.50877 21.0526 0 56.4706 44.7059 0 11.4286 0 45.2489 29.7872 18.1818 10.9091 0 35.6164 0 64.9351 9.30233 3.47826 4.3956 37.3333 39.5062 62.7451 0 88.3721 60.2151 40.367 0 51.6854 40.404 6.06061 19.3548 22.2222 0 12.1212 47.343 48 11.2676 24 8.69565 ]

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:347) Done 295 files, 0 with no tgt_mats, 0 with other errors. [CROSS-VALIDATION, 0.020914 min, fps63125.1]
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:353) AvgLoss: 2.29138 (Xent), [AvgXent: 2.29138, AvgTargetEnt: 0]
progress: []
FRAME_ACCURACY >> 40.5343% <<

WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Close():kaldi-io.cc:515) Pipe ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- | had nonzero return status 36096
