speech-HP-Z2-Tower-G9
nnet-train-multistream-perutt --cross-validate=false --randomize=true --verbose=0 --num-streams=10 --max-frames=15000 --learn-rate=0.00004 --momentum=0.9 --l1-penalty=0 --l2-penalty=0 --feature-transform=exp_FG/blstm4i/final.feature_transform 'ark:copy-feats scp:exp_FG/blstm4i/train.scp ark:- | apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_tr90/utt2spk scp:data-fbank/train_tr90/cmvn.scp ark:- ark:- | add-deltas --delta-order=2 ark:- ark:- |' 'ark:ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- |' exp_FG/blstm4i/nnet/nnet_iter03_learnrate0.00004_tr1.5242_cv2.2914 exp_FG/blstm4i/nnet/nnet_iter04 
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:243) Not in compute-exclusive mode.  Suggestion: use 'nvidia-smi -c 3' to set compute exclusive mode
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:438) Selecting from 1 GPUs
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:453) cudaSetDevice(0): NVIDIA RTX A2000 12GB	free:11515M, used:515M, total:12031M, free/total:0.957153
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:501) Device: 0, mem_ratio: 0.957153
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:382) Trying to select device: 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:511) Success selecting device 0 free mem ratio: 0.957153
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:FinalizeActiveGpu():cu-device.cc:338) The active GPU is [0]: NVIDIA RTX A2000 12GB	free:11001M, used:1029M, total:12031M, free/total:0.91443 version 8.6
copy-feats scp:exp_FG/blstm4i/train.scp ark:- 
apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_tr90/utt2spk scp:data-fbank/train_tr90/cmvn.scp ark:- ark:- 
add-deltas --delta-order=2 ark:- ark:- 
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:144) TRAINING STARTED
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Read():nnet/nnet-matrix-buffer.h:191) Read() started... Buffer size in MB: 0, max 3072, having 0 utterances.
ali-to-post ark:- ark:- 
ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl 'ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |' ark:- 
LOG (copy-feats[5.5.1074~1-71f3]:main():copy-feats.cc:143) Copied 2624 feature matrices.
LOG (apply-cmvn[5.5.1074~1-71f3]:main():apply-cmvn.cc:159) Applied cepstral mean and variance normalization to 2624 utterances, errors on 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:303) ### After 0 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:304) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.408182, max 0.41942, mean 0.00376615, stddev 0.0769694, skewness 0.0149647, kurtosis 0.0459712 ) 
  f_w_gifo_r_   ( min -0.445026, max 0.400832, mean -0.000576039, stddev 0.0766373, skewness 0.00098948, kurtosis -0.00424433 ) 
  f_bias_   ( min -0.35185, max 1.3205, mean 0.215857, stddev 0.456506, skewness 1.07026, kurtosis -0.655537 ) 
  f_peephole_i_c_   ( min -0.389187, max 0.413419, mean -0.00392862, stddev 0.120011, skewness 0.0258441, kurtosis 0.741419 ) 
  f_peephole_f_c_   ( min -0.692215, max 0.710522, mean 0.00229439, stddev 0.157124, skewness 0.081451, kurtosis 4.18204 ) 
  f_peephole_o_c_   ( min -0.529251, max 0.458413, mean -0.0112341, stddev 0.168023, skewness 0.240152, kurtosis -0.0563197 ) 
  f_w_r_m_   ( min -0.537146, max 0.471269, mean 0.000575526, stddev 0.097694, skewness -0.00159968, kurtosis -0.0300684 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -0.571213, max 0.581324, mean 0.00621112, stddev 0.080257, skewness -0.0291152, kurtosis 0.937598 ) 
  b_w_gifo_r_   ( min -0.345452, max 0.304, mean -0.000243909, stddev 0.0680733, skewness 0.00161512, kurtosis -0.403641 ) 
  b_bias_   ( min -0.298527, max 1.16774, mean 0.210165, stddev 0.448263, skewness 1.06139, kurtosis -0.682923 ) 
  b_peephole_i_c_   ( min -0.369142, max 0.261911, mean 0.00537934, stddev 0.0887458, skewness -0.172334, kurtosis 1.11171 ) 
  b_peephole_f_c_   ( min -0.543768, max 0.596272, mean 0.0111932, stddev 0.143547, skewness 0.475139, kurtosis 3.56442 ) 
  b_peephole_o_c_   ( min -0.538809, max 0.384107, mean -0.0170235, stddev 0.169322, skewness -0.184632, kurtosis 0.158221 ) 
  b_w_r_m_   ( min -0.360516, max 0.354565, mean -0.00014578, stddev 0.0849144, skewness -0.00221375, kurtosis -0.131654 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.850958, max 0.707809, mean -0.000155921, stddev 0.104017, skewness 0.0065325, kurtosis 0.0658057 ) , lr-coef 1, max-norm 0
  bias ( min -0.0702063, max 2.09002, mean -2.79397e-10, stddev 0.0671252, skewness 24.3341, kurtosis 736.678 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:305) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -8.67671, max 9.02116, mean 0.0321104, stddev 1.01793, skewness 0.450612, kurtosis 3.20408 ) 
[1] output of <BlstmProjected> ( min -4.11082, max 4.76273, mean 0.00148081, stddev 0.767407, skewness 0.00653963, kurtosis 1.26883 ) 
[2] output of <Tanh> ( min -0.999463, max 0.999854, mean 0.00108048, stddev 0.510852, skewness -0.00424263, kurtosis -0.777606 ) 
[3] output of <AffineTransform> ( min -16.0959, max 19.7037, mean 0.00648699, stddev 2.22015, skewness 0.753543, kurtosis 2.8914 ) 
[4] output of <Softmax> ( min 1.67736e-13, max 0.9995, mean 0.000779654, stddev 0.0175474, skewness 36.9249, kurtosis 1541.28 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:307) 
### BACKWARD PROPAGATION BUFFER CONTENT :
[0] diff of <Input>  ( min -2.32092, max 3.6208, mean 0.00162734, stddev 0.214607, skewness 1.71711, kurtosis 18.2289 ) 
[1] diff-output of <BlstmProjected> ( min -0.593189, max 0.654195, mean 7.21547e-05, stddev 0.0551465, skewness -0.0250043, kurtosis 4.8795 ) 
[2] diff-output of <Tanh> ( min -0.719724, max 0.85065, mean 3.74e-05, stddev 0.0726759, skewness -0.00912884, kurtosis 2.63911 ) 
[3] diff-output of <AffineTransform> ( min -0.999999, max 0.937915, mean -4.2087e-07, stddev 0.0216556, skewness -23.246, kurtosis 1307.66 ) 
[4] diff-output of <Softmax> ( min -0.999999, max 0.937915, mean -4.2087e-07, stddev 0.0216556, skewness -23.246, kurtosis 1307.66 ) 
### END BACKWARD


LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:308) 
### GRADIENT STATS :
Component 1 : <BlstmProjected>, ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  ### Gradients 
  f_w_gifo_x_corr_   ( min -29.5336, max 30.6608, mean 0.00834896, stddev 2.87705, skewness 0.143339, kurtosis 9.53012 ) 
  f_w_gifo_r_corr_   ( min -46.2512, max 76.2973, mean 0.00279297, stddev 2.17321, skewness 0.341469, kurtosis 21.6292 ) 
  f_bias_corr_   ( min -14.0338, max 49.3991, mean 0.309213, stddev 3.74401, skewness 2.4781, kurtosis 26.8402 ) 
  f_peephole_i_c_corr_   ( min -27.288, max 28.6993, mean -0.404186, stddev 3.96343, skewness -0.599095, kurtosis 17.9746 ) 
  f_peephole_f_c_corr_   ( min -63.3392, max 83.853, mean -0.3382, stddev 10.6975, skewness 0.620658, kurtosis 17.479 ) 
  f_peephole_o_c_corr_   ( min -64.0661, max 165.451, mean 1.25543, stddev 14.7774, skewness 4.66635, kurtosis 52.2345 ) 
  f_w_r_m_corr_   ( min -40.0642, max 39.5707, mean 0.00547461, stddev 3.85962, skewness -0.0251194, kurtosis 4.59065 ) 
  ---
  b_w_gifo_x_corr_   ( min -77.54, max 78.845, mean -0.102165, stddev 3.56558, skewness -0.219213, kurtosis 23.6014 ) 
  b_w_gifo_r_corr_   ( min -30.4421, max 35.1018, mean -0.00264687, stddev 2.42594, skewness 0.0266148, kurtosis 7.85828 ) 
  b_bias_corr_   ( min -46.499, max 33.2682, mean 0.252233, stddev 5.37124, skewness -0.0779472, kurtosis 12.9676 ) 
  b_peephole_i_c_corr_   ( min -69.7919, max 37.9979, mean -0.0694462, stddev 6.27423, skewness -3.71128, kurtosis 54.9244 ) 
  b_peephole_f_c_corr_   ( min -87.2719, max 75.8888, mean -0.366812, stddev 11.6637, skewness -1.49933, kurtosis 19.4547 ) 
  b_peephole_o_c_corr_   ( min -89.9053, max 83.2755, mean 0.754123, stddev 14.1511, skewness 0.617121, kurtosis 13.8514 ) 
  b_w_r_m_corr_   ( min -27.932, max 25.2901, mean -0.0190831, stddev 3.97805, skewness -0.00104123, kurtosis 1.13531 ) 

  ### Activations (mostly after non-linearities)
  YI_FW(0..1)^   ( min 0, max 1, mean 0.440097, stddev 0.341782, skewness 0.326728, kurtosis -1.30432 ) 
  YF_FW(0..1)^   ( min 0, max 1, mean 0.635671, stddev 0.317818, skewness -0.50944, kurtosis -1.02181 ) 
  YO_FW(0..1)^   ( min 0, max 1, mean 0.372758, stddev 0.350519, skewness 0.611663, kurtosis -1.14406 ) 
  YG_FW(-1..1)   ( min -1, max 1, mean 0.0232169, stddev 0.871206, skewness -0.0448289, kurtosis -1.80689 ) 
  YC_FW(-R..R)*  ( min -50, max 50, mean 0.38156, stddev 13.1781, skewness 0.151444, kurtosis 9.51363 ) 
  YH_FW(-1..1)   ( min -1, max 1, mean 0.0341441, stddev 0.682697, skewness -0.0593069, kurtosis -1.23999 ) 
  YM_FW(-1..1)   ( min -1, max 1, mean 0.002231, stddev 0.333414, skewness -0.0695382, kurtosis 2.57403 ) 
  YR_FW(-R..R)   ( min -3.81357, max 3.9115, mean 0.0217543, stddev 0.743836, skewness 0.0706224, kurtosis 1.09796 ) 
  ---
  YI_BW(0..1)^   ( min 0, max 1, mean 0.465731, stddev 0.323675, skewness 0.23269, kurtosis -1.30989 ) 
  YF_BW(0..1)^   ( min 0, max 1, mean 0.652056, stddev 0.270722, skewness -0.545661, kurtosis -0.609756 ) 
  YO_BW(0..1)^   ( min 0, max 1, mean 0.39048, stddev 0.350609, skewness 0.520343, kurtosis -1.24212 ) 
  YG_BW(-1..1)   ( min -1, max 1, mean 0.00738999, stddev 0.848316, skewness -0.0116692, kurtosis -1.76906 ) 
  YC_BW(-R..R)*  ( min -50, max 50, mean 1.09597, stddev 11.4995, skewness 0.978203, kurtosis 11.6694 ) 
  YH_BW(-1..1)   ( min -1, max 1, mean 0.0388773, stddev 0.697524, skewness -0.0516647, kurtosis -1.31837 ) 
  YM_BW(-1..1)   ( min -0.999987, max 1, mean 0.00481913, stddev 0.344224, skewness 0.0185853, kurtosis 2.0223 ) 
  YR_BW(-R..R)   ( min -4.11082, max 4.76273, mean -0.0188079, stddev 0.786191, skewness -0.0395395, kurtosis 1.41169 ) 

  ### Derivatives (w.r.t. inputs of non-linearities)
  DI_FW^  ( min -1, max 1, mean 4.21427e-05, stddev 0.0247793, skewness 0.225524, kurtosis 255.583 ) 
  DF_FW^  ( min -1, max 1, mean 0.000107006, stddev 0.0191978, skewness 2.95321, kurtosis 470.472 ) 
  DO_FW^  ( min -1, max 1, mean 9.8305e-05, stddev 0.0313552, skewness 0.00854596, kurtosis 150.567 ) 
  DG_FW   ( min -1, max 1, mean 6.80688e-05, stddev 0.0328652, skewness 0.682141, kurtosis 332.385 ) 
  DC_FW*  ( min -14.5906, max 12.0065, mean 4.5666e-05, stddev 0.24748, skewness -2.09995, kurtosis 239.338 ) 
  DH_FW   ( min -9.84834, max 9.41107, mean -8.31941e-06, stddev 0.146437, skewness -0.357205, kurtosis 320.286 ) 
  DM_FW   ( min -14.4555, max 14.062, mean 0.000492043, stddev 0.403347, skewness 0.167315, kurtosis 90.9301 ) 
  DR_FW   ( min -3.18144, max 3.02557, mean 0.000203501, stddev 0.0988396, skewness 0.141767, kurtosis 28.6314 ) 
  ---
  DI_BW^  ( min -1, max 1, mean 3.58143e-05, stddev 0.0208701, skewness 0.676524, kurtosis 317.783 ) 
  DF_BW^  ( min -1, max 1, mean 4.02814e-05, stddev 0.0159703, skewness 4.63047, kurtosis 441.772 ) 
  DO_BW^  ( min -0.700602, max 0.825435, mean 0.000178343, stddev 0.020876, skewness 0.742639, kurtosis 60.7137 ) 
  DG_BW   ( min -1, max 1, mean 2.94056e-06, stddev 0.0335217, skewness -0.195269, kurtosis 210.051 ) 
  DC_BW*  ( min -13.6617, max 22.3153, mean -3.88733e-05, stddev 0.268344, skewness 11.1353, kurtosis 1378.35 ) 
  DH_BW   ( min -2.20902, max 2.58597, mean 0.000207991, stddev 0.0916137, skewness 0.287658, kurtosis 39.1938 ) 
  DM_BW   ( min -4.00886, max 3.8196, mean 0.00175432, stddev 0.254907, skewness 0.0940948, kurtosis 12.3055 ) 
  DR_BW   ( min -1.14769, max 1.1938, mean -9.46954e-05, stddev 0.0880681, skewness -0.0266309, kurtosis 6.3203 ) 
Component 2 : <Tanh>, 
Component 3 : <AffineTransform>, 
  linearity_grad ( min -114.818, max 129.821, mean -3.29293e-08, stddev 2.07959, skewness 1.21263, kurtosis 426.746 ) , lr-coef 1, max-norm 0
  bias_grad ( min -205.256, max 302.034, mean 1.66893e-07, stddev 11.0283, skewness 10.2097, kurtosis 537.721 ) , lr-coef 1
Component 4 : <Softmax>, 
### END GRADIENT

LOG (ali-to-pdf[5.5.1074~1-71f3]:main():ali-to-pdf.cc:68) Converted 2919 alignments to pdf sequences.
LOG (ali-to-post[5.5.1074~1-71f3]:main():ali-to-post.cc:73) Converted 2919 alignments.
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:168) MC05_98, missing targets
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:334) ### After 755062 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:335) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.462523, max 0.503519, mean 0.00397666, stddev 0.0801092, skewness 0.0101798, kurtosis 0.256305 ) 
  f_w_gifo_r_   ( min -0.422243, max 0.399551, mean -0.000603555, stddev 0.0771586, skewness -0.00046548, kurtosis 0.000619411 ) 
  f_bias_   ( min -0.351716, max 1.31913, mean 0.213927, stddev 0.45893, skewness 1.06779, kurtosis -0.655864 ) 
  f_peephole_i_c_   ( min -0.465029, max 0.442255, mean -0.00218984, stddev 0.124593, skewness 0.124501, kurtosis 1.11836 ) 
  f_peephole_f_c_   ( min -0.705292, max 0.785748, mean 0.00318549, stddev 0.164768, skewness 0.259382, kurtosis 4.4216 ) 
  f_peephole_o_c_   ( min -0.515255, max 0.415366, mean -0.00973649, stddev 0.175047, skewness 0.218968, kurtosis -0.210873 ) 
  f_w_r_m_   ( min -0.558468, max 0.468663, mean 0.000596786, stddev 0.0996326, skewness 0.000344229, kurtosis -0.0201168 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -0.860912, max 0.762125, mean 0.00627521, stddev 0.0848214, skewness -0.0544364, kurtosis 1.65198 ) 
  b_w_gifo_r_   ( min -0.367031, max 0.293487, mean -0.00021847, stddev 0.0692153, skewness 0.00106212, kurtosis -0.351764 ) 
  b_bias_   ( min -0.318202, max 1.18163, mean 0.20853, stddev 0.449851, skewness 1.05414, kurtosis -0.686093 ) 
  b_peephole_i_c_   ( min -0.355501, max 0.27163, mean 0.00533876, stddev 0.0905548, skewness -0.102579, kurtosis 0.886673 ) 
  b_peephole_f_c_   ( min -0.600643, max 0.661436, mean 0.0116935, stddev 0.155359, skewness 0.547026, kurtosis 3.77405 ) 
  b_peephole_o_c_   ( min -0.541353, max 0.465101, mean -0.0167221, stddev 0.180103, skewness -0.11891, kurtosis 0.231351 ) 
  b_w_r_m_   ( min -0.383361, max 0.359649, mean -0.000205052, stddev 0.0875846, skewness 0.000784828, kurtosis -0.102961 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.914352, max 0.724912, mean -0.000155918, stddev 0.105338, skewness 0.00617894, kurtosis 0.0667777 ) , lr-coef 1, max-norm 0
  bias ( min -0.0775918, max 2.21985, mean 5.12227e-10, stddev 0.0723276, skewness 23.5702, kurtosis 697.995 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:336) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -14.8312, max 13.8231, mean 0.00769689, stddev 0.940803, skewness 0.731578, kurtosis 12.6034 ) 
[1] output of <BlstmProjected> ( min -3.96881, max 3.98172, mean 0.00156474, stddev 0.644646, skewness -0.045876, kurtosis 3.1217 ) 
[2] output of <Tanh> ( min -0.999286, max 0.999304, mean 0.00214031, stddev 0.428574, skewness -0.0102471, kurtosis 0.147916 ) 
[3] output of <AffineTransform> ( min -11.5126, max 20.7962, mean 0.00733264, stddev 1.92433, skewness 0.989372, kurtosis 5.75245 ) 
[4] output of <Softmax> ( min 2.98378e-12, max 0.99948, mean 0.000781018, stddev 0.0166294, skewness 42.9132, kurtosis 2029.09 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:338) 
### BACKWARD PROPAGATION BUFFER CONTENT :
[0] diff of <Input>  ( min -2.28076, max 2.41458, mean 0.00159603, stddev 0.193159, skewness -0.110065, kurtosis 16.7066 ) 
[1] diff-output of <BlstmProjected> ( min -0.482021, max 0.432703, mean -6.25152e-05, stddev 0.0428262, skewness 0.0181567, kurtosis 9.13997 ) 
[2] diff-output of <Tanh> ( min -0.504477, max 0.500797, mean -2.50689e-06, stddev 0.0582449, skewness 0.0309717, kurtosis 5.75079 ) 
[3] diff-output of <AffineTransform> ( min -0.999411, max 0.85263, mean -1.30477e-08, stddev 0.016399, skewness -31.4426, kurtosis 2227.2 ) 
[4] diff-output of <Softmax> ( min -0.999411, max 0.85263, mean -1.30477e-08, stddev 0.016399, skewness -31.4426, kurtosis 2227.2 ) 
### END BACKWARD


LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:339) 
### GRADIENT STATS :
Component 1 : <BlstmProjected>, ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  ### Gradients 
  f_w_gifo_x_corr_   ( min -33.4646, max 42.86, mean 0.132621, stddev 3.66095, skewness 0.24456, kurtosis 7.49634 ) 
  f_w_gifo_r_corr_   ( min -56.5247, max 57.476, mean 0.0023426, stddev 2.93658, skewness -0.0615323, kurtosis 10.1893 ) 
  f_bias_corr_   ( min -26.0016, max 19.1079, mean -0.193096, stddev 4.43985, skewness -0.0688091, kurtosis 3.00643 ) 
  f_peephole_i_c_corr_   ( min -35.0016, max 147.369, mean 0.617879, stddev 9.86048, skewness 10.39, kurtosis 152.526 ) 
  f_peephole_f_c_corr_   ( min -73.5609, max 103.899, mean 1.84439, stddev 15.1092, skewness 1.96461, kurtosis 14.9772 ) 
  f_peephole_o_c_corr_   ( min -198.038, max 125.562, mean -2.75255, stddev 25.2234, skewness -2.67062, kurtosis 22.413 ) 
  f_w_r_m_corr_   ( min -49.232, max 42.6921, mean -0.00860335, stddev 4.70516, skewness -0.0161295, kurtosis 4.48806 ) 
  ---
  b_w_gifo_x_corr_   ( min -86.6156, max 114.983, mean 0.204558, stddev 5.67173, skewness -0.73727, kurtosis 27.8048 ) 
  b_w_gifo_r_corr_   ( min -45.1117, max 44.1512, mean -0.0128611, stddev 3.31087, skewness 0.0214509, kurtosis 7.31761 ) 
  b_bias_corr_   ( min -55.7116, max 79.8355, mean -1.04289, stddev 9.22046, skewness 1.55141, kurtosis 15.4515 ) 
  b_peephole_i_c_corr_   ( min -43.4747, max 75.7421, mean 0.183728, stddev 8.95344, skewness 1.95591, kurtosis 22.6838 ) 
  b_peephole_f_c_corr_   ( min -77.2397, max 54.3126, mean -0.443378, stddev 12.8075, skewness -0.754746, kurtosis 6.48421 ) 
  b_peephole_o_c_corr_   ( min -154.764, max 91.383, mean -1.65617, stddev 21.394, skewness -1.67093, kurtosis 12.6603 ) 
  b_w_r_m_corr_   ( min -73.0412, max 71.0797, mean 0.0306955, stddev 5.46516, skewness -0.0146782, kurtosis 5.54274 ) 

  ### Activations (mostly after non-linearities)
  YI_FW(0..1)^   ( min 0, max 1, mean 0.321922, stddev 0.35146, skewness 0.744595, kurtosis -0.932919 ) 
  YF_FW(0..1)^   ( min 0, max 1, mean 0.468084, stddev 0.387587, skewness 0.0318184, kurtosis -1.58881 ) 
  YO_FW(0..1)^   ( min 0, max 1, mean 0.267383, stddev 0.338593, skewness 1.07212, kurtosis -0.357771 ) 
  YG_FW(-1..1)   ( min -1, max 1, mean 0.0196279, stddev 0.752953, skewness -0.0339718, kurtosis -1.40585 ) 
  YC_FW(-R..R)*  ( min -50, max 50, mean 0.249417, stddev 10.3724, skewness 0.319871, kurtosis 16.7519 ) 
  YH_FW(-1..1)   ( min -1, max 1, mean 0.0258908, stddev 0.582409, skewness -0.0403758, kurtosis -0.59762 ) 
  YM_FW(-1..1)   ( min -1, max 1, mean 0.00393123, stddev 0.274852, skewness -0.0745164, kurtosis 5.11252 ) 
  YR_FW(-R..R)   ( min -3.96825, max 3.98172, mean 0.0101538, stddev 0.625752, skewness 0.0619791, kurtosis 2.85553 ) 
  ---
  YI_BW(0..1)^   ( min 0, max 1, mean 0.337595, stddev 0.345592, skewness 0.639149, kurtosis -1.05447 ) 
  YF_BW(0..1)^   ( min 0, max 1, mean 0.477291, stddev 0.368195, skewness -0.0767085, kurtosis -1.47235 ) 
  YO_BW(0..1)^   ( min 0, max 1, mean 0.278746, stddev 0.344601, skewness 1.00306, kurtosis -0.522136 ) 
  YG_BW(-1..1)   ( min -1, max 1, mean 0.0120209, stddev 0.742804, skewness -0.0212293, kurtosis -1.37905 ) 
  YC_BW(-R..R)*  ( min -50, max 50, mean 0.775564, stddev 9.34746, skewness 1.37793, kurtosis 17.8895 ) 
  YH_BW(-1..1)   ( min -1, max 1, mean 0.031361, stddev 0.593592, skewness -0.0187211, kurtosis -0.68069 ) 
  YM_BW(-1..1)   ( min -0.999978, max 0.999995, mean 0.00189404, stddev 0.287114, skewness -0.0521492, kurtosis 4.33523 ) 
  YR_BW(-R..R)   ( min -3.96881, max 3.7777, mean -0.00704291, stddev 0.659131, skewness -0.134412, kurtosis 3.36761 ) 

  ### Derivatives (w.r.t. inputs of non-linearities)
  DI_FW^  ( min -1, max 1, mean -2.3665e-05, stddev 0.0214667, skewness -3.52124, kurtosis 368.537 ) 
  DF_FW^  ( min -1, max 1, mean 2.17467e-05, stddev 0.0173198, skewness 5.83282, kurtosis 586.503 ) 
  DO_FW^  ( min -0.990436, max 1, mean 3.58693e-05, stddev 0.0269749, skewness 0.259336, kurtosis 119.554 ) 
  DG_FW   ( min -1, max 1, mean 0.000131773, stddev 0.0311141, skewness 0.021088, kurtosis 455.366 ) 
  DC_FW*  ( min -7.28433, max 7.77623, mean 0.00260875, stddev 0.229974, skewness -0.603338, kurtosis 177.647 ) 
  DH_FW   ( min -5.41776, max 3.40609, mean 0.000667302, stddev 0.123428, skewness -0.149093, kurtosis 87.1972 ) 
  DM_FW   ( min -6.87041, max 6.64106, mean 0.000809677, stddev 0.315507, skewness 0.00996493, kurtosis 29.7457 ) 
  DR_FW   ( min -1.40803, max 1.59388, mean -0.000308432, stddev 0.084306, skewness 0.113571, kurtosis 17.0549 ) 
  ---
  DI_BW^  ( min -1, max 1, mean -2.83705e-05, stddev 0.0197885, skewness -0.381001, kurtosis 404.413 ) 
  DF_BW^  ( min -0.548613, max 0.837047, mean -1.31064e-06, stddev 0.0134823, skewness 2.7836, kurtosis 250.33 ) 
  DO_BW^  ( min -0.367621, max 0.446774, mean 4.11134e-05, stddev 0.017154, skewness 0.486644, kurtosis 49.5421 ) 
  DG_BW   ( min -1, max 1, mean 9.80838e-05, stddev 0.030215, skewness 0.824272, kurtosis 331.48 ) 
  DC_BW*  ( min -6.74777, max 13.5308, mean 0.0015417, stddev 0.251992, skewness 19.0771, kurtosis 951.277 ) 
  DH_BW   ( min -2.70731, max 2.12649, mean 3.05401e-05, stddev 0.0908835, skewness 0.0564656, kurtosis 57.2427 ) 
  DM_BW   ( min -2.87859, max 2.70528, mean 0.000644948, stddev 0.21697, skewness -0.0437353, kurtosis 9.89234 ) 
  DR_BW   ( min -0.844299, max 0.788093, mean -4.73253e-05, stddev 0.0756507, skewness 0.00613275, kurtosis 7.00628 ) 
Component 2 : <Tanh>, 
Component 3 : <AffineTransform>, 
  linearity_grad ( min -106.066, max 104.259, mean -4.82662e-08, stddev 2.26995, skewness 0.158143, kurtosis 243.222 ) , lr-coef 1, max-norm 0
  bias_grad ( min -249.841, max 245.772, mean 5.06639e-08, stddev 10.695, skewness -0.926009, kurtosis 451.214 ) , lr-coef 1
Component 4 : <Softmax>, 
### END GRADIENT

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:346) PER-CLASS PERFORMANCE:
@@@ Frames per-class: [ 181212 144 127 606 532 43 173 190 515 703 138 211 211 608 45 289 271 572 372 153 368 892 307 454 467 612 374 177 314 165 203 169 193 120 162 350 506 100 142 40889 89709 154 463 1777 286 231 687 352 545 459 79 394 481 237 200 663 1037 475 2210 428 257 167 4192 1910 93 230 151 223 4363 199 339 612 931 689 168 53 430 425 404 147 72 468 398 184 543 242 103 448 1376 60 779 717 668 146 541 163 358 830 561 408 249 268 103 1105 608 477 208 629 257 136 769 185 359 2082 165 69 303 74 247 336 172 147 117 128 443 612 117 125 52 316 956 165 305 574 202 297 222 1167 208 209 172 556 182 606 267 121 534 323 470 239 776 321 812 179 268 1139 537 191 554 5133 217 193 202 128 264 242 282 141 453 190 390 248 274 208 168 117 250 72 227 832 107 208 74 120 577 200 74 173 93 435 313 308 638 462 201 239 312 172 59 151 330 221 210 68 77 424 248 789 348 126 86 566 208 142 331 485 286 372 234 328 637 273 293 132 41 150 57 102 173 655 328 426 861 178 245 87 533 72 131 265 150 790 182 357 233 69 187 112 206 273 85 62 302 205 411 179 278 260 193 525 723 141 288 139 513 788 215 97 150 236 273 455 166 615 208 63 196 43 120 342 911 206 87 1156 192 146 131 166 31 221 252 417 373 499 401 233 179 409 182 439 579 96 189 251 1812 272 55 134 245 227 120 170 454 69 211 126 232 303 701 89 59 87 90 418 122 334 282 91 119 125 379 59 189 157 163 620 120 203 224 199 71 381 86 544 80 247 220 94 114 76 256 175 1186 420 378 336 135 140 331 666 306 356 196 203 291 152 221 543 214 149 360 365 670 264 264 692 216 616 923 524 408 234 395 301 375 105 525 80 359 98 456 204 309 258 288 286 181 90 101 263 53 253 167 354 150 174 194 393 579 494 437 273 142 236 159 226 115 606 198 255 508 136 282 304 327 234 1467 199 113 213 747 267 237 238 267 223 423 127 334 351 773 400 668 269 762 507 73 172 259 944 420 60 318 817 125 150 155 430 99 155 360 316 425 603 146 124 301 210 170 102 140 531 288 716 282 161 233 181 349 120 133 242 135 51 141 111 147 205 551 427 131 99 109 133 445 7274 204 271 135 128 125 377 120 372 449 808 108 557 45 280 47 596 57 276 400 121 347 315 193 77 146 96 105 216 193 341 1125 236 219 105 417 109 338 639 130 396 177 211 138 121 275 91 104 194 51 542 461 346 197 315 121 109 444 210 64 135 84 52 96 283 268 315 144 187 498 569 470 194 79 151 71 285 409 404 165 215 321 466 69 209 585 117 83 141 150 364 478 225 439 81 192 156 773 344 306 243 112 146 203 501 258 143 117 231 316 156 83 53 468 163 539 465 176 116 79 157 350 139 86 253 155 101 132 342 440 324 189 390 153 111 192 365 208 291 263 103 139 134 307 402 452 451 5943 88 406 111 189 488 786 548 86 632 99 382 216 141 365 388 65 90 187 430 300 463 604 247 79 314 305 463 45 473 718 330 223 226 1312 201 160 62 170 227 141 90 90 193 68 539 610 241 156 182 257 224 114 99 209 178 32 193 174 235 238 150 123 308 261 195 275 142 111 371 821 518 831 306 124 432 87 357 932 174 327 225 300 36 190 272 120 358 461 397 462 306 105 348 166 390 216 520 219 260 71 179 258 429 201 53 142 256 406 60 128 173 26 88 191 83 60 155 179 1000 125 44 191 230 383 687 216 257 321 233 417 202 1061 342 327 163 248 165 412 229 303 517 124 815 539 79 306 155 255 89 321 245 73 308 228 196 192 269 176 228 101 8591 469 337 966 237 430 177 258 334 85 304 414 487 401 57 178 99 260 178 151 248 261 926 126 248 230 264 241 147 220 155 139 646 219 209 393 1097 85 160 936 605 267 175 213 208 155 166 207 194 260 180 542 294 745 650 41 235 409 387 268 281 172 798 467 256 138 171 316 378 542 197 77 199 253 693 506 326 151 543 272 142 171 242 241 191 161 274 304 47 226 320 127 248 43 388 236 233 244 332 111 261 196 246 68 466 707 315 146 315 282 30 427 201 153 510 1026 278 186 316 45 464 106 316 404 415 543 346 257 452 1347 138 153 366 558 585 171 109 855 530 221 155 160 161 111 196 131 341 863 116 777 103 428 210 393 95 260 221 295 129 320 516 185 97 126 343 492 405 270 645 150 141 159 382 1153 426 252 535 332 205 417 575 597 292 372 180 236 213 343 301 154 349 528 290 243 378 616 213 266 230 676 309 338 165 324 314 639 310 148 365 184 119 116 199 264 1367 367 467 1442 213 598 641 335 156 122 351 114 129 98 489 213 162 386 116 74 274 197 265 173 129 167 753 211 214 1711 286 746 207 83 123 191 204 353 783 57 223 151 323 285 99 1197 111 116 189 130 401 488 428 96 280 248 468 506 117 284 315 328 129 334 124 557 265 243 383 268 66 97 474 153 221 139 391 194 634 156 322 748 309 195 326 651 462 370 172 385 490 392 610 304 249 70 501 324 335 228 370 206 587 560 108 850 256 209 153 168 206 131 204 232 340 200 284 174 301 124 107 220 67 370 1390 1253 311 434 127 136 104 493 416 778 103 335 338 242 66 208 459 231 96 291 160 220 368 145 728 458 351 255 350 348 137 125 1039 1466 285 221 232 361 605 549 4192 543 376 361 3105 243 319 117 225 232 542 309 859 113 244 913 235 271 853 881 252 243 274 129 90 371 274 176 453 107 172 140 172 411 267 778 147 558 2509 132 95 258 162 203 87 21 467 192 231 265 665 552 160 225 162 279 3235 297 303 235 317 297 155 692 494 82 179 192 734 178 103 204 87 225 63 351 225 260 282 352 367 219 77 182 384 386 147 340 273 260 234 61 251 141 663 194 320 94 144 ]
@@@ Loss per-class: [ 0.39235 1.36389 1.60834 1.16989 1.07191 3.17362 2.23984 1.24253 1.45696 2.15242 1.22774 2.31801 2.14961 1.36338 5.50771 1.19151 1.39358 1.85838 1.96569 2.23373 1.17374 1.30157 1.39527 0.950103 1.18764 1.50099 1.04232 2.91463 1.36931 2.41988 1.36206 1.18303 1.89318 2.26651 0.786779 1.51874 2.1149 1.9898 2.25096 1.60873 0.66341 1.67191 1.44577 1.27463 2.03508 1.34478 0.722197 2.11948 2.06354 1.38499 2.42884 1.6098 1.61614 2.02316 1.9802 1.51408 1.29257 0.864962 1.0151 1.4183 1.92097 1.19697 0.523798 0.408847 2.67492 1.93035 2.40574 1.76538 1.36123 1.08076 0.975458 1.10656 1.10521 1.49949 1.58511 2.93046 0.5933 0.379228 1.06162 1.64582 2.17491 1.75397 0.816316 1.22735 1.37153 1.9009 2.62452 1.60729 1.52146 3.48431 1.22731 1.3761 1.10412 2.9615 1.61118 2.52461 1.27434 0.620958 0.902511 1.36437 2.44871 1.48083 2.33431 1.16053 0.901406 1.14687 1.94937 1.9848 1.76616 1.36795 2.30524 2.6509 2.4269 0.841011 2.47088 4.41085 1.82813 2.61082 3.05272 1.72773 2.08607 3.8288 2.26622 2.46363 1.06147 0.872518 2.81514 2.54902 3.75656 1.15686 0.577785 2.27826 1.00739 1.19935 2.3477 0.957845 1.40055 0.586248 1.67975 1.91931 2.13438 1.00959 1.4271 1.09395 1.36986 2.07256 1.69089 2.48167 1.27063 0.911708 0.458865 1.78818 2.06323 2.5993 1.72348 0.80888 0.527511 1.25962 0.950556 0.632491 1.15101 1.85704 2.42287 5.29119 1.47917 1.72066 1.21217 1.86214 0.875515 2.00314 0.768919 2.27223 1.69039 1.72859 2.42836 1.16825 1.22729 1.66556 2.72772 0.556943 2.15705 2.09842 2.36094 1.78571 0.598287 1.61907 4.75017 1.99561 2.71711 1.3292 1.31796 1.49652 0.595004 1.17754 1.64431 1.44472 1.74943 2.3764 2.56325 1.35253 2.58812 2.30747 1.92144 3.2575 2.93882 1.30275 2.33049 1.23392 1.37722 2.27414 2.94934 1.70055 1.07248 13.0476 2.00154 1.08185 1.78728 1.03516 1.08176 2.03176 1.26277 1.12374 2.4662 1.92462 3.0316 2.35555 3.62262 1.95263 1.5062 1.34382 2.06466 1.12774 1.29753 2.98159 1.39505 3.65349 1.21826 3.36639 2.24892 1.64926 2.4448 0.723958 1.66182 1.40313 1.98942 3.70239 1.79743 4.91363 2.46094 2.18414 3.83201 2.035 2.09715 1.9006 1.83938 1.2095 2.57983 1.80882 1.25707 1.24539 1.29619 2.15539 1.83532 2.81048 1.62678 1.09664 2.10447 2.72583 1.98445 2.16383 1.19816 1.64467 2.79416 2.25179 3.15483 3.80287 1.87194 5.02401 1.45396 2.19891 1.58507 2.63795 2.61928 1.10108 1.7007 2.15411 3.06849 1.693 8.72557 2.92162 1.9868 2.43029 1.17266 1.23323 2.44579 1.89355 1.37797 1.42699 2.23459 2.29681 0.882642 2.05367 1.56083 1.19252 0.452904 1.48092 5.22763 2.47459 2.80649 2.73574 1.83055 1.79886 1.57572 2.41371 2.11397 2.91051 1.0715 1.91737 0.746706 2.52059 3.98595 1.44677 3.58849 0.69674 2.902 2.72319 2.17372 4.93699 2.78789 1.8609 1.28145 2.599 2.57729 1.81195 1.93083 0.932989 2.76989 2.16991 1.89804 1.8296 1.77353 1.30623 2.22731 1.2592 2.08232 1.76388 1.81008 2.67339 2.4357 2.48756 1.937 1.77782 0.829117 0.554193 1.57282 1.53495 1.53746 1.95551 2.23371 0.639805 1.29802 2.70614 2.08444 3.26804 1.64342 2.44331 0.904546 1.62439 1.20229 2.23966 1.73342 2.18149 0.90745 2.05983 1.46534 1.03138 1.30833 1.29993 1.22964 1.72165 1.76922 1.77734 1.07593 1.71279 1.65953 2.39859 1.39163 2.18161 1.77369 2.37076 1.7531 2.08321 1.66631 1.32588 1.40356 1.58013 2.37556 1.8534 2.36737 1.01276 5.04945 1.0666 1.81534 1.61005 2.87196 1.75696 1.57489 1.84814 1.53415 2.21491 3.47095 3.25432 2.50394 1.8174 2.02112 1.14293 2.1065 0.793478 2.98252 1.29683 0.921806 1.3287 1.20849 1.19862 1.2375 1.97988 0.514385 0.797499 3.10955 1.03897 1.51753 2.44097 2.16156 2.02524 1.48361 1.14184 2.04543 1.73974 1.44571 1.57765 0.695048 1.86026 0.857561 1.97096 1.516 0.999246 4.16887 1.57651 1.91661 0.570781 1.38666 1.90568 1.65554 0.848775 1.80251 6.92415 1.11478 1.8424 2.61906 2.49901 0.954335 1.44905 2.49222 1.76839 2.5212 3.51815 2.16559 1.61123 2.01149 2.44541 0.920329 0.752687 1.73652 1.25832 1.46279 0.886522 0.938865 1.36215 1.67155 1.86346 4.93122 2.4474 1.62395 2.52503 2.15489 1.78152 2.32611 1.85546 0.788531 1.95694 1.93101 2.17961 3.72865 2.49514 1.07554 0.842596 1.03181 2.70205 3.34586 1.43268 2.06588 1.92253 1.885 2.83937 0.966959 2.04359 3.80446 0.943196 4.97394 1.67736 4.56518 1.54913 2.49263 1.49156 1.4527 2.41503 2.24732 1.0703 3.44147 2.52067 3.52144 2.34398 1.69311 4.75201 2.58089 1.49381 1.01383 2.39116 1.90307 3.50495 2.60463 3.47816 1.66944 1.26554 2.75987 2.37344 1.57675 1.69001 1.78438 2.8381 1.79075 2.5639 2.62657 2.19986 4.22578 0.472817 1.01904 1.66006 1.23933 1.90145 2.72947 4.16032 1.45895 2.63474 2.6373 1.63779 3.10895 3.31883 2.64684 2.68998 4.30574 2.52032 2.59583 2.24719 0.852583 1.21197 0.921117 1.35441 2.81061 2.29892 2.72707 1.47758 1.32184 1.00063 2.45948 2.59627 1.34607 1.16372 3.42664 1.62132 1.60487 3.15383 2.41923 3.78895 1.8466 1.63119 1.49374 1.99778 2.08036 3.46091 1.65525 1.91016 1.57445 0.955069 2.29925 2.4642 3.9717 2.176 2.78129 1.34792 2.80232 3.31223 3.06225 2.88439 1.78311 2.18116 2.19131 3.95449 0.578407 2.12102 1.32979 1.21931 1.7734 2.01147 2.36579 2.8886 3.1265 2.87 1.68443 2.90152 2.02367 1.73964 2.05904 1.27732 1.74841 1.42394 1.74442 2.72193 1.77916 2.34614 3.37084 3.35721 2.17073 2.21238 1.23112 2.98555 1.58567 1.95956 1.82766 1.79978 1.20245 1.08142 1.06119 1.57036 1.28844 1.32271 2.47643 1.28449 1.09587 1.46064 6.36375 0.658694 4.36406 1.64842 2.64331 2.39027 1.8109 1.51956 7.24598 2.95528 2.14057 1.70722 1.04639 1.11058 0.976477 3.02782 3.20445 1.73634 2.10217 1.95586 7.91534 1.4303 1.26417 1.66648 3.27569 3.48207 1.41748 1.95662 2.72511 5.17252 2.36775 2.40742 2.05886 2.59186 2.91371 2.12897 3.77274 1.37398 1.12565 2.99076 2.67015 1.56494 2.38679 1.41245 3.04543 2.66727 2.37405 3.38352 4.76916 1.93163 1.89091 1.01298 1.77132 2.19704 2.20427 1.76313 1.82756 2.44491 1.1498 2.25705 5.03624 1.12148 0.654454 1.44088 0.901191 1.76164 2.57113 1.94189 3.76719 3.50848 2.63759 2.81629 1.99059 3.01171 2.12904 6.15773 2.06803 2.69668 2.31902 0.886735 1.70856 1.46908 1.51044 2.7906 2.70572 1.88053 2.64628 1.65772 2.1269 1.34317 1.95843 1.62444 2.76517 1.76529 1.50573 1.62053 1.65285 5.84919 1.64775 1.85513 1.65429 3.19537 2.43103 1.77525 3.98303 2.96692 2.30676 4.72327 3.52069 1.80692 2.20134 0.983638 3.07495 4.50353 3.10723 2.86193 2.10947 1.09939 2.30893 1.54414 1.65499 1.67372 1.67697 1.3996 1.96568 1.93579 2.39039 4.17914 1.967 2.08477 1.11291 1.43288 2.3086 1.48236 4.08044 0.650517 1.40783 2.05411 2.04514 1.33909 1.64925 3.93169 1.5617 1.79547 4.06774 1.08306 2.16465 3.02526 1.98054 1.85993 2.82663 5.3803 3.37292 0.32844 1.23878 3.04123 0.863684 2.62227 2.00051 3.07154 1.44702 1.67186 5.73813 1.40484 1.84212 1.64448 2.18923 4.5453 2.80202 3.28249 0.945104 1.71612 2.11986 2.88643 1.85643 1.21026 3.34756 2.39669 2.34594 2.34535 1.64811 3.57738 2.61682 1.89781 2.45636 1.34142 1.94215 3.43509 2.17238 1.2854 3.55923 2.16448 0.913923 1.4676 1.38539 1.70337 2.28915 2.21012 2.49772 2.15508 2.60956 2.47959 3.0085 2.22361 1.63176 1.76948 1.08412 2.03426 4.4612 2.12068 2.16894 1.48349 1.87545 1.65474 3.0266 1.26998 1.56492 2.4789 3.63503 0.973777 1.96105 1.92424 1.48298 2.23627 4.03671 3.22678 1.48911 1.09334 1.41806 1.8087 1.48341 1.00326 1.3369 3.58521 2.53571 2.27974 2.15057 2.34274 1.07812 1.55536 1.38083 3.1223 1.81803 1.46873 2.68935 1.06895 4.14764 1.23581 2.02582 2.10381 2.73982 1.74494 2.40286 1.62294 2.63713 1.71056 3.98837 0.924351 1.03029 1.62168 4.01904 1.32514 1.34694 7.45816 1.71923 2.90464 2.18909 1.60067 2.02328 1.80822 2.79802 2.07093 5.87225 2.23463 2.29864 1.74333 1.43541 1.69329 2.26375 2.33033 1.75961 1.07914 1.60379 2.68145 3.43101 2.48196 2.13106 1.79055 2.68436 3.0464 1.57565 2.08112 2.09295 2.72198 1.79087 2.10835 3.21099 1.49784 2.99846 1.72049 1.18612 1.55519 1.36761 2.25458 1.31474 0.944673 1.45825 2.21076 2.43028 1.85469 3.70907 2.96449 3.11898 1.86703 2.47768 2.458 5.40108 2.5256 1.18053 1.44792 2.3133 1.46639 2.06863 2.92586 2.71893 2.48714 2.50702 1.89437 2.66826 1.71469 1.22702 2.4742 0.904458 1.11179 2.39035 1.81399 1.50317 1.80051 2.11663 2.25662 2.10997 1.76106 2.2685 1.90619 1.83771 1.11932 1.85716 1.36134 1.85956 1.66852 2.21908 2.16525 1.28954 1.47761 1.44241 1.91662 1.63628 1.61841 1.79945 2.62598 3.28168 1.99727 1.72913 1.44594 3.03856 2.11092 2.16459 1.75321 2.61217 1.74876 0.636885 1.19146 2.26163 0.818565 1.77967 1.31328 3.03894 2.31795 2.30179 3.09638 3.41473 1.62935 2.15742 1.88992 1.79767 3.37277 3.46531 2.18628 2.80864 2.68921 2.77055 2.22966 2.28746 2.70988 1.31362 3.4182 0.861055 2.94632 1.06652 3.45533 2.43537 4.03393 2.65892 2.29878 1.91394 0.703454 3.77287 1.44022 2.65028 2.1573 2.04797 4.27883 1.16817 3.31534 2.23966 1.69303 2.43861 1.28219 1.54517 1.3596 3.6128 1.55932 2.25863 2.00193 1.15945 2.32827 1.83795 2.25673 1.69628 3.20235 1.8623 3.16237 2.67035 2.87261 1.98395 0.956455 1.11802 3.25506 4.04806 1.49726 2.78195 2.85124 2.42799 1.51444 1.81479 1.57818 2.97466 1.36041 1.57611 1.29416 3.08816 1.67871 1.71537 2.08759 1.42792 2.39353 1.38582 2.13051 1.28396 1.04458 1.89589 3.43777 4.694 2.1881 2.14259 2.92061 2.00832 1.57684 3.21378 1.33867 1.42797 2.31118 1.75645 2.42228 1.69202 1.62494 1.36145 2.15243 2.91838 1.70152 2.61316 1.66559 2.64109 1.58044 2.76187 1.53624 3.12619 4.46083 2.25121 2.34043 1.99776 0.578169 0.528217 1.00181 2.2184 3.46457 2.68633 2.81035 1.87383 1.98475 1.03214 2.1981 2.62367 2.10805 1.29883 6.90519 1.88318 1.32716 1.88418 2.38912 1.99425 2.37921 1.28941 2.71377 2.53152 1.30829 1.0565 1.0261 1.97958 2.48319 0.966326 1.29814 1.66579 1.12001 0.896645 2.37576 1.99736 2.2243 1.48729 1.31786 1.33071 0.801022 2.08274 2.05333 1.30603 0.820106 2.16187 2.08521 2.19886 2.43671 2.53373 2.07685 2.74699 0.953647 3.02825 1.48716 1.26288 2.42583 1.89605 2.00006 0.980622 1.53889 3.10313 2.51522 2.0863 3.25171 2.65818 2.18572 2.58839 1.83938 3.4143 2.00932 3.28924 1.43854 1.93925 2.97806 2.0741 2.26468 1.40685 1.15834 2.25429 2.67148 2.79924 1.70693 2.59063 3.85632 11.2653 1.68226 1.54881 2.67186 2.77081 2.41638 3.23623 2.681 2.60799 1.6125 2.61877 0.987729 1.705 2.36969 1.67101 2.65014 3.29865 3.0491 1.67028 2.18204 3.90011 3.95663 2.59751 1.23016 2.07691 2.24516 2.36947 3.02983 1.79801 5.26335 1.8677 2.35607 2.37774 2.42176 1.35925 2.22617 1.15461 4.04605 1.12916 1.59119 1.63754 3.20391 1.78682 1.50248 2.83718 2.8555 3.11017 1.84678 3.34926 1.49953 1.60212 2.74465 2.0971 2.89309 ]
@@@ Frame-accuracy per-class: [ 85.3495 60.2076 50.1961 69.4147 72.8638 2.29885 41.4986 62.9921 56.0621 35.5366 65.704 44.4444 46.3357 60.1479 0 71.1572 57.0902 48.0349 35.4362 38.4365 70.5563 59.2717 60.4878 76.1276 72.9412 54.5306 77.1696 13.5211 58.8235 39.8792 60.9337 54.8673 50.646 21.5768 78.1538 58.2026 34.156 40.796 42.807 36.7552 74.7435 48.5437 50.2697 61.7159 32.1117 61.7711 80.2909 43.9716 31.8973 58.9771 17.6101 53.9924 51.7134 40.8421 38.9027 56.8199 51.4699 75.0789 72.0199 57.4096 49.3204 60.8955 83.4109 87.3593 29.9465 28.1996 28.3828 51.0067 61.9915 68.6717 74.2268 71.3469 69.7799 55.4025 58.1602 14.9533 82.9268 91.6569 71.9407 69.1525 35.8621 55.0694 77.2898 63.9566 62.7415 37.1134 32.8502 62.2074 59.4261 14.876 61.7062 63.9721 70.6058 23.2082 58.9104 30.581 67.5035 81.8784 72.6625 61.4443 30.0601 59.9628 25.1208 68.7472 76.4174 64.9215 46.5228 45.274 44.2718 69.5971 29.1098 19.9461 35.3268 77.6951 31.4199 4.31655 43.1631 29.5302 21.4141 59.7325 49.2754 7.45763 48.5106 17.1206 70.8005 72.6531 22.1277 31.8725 1.90476 68.5624 84.3701 32.6284 67.4304 66.6667 40 80.3361 59.7753 82.8266 56.1151 49.642 28.9855 68.823 62.4658 68.92 68.4112 43.6214 55.5659 36.1669 61.2115 72.6514 88.3451 41.6796 37.7846 15.5989 55.4935 76.086 85.2093 65.7963 71.4157 80.0234 65.2874 46.5116 38.5185 0.77821 58.2231 56.0825 71.5044 58.6572 76.516 37.2703 77.0807 40.2414 52.459 61.8705 32.6409 63.8298 65.8683 44.1379 23.2967 81.4414 37.2093 44.6043 25.5034 55.6017 85.8874 47.3815 0 41.4986 18.1818 61.0792 69.8565 53.1605 85.8262 68.1081 51.1166 55.5324 42.24 37.1014 35.2941 67.9868 24.5083 32.9571 44.1805 7.29927 10.3226 68.7868 36.6197 69.6643 66.571 30.83 27.7457 48.3672 71.4628 0 46.1538 71.8847 48.8656 75.1678 68.6567 47.793 62.2745 71.298 43.6116 40.7547 9.63855 29.9003 5.21739 50.7317 62.2478 55.5301 43.2268 64.9472 63.3778 26.3305 65.9878 4.57143 65.2296 4.13793 20.5323 53.8606 37.8738 75.3953 58.0822 59.8601 47.9657 0 49.6 2.66667 36.8039 42.4132 10.5263 40 46.6116 51.0949 50.5468 71.3092 34.8294 51.8234 64.0827 68.8868 66.3442 36.0424 53.0329 40.1434 47.1276 75.5866 45.0116 24.6154 28.5714 51.1628 66.5448 48.7377 32.4324 31.6816 20.6235 0 53.4351 0 58.9212 35.6204 56.8294 28.5714 19.4286 67.3584 43.1169 47.7816 17.4905 47.4474 0 28.8939 49.505 38.3234 71.2182 62.2623 38.3562 47.5375 64.0669 63.4921 38.9041 40.5006 77.4806 52.8497 54.8813 64.4135 89.6552 66.789 0 34.2007 24.4399 27.2527 45.6432 49.8534 56.7657 17.2662 40.6619 22.1344 79.5699 47.7759 79.9715 31.2849 1.68067 74.2857 12.1547 80.2867 13.0612 20.6278 32.5664 2.18579 20.9205 51.7928 65.8762 13.4454 40.6332 46.3492 50.7645 72.6833 24.8963 48.1572 44.5434 46.6165 46.1538 62.9096 45.0867 61.157 32.2981 51.3131 46.712 25.3968 34.0611 31.3725 39.7661 52.4217 75.5162 87.7527 62.3514 64.1902 42.0664 60.4982 46.7572 83.7209 66.5579 29.453 47.8372 15.2334 53.8593 27.541 81.2641 60.7176 71.3287 44.1472 51.0402 43.5021 72.7815 45.3686 48.3932 80.5776 71.5935 65.2068 68.2187 49.571 50.1836 41.791 73.3249 53.068 56.7244 26.5403 67.3644 24.8447 57.58 44.6701 43.1544 46.4548 61.3893 56.4797 48.5269 54.4503 40.2204 51.9337 42.3645 71.7268 0 80.4734 48.9552 62.0592 27.907 58.4527 61.6967 51.0801 61.2597 37.2093 17.1429 24.4973 30.8772 50.74 47.6489 74.6137 38.961 77.4938 14.6096 65.362 77.4828 56.4103 64.7788 70.2791 70.229 59.2751 88.8586 81.203 19.3833 73.5363 59.6656 34.7664 43.7895 44.0252 59.8131 72.0358 32.8217 50.1961 60.6876 60.5974 80.8016 54.6816 71.653 55.6586 60.459 76.2562 0 59.7101 53.1792 82.7951 63.0202 28.0992 56.8289 75.2294 55.7769 0 56.5916 46.6899 30.1508 40.5145 72.3994 63.5071 30.5523 47.3902 30.0341 13.6546 39.1376 58.9074 44.5748 34.1463 84.6975 82.9727 52.3397 67.5506 63.7168 75.5418 75.803 61.708 42.0601 49.7925 2.99625 40.8247 67.1587 31.068 29.682 50.2242 26.4407 57.4209 78.6945 48.4211 31.1787 37.1859 1.82648 33.7079 69.5847 70.589 76.2836 26.8877 16.2362 52.1401 33.4661 39.4702 39.834 35.4362 77.4194 40.1979 2.76498 75.8744 4.3956 53.8324 2.10526 54.3168 19.1304 58.2278 60.4245 37.8601 38.5612 68.4628 9.81912 24.5161 8.87372 44.5596 51.1848 5.08083 32.5581 48.9019 69.9245 36.3636 57.8588 32.2275 23.2335 6.39269 55.8346 61.767 22.9885 49.9369 58.0282 58.6288 59.9278 6.58436 39.9274 33.8798 28.7081 43.1877 9.70874 84.7926 72.3727 49.3506 71.3924 46.5927 23.0453 2.73973 64.342 16.6271 21.7054 47.9705 13.0178 22.8571 17.6166 32.4515 1.48976 38.3518 25.6055 34.1333 74.0221 67.252 76.7269 62.7249 20.1258 35.6436 32.1678 59.1944 57.3871 75.6489 31.4199 29.6984 66.563 69.6677 12.9496 58.2339 45.9436 17.0213 41.9162 6.36042 36.5449 55.144 49.1118 47.8936 41.8658 7.36196 64.9351 47.9233 59.3407 76.3425 42.0881 31.6222 13.3333 43.0034 34.398 62.4128 25.9188 14.6341 9.3617 21.5983 42.0221 38.9776 31.1377 1.86916 84.952 42.8135 69.1381 72.8249 54.3909 44.6352 26.4151 24.7619 17.4037 27.957 50.8671 26.0355 37.299 57.1429 49.8113 66.5693 42.2247 65.0231 56.9921 30.4738 42.3453 35.8744 12.987 26.8126 47.0024 44.2539 62.6186 13.5266 62.3656 26.0223 51.0569 50.1863 66.5193 71.3178 63.6998 65.5367 58.3026 69.0583 34.3008 67.9632 73.1087 52.8715 0 82.2134 4.0201 60.3922 30.485 20.4947 62.3803 54.0541 0 28.7293 28.8 52.0325 74.5424 69.4714 75.5997 11.3131 10.0629 52.4642 45.8265 40.5609 0 61.6684 64.0223 53.8578 13.4228 13.6865 60.8762 43.6725 23.676 0 38.1232 29.4505 41.6961 27.6243 13.2597 32.0413 0 64.3188 71.9083 8.69565 24.2812 50.9589 30.2913 66.3697 13.1004 15.0754 34.8449 12.3249 0 50.1292 48.1375 75.1592 44.4444 34.5515 34.8178 47.0016 58.1262 35.8056 68.2396 33.6842 0.896861 64.603 82.1668 58.6307 78.653 54.8124 28.9157 45.3179 0 17.6224 26.4879 29.7994 51.6031 13.7472 23.2945 0 44.6194 25.3211 49.7925 72.8033 53.7378 54.8428 60.7568 33.9315 28.436 51.363 30.03 53.5211 42.9561 67.8194 45.1025 54.5106 16.7832 59.61 56.0928 54.2491 50.1241 0 50.5263 60.8187 59.7786 8.26446 32.6848 48.9914 0 28.2486 28.7206 4.79042 3.30579 42.4437 50.6964 75.962 10.3586 0 9.39948 18.2213 44.8501 64.1455 39.261 56.699 55.9876 57.3876 51.7365 53.3333 39.8493 55.1825 33.5878 3.66972 57.1429 38.6707 68.8485 60.1307 37.2323 64.5411 8.03213 84.9785 65.6163 37.7358 46.6558 68.1672 48.9237 18.9944 50.3888 50.9165 1.36054 73.5818 47.7024 23.9186 49.8701 47.4954 18.6969 8.75274 13.7931 89.8795 65.6017 16 80.1862 34.5263 49.4774 27.6056 59.9613 46.6368 3.50877 66.6667 45.5971 53.3333 39.8506 10.4348 24.6499 10.0503 69.4818 46.4986 34.3234 31.3883 50.8604 66.3788 27.668 35.8149 38.1779 41.2098 52.588 12.2034 31.746 39.8714 35.8423 64.5012 40.5467 11.4558 35.0699 61.4123 9.35673 44.2368 74.2125 61.4368 49.7196 53.5613 31.3817 35.012 16.0772 47.4474 32.2892 39.0746 19.9616 37.6731 56.0369 47.5382 69.7518 42.1214 2.40964 34.8195 44.2002 56.7742 36.1266 52.2202 20.2899 63.1183 62.246 37.8168 20.9386 68.8047 46.7615 42.5363 55.6682 28.8608 3.87097 32.0802 56.4103 72.0981 61.9941 52.9862 49.505 78.7489 63.4862 13.3333 18.6589 31.7526 38.5093 35.5091 73.065 63.388 59.4417 14.7368 57.3951 63.9626 17.2549 72.837 2.29885 69.7555 39.7463 38.5439 34.7648 57.4436 31.3901 58.1262 29.0076 53.5497 7.29927 77.3848 69.8233 54.8336 7.50853 61.8067 60.885 0 50.7602 23.8213 44.9511 50.3428 43.1564 49.9102 19.8391 50.8689 0 39.1819 45.0704 52.4487 64.5241 47.4128 39.7424 42.7128 47.767 68.5083 48.9796 25.2708 8.46906 33.8336 45.658 46.1144 29.7376 22.8311 55.9906 50.3299 46.5011 27.0096 51.7134 40.8669 16.1435 65.1399 8.36502 49.1947 69.3688 64.3777 58.135 37.6812 66.2777 75.5344 68.615 46.0733 41.0749 43.7923 3.38409 22.3938 24.649 48.9835 33.9623 23.5897 0 25.6186 66.599 57.2133 34.7505 64.2912 43.8538 29.682 18.1818 33.9869 25.401 48.5346 24.9505 47.0588 66.4662 43.309 66.3473 64.1182 32.636 47.1795 53.9597 56.5097 44.3975 46.8384 29.4032 53.068 35.5987 49.7854 43.3302 72.2892 52.5667 65.2576 50.4461 56.6745 44.2777 41.2148 61.3452 58.8045 59.0842 49.5468 58.5516 52.1463 56.294 28.3414 18.8552 41.3133 55.8266 53.5565 21.4592 42.6065 43.1002 48.2633 36.4626 57.754 78.1282 68.3841 32.7485 78.7217 54.2474 58.147 29.3878 48.0797 32.3144 14.6718 6.09137 47.191 44.4965 48.6154 57.6973 9.44206 5.36913 36.7942 31.8987 27.4953 23.0548 37.0656 29.2537 28.2681 61.4657 18.1818 74.7298 18.4991 70.4622 21.6867 33.5329 6.47773 28.7206 45.4768 51.2023 82.4505 3.47826 63.5347 26.4026 40.8037 44.8336 0 67.3904 20.6278 35.1931 59.1029 37.5479 64.5081 68.5773 55.3092 11.399 67.0232 42.2535 44.8239 61.7966 28.9362 41.1248 43.1062 58.1431 18.5328 47.2347 11.245 27.8027 26.742 48.8706 73.5332 62.5698 13.5338 20.5128 61.117 26.0586 27.088 34.4086 53.129 58.6118 60.3625 15.9744 61.7054 59.4522 64.9435 16.3683 45.6355 46.815 46.4865 66.9366 33.0435 60.1816 43.0173 66.7516 73.5463 57.1429 18.4369 1.41844 42.4726 40.3698 28.9121 44.2013 56.1404 5.32688 64.5106 54.9509 34.1014 48.6772 24.1715 51.5513 46.2541 55.1929 37.2881 15.9696 57.2127 23.2258 58.7372 34.414 60.4569 26.361 54.063 20.8835 1.86047 33.1066 37.037 48.8529 84.7177 87.0363 68.6998 30.3797 14.1176 24.9084 28.7081 43.9716 38.6555 72.8324 36.715 27.4218 32.2009 63.0928 0 45.0839 65.7236 41.9006 36.2694 51.1149 41.7445 60.771 35.5495 32.9897 64.6534 74.373 71.6927 46.5753 39.6576 73.4577 60.3636 43.8247 74.5551 72.3491 42.3818 32.0542 46.4516 64.177 64.7399 65.3321 78.2826 36.0626 36.919 61.964 72.2589 42.7105 48.5133 53.617 35.9202 27.5269 34.6544 20.6785 72.484 14.0969 61.7587 63.3826 33.5456 47.5138 49.5606 75.7799 59.4059 21.3552 36.4299 37.0656 15.4696 32.8398 34.6084 30.0283 46.7475 24.186 48.6957 22.7758 56.2319 40.5832 23.1776 44.9583 35.2542 62.8469 64.1164 36.2264 25.1309 27.853 53.5385 25.0614 4.57143 0 53.0481 56.6234 36.2851 27.8719 30.3531 17.9186 21.1838 24.3902 65.8462 27.907 71.7354 51.7647 34.2669 63.2696 40.315 18.8235 23.1511 59.4946 36.1982 2.42424 15.0418 23.8961 70.2519 40.3361 36.715 37.6528 20.5714 49.2239 0 53.2006 54.5455 39.1555 31.5044 55.6028 48.4354 71.5262 5.16129 74.5205 55.1365 53.5576 14.2373 54.9192 48.6289 18.0422 20.0426 24.3902 46.9185 16.2544 64.8078 57.5835 28.7051 38.0952 20.7612 ]

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:347) Done 2622 files, 1 with no tgt_mats, 0 with other errors. [TRAINING, 0.335083 min, fps37556]
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:353) AvgLoss: 1.21172 (Xent), [AvgXent: 1.21172, AvgTargetEnt: 0]
progress: []
FRAME_ACCURACY >> 64.1953% <<

