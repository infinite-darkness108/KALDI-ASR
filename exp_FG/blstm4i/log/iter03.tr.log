speech-HP-Z2-Tower-G9
nnet-train-multistream-perutt --cross-validate=false --randomize=true --verbose=0 --num-streams=10 --max-frames=15000 --learn-rate=0.00004 --momentum=0.9 --l1-penalty=0 --l2-penalty=0 --feature-transform=exp_FG/blstm4i/final.feature_transform 'ark:copy-feats scp:exp_FG/blstm4i/train.scp ark:- | apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_tr90/utt2spk scp:data-fbank/train_tr90/cmvn.scp ark:- ark:- | add-deltas --delta-order=2 ark:- ark:- |' 'ark:ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- |' exp_FG/blstm4i/nnet/nnet_iter02_learnrate0.00004_tr2.0871_cv2.6616 exp_FG/blstm4i/nnet/nnet_iter03 
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:243) Not in compute-exclusive mode.  Suggestion: use 'nvidia-smi -c 3' to set compute exclusive mode
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:438) Selecting from 1 GPUs
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:453) cudaSetDevice(0): NVIDIA RTX A2000 12GB	free:11515M, used:515M, total:12031M, free/total:0.957153
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:501) Device: 0, mem_ratio: 0.957153
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:382) Trying to select device: 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:511) Success selecting device 0 free mem ratio: 0.957153
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:FinalizeActiveGpu():cu-device.cc:338) The active GPU is [0]: NVIDIA RTX A2000 12GB	free:11001M, used:1029M, total:12031M, free/total:0.91443 version 8.6
copy-feats scp:exp_FG/blstm4i/train.scp ark:- 
apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_tr90/utt2spk scp:data-fbank/train_tr90/cmvn.scp ark:- ark:- 
add-deltas --delta-order=2 ark:- ark:- 
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:144) TRAINING STARTED
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Read():nnet/nnet-matrix-buffer.h:191) Read() started... Buffer size in MB: 0, max 3072, having 0 utterances.
ali-to-post ark:- ark:- 
ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl 'ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |' ark:- 
LOG (copy-feats[5.5.1074~1-71f3]:main():copy-feats.cc:143) Copied 2624 feature matrices.
LOG (apply-cmvn[5.5.1074~1-71f3]:main():apply-cmvn.cc:159) Applied cepstral mean and variance normalization to 2624 utterances, errors on 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:303) ### After 0 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:304) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.355111, max 0.336743, mean 0.00350754, stddev 0.0736472, skewness 0.013446, kurtosis -0.132832 ) 
  f_w_gifo_r_   ( min -0.445032, max 0.400516, mean -0.000572561, stddev 0.0761557, skewness 0.00093636, kurtosis -0.0106518 ) 
  f_bias_   ( min -0.353698, max 1.28263, mean 0.218172, stddev 0.453301, skewness 1.07434, kurtosis -0.655883 ) 
  f_peephole_i_c_   ( min -0.375989, max 0.369434, mean -0.00579134, stddev 0.114224, skewness -0.0540289, kurtosis 0.698678 ) 
  f_peephole_f_c_   ( min -0.49363, max 0.788645, mean 0.00204381, stddev 0.141326, skewness 0.32287, kurtosis 4.37867 ) 
  f_peephole_o_c_   ( min -0.495372, max 0.422945, mean -0.015189, stddev 0.159471, skewness 0.242536, kurtosis -0.181462 ) 
  f_w_r_m_   ( min -0.485376, max 0.481062, mean 0.000478289, stddev 0.0957115, skewness 0.000127524, kurtosis -0.0422635 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -0.475218, max 0.440725, mean 0.00599599, stddev 0.0751405, skewness -0.0224974, kurtosis 0.311476 ) 
  b_w_gifo_r_   ( min -0.339368, max 0.30896, mean -0.000269816, stddev 0.0668426, skewness 0.00194501, kurtosis -0.467899 ) 
  b_bias_   ( min -0.293372, max 1.16398, mean 0.211784, stddev 0.446314, skewness 1.06912, kurtosis -0.679056 ) 
  b_peephole_i_c_   ( min -0.341488, max 0.265574, mean 0.00594144, stddev 0.0855194, skewness -0.0237139, kurtosis 0.688341 ) 
  b_peephole_f_c_   ( min -0.421799, max 0.560813, mean 0.0115371, stddev 0.131154, skewness 0.627457, kurtosis 3.35238 ) 
  b_peephole_o_c_   ( min -0.524336, max 0.345686, mean -0.0175651, stddev 0.15546, skewness -0.215057, kurtosis 0.133917 ) 
  b_w_r_m_   ( min -0.358216, max 0.33664, mean -0.000146665, stddev 0.0816151, skewness -0.0029994, kurtosis -0.17505 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.77182, max 0.67057, mean -0.000155923, stddev 0.102461, skewness 0.00622658, kurtosis 0.0578327 ) , lr-coef 1, max-norm 0
  bias ( min -0.0638723, max 1.94461, mean -9.31323e-10, stddev 0.0614953, skewness 25.218, kurtosis 781.021 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:305) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -8.67671, max 9.02116, mean 0.0321104, stddev 1.01793, skewness 0.450612, kurtosis 3.20408 ) 
[1] output of <BlstmProjected> ( min -4.32684, max 4.44516, mean -0.000224099, stddev 0.752655, skewness 0.00822377, kurtosis 1.36532 ) 
[2] output of <Tanh> ( min -0.999651, max 0.999725, mean -0.000199754, stddev 0.503621, skewness -0.00170748, kurtosis -0.731736 ) 
[3] output of <AffineTransform> ( min -15.2574, max 19.6196, mean 0.00521868, stddev 2.05338, skewness 0.697367, kurtosis 2.91869 ) 
[4] output of <Softmax> ( min 8.63878e-13, max 0.997011, mean 0.000779273, stddev 0.0157453, skewness 36.7943, kurtosis 1552.51 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:307) 
### BACKWARD PROPAGATION BUFFER CONTENT :
[0] diff of <Input>  ( min -3.58748, max 3.34071, mean -0.0019083, stddev 0.212878, skewness -0.220757, kurtosis 15.8332 ) 
[1] diff-output of <BlstmProjected> ( min -0.634959, max 0.631328, mean 0.000143665, stddev 0.0595914, skewness -0.0101043, kurtosis 3.5148 ) 
[2] diff-output of <Tanh> ( min -0.738025, max 0.82101, mean 0.000144816, stddev 0.0785499, skewness -0.00236795, kurtosis 1.83491 ) 
[3] diff-output of <AffineTransform> ( min -0.999999, max 0.953745, mean -6.48761e-07, stddev 0.0236847, skewness -23.9146, kurtosis 1178.19 ) 
[4] diff-output of <Softmax> ( min -0.999999, max 0.953745, mean -6.48761e-07, stddev 0.0236847, skewness -23.9146, kurtosis 1178.19 ) 
### END BACKWARD


LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:308) 
### GRADIENT STATS :
Component 1 : <BlstmProjected>, ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  ### Gradients 
  f_w_gifo_x_corr_   ( min -39.5422, max 31.6837, mean 0.0389421, stddev 3.17912, skewness 0.00986112, kurtosis 10.92 ) 
  f_w_gifo_r_corr_   ( min -64.3897, max 68.5728, mean 0.00282097, stddev 2.63324, skewness 0.0422728, kurtosis 23.5422 ) 
  f_bias_corr_   ( min -27.8925, max 59.1202, mean 0.139137, stddev 4.46984, skewness 1.72445, kurtosis 28.1057 ) 
  f_peephole_i_c_corr_   ( min -71.5136, max 38.2918, mean -0.084581, stddev 7.17593, skewness -2.69113, kurtosis 38.7275 ) 
  f_peephole_f_c_corr_   ( min -58.4775, max 250, mean 1.05325, stddev 20.652, skewness 7.41047, kurtosis 81.3128 ) 
  f_peephole_o_c_corr_   ( min -250, max 250, mean 1.31031, stddev 27.6808, skewness 1.31719, kurtosis 46.7816 ) 
  f_w_r_m_corr_   ( min -48.6547, max 48.9138, mean -0.00907291, stddev 4.80848, skewness -0.0271169, kurtosis 7.22013 ) 
  ---
  b_w_gifo_x_corr_   ( min -60.5285, max 62.6184, mean 0.122389, stddev 3.71971, skewness -0.241709, kurtosis 26.9268 ) 
  b_w_gifo_r_corr_   ( min -46.8067, max 45.6088, mean -0.00121528, stddev 2.75413, skewness 0.0431485, kurtosis 12.8358 ) 
  b_bias_corr_   ( min -38.218, max 57.9641, mean -0.244754, stddev 6.09122, skewness 0.826446, kurtosis 14.8404 ) 
  b_peephole_i_c_corr_   ( min -32.9117, max 83.7897, mean 0.472639, stddev 6.47975, skewness 6.32171, kurtosis 86.7361 ) 
  b_peephole_f_c_corr_   ( min -81.1489, max 67.5438, mean -0.338823, stddev 11.5667, skewness -0.129847, kurtosis 14.2587 ) 
  b_peephole_o_c_corr_   ( min -50.8227, max 54.2285, mean 0.0215319, stddev 12.7614, skewness -0.0641219, kurtosis 4.73315 ) 
  b_w_r_m_corr_   ( min -24.2481, max 26.7594, mean -0.0142417, stddev 4.47859, skewness 0.0133968, kurtosis 0.886253 ) 

  ### Activations (mostly after non-linearities)
  YI_FW(0..1)^   ( min 0, max 1, mean 0.441079, stddev 0.335534, skewness 0.315555, kurtosis -1.27499 ) 
  YF_FW(0..1)^   ( min 0, max 1, mean 0.628107, stddev 0.317531, skewness -0.480338, kurtosis -1.04817 ) 
  YO_FW(0..1)^   ( min 0, max 1, mean 0.368101, stddev 0.345789, skewness 0.630261, kurtosis -1.09757 ) 
  YG_FW(-1..1)   ( min -1, max 1, mean 0.0272771, stddev 0.871758, skewness -0.0547784, kurtosis -1.80762 ) 
  YC_FW(-R..R)*  ( min -50, max 50, mean 0.457049, stddev 13.152, skewness 0.296846, kurtosis 9.4693 ) 
  YH_FW(-1..1)   ( min -1, max 1, mean 0.0358067, stddev 0.680254, skewness -0.066531, kurtosis -1.2315 ) 
  YM_FW(-1..1)   ( min -1, max 1, mean -0.00196065, stddev 0.328439, skewness -0.140631, kurtosis 2.6722 ) 
  YR_FW(-R..R)   ( min -3.98256, max 4.44516, mean 0.0157991, stddev 0.711901, skewness 0.0797517, kurtosis 1.11951 ) 
  ---
  YI_BW(0..1)^   ( min 0, max 1, mean 0.473139, stddev 0.314704, skewness 0.218087, kurtosis -1.2749 ) 
  YF_BW(0..1)^   ( min 0, max 1, mean 0.654981, stddev 0.259251, skewness -0.5152, kurtosis -0.5491 ) 
  YO_BW(0..1)^   ( min 0, max 1, mean 0.390799, stddev 0.345165, skewness 0.534904, kurtosis -1.20473 ) 
  YG_BW(-1..1)   ( min -1, max 1, mean 0.00842188, stddev 0.843963, skewness -0.0133292, kurtosis -1.76067 ) 
  YC_BW(-R..R)*  ( min -50, max 50, mean 1.08009, stddev 11.166, skewness 1.00711, kurtosis 12.58 ) 
  YH_BW(-1..1)   ( min -1, max 1, mean 0.0396516, stddev 0.700172, skewness -0.0506441, kurtosis -1.32828 ) 
  YM_BW(-1..1)   ( min -0.99997, max 0.999999, mean 0.00198975, stddev 0.349382, skewness -0.0188121, kurtosis 1.90448 ) 
  YR_BW(-R..R)   ( min -4.32684, max 4.39163, mean -0.0162453, stddev 0.78757, skewness -0.0334579, kurtosis 1.48524 ) 

  ### Derivatives (w.r.t. inputs of non-linearities)
  DI_FW^  ( min -1, max 1, mean 6.58033e-05, stddev 0.0344341, skewness -0.0331009, kurtosis 197.509 ) 
  DF_FW^  ( min -1, max 1, mean 2.78284e-05, stddev 0.0248375, skewness 0.162708, kurtosis 341.318 ) 
  DO_FW^  ( min -1, max 1, mean 3.2315e-05, stddev 0.0546177, skewness 0.0876407, kurtosis 105.382 ) 
  DG_FW   ( min -1, max 1, mean 1.60288e-05, stddev 0.0430358, skewness 0.0727577, kurtosis 236.251 ) 
  DC_FW*  ( min -16.255, max 13.2274, mean 0.000759651, stddev 0.321627, skewness -1.44363, kurtosis 186.355 ) 
  DH_FW   ( min -11.6664, max 20.0115, mean 0.000122772, stddev 0.243946, skewness 0.362251, kurtosis 280.357 ) 
  DM_FW   ( min -19.5263, max 21.7626, mean -3.21232e-05, stddev 0.704647, skewness 0.0579445, kurtosis 53.309 ) 
  DR_FW   ( min -4.13434, max 3.25366, mean 0.000198046, stddev 0.152026, skewness -0.0271741, kurtosis 30.0635 ) 
  ---
  DI_BW^  ( min -1, max 1, mean -0.000113844, stddev 0.0197653, skewness 0.796599, kurtosis 232.433 ) 
  DF_BW^  ( min -1, max 1, mean -0.000134751, stddev 0.0159442, skewness 0.78696, kurtosis 343.297 ) 
  DO_BW^  ( min -0.618982, max 0.570561, mean -0.000105923, stddev 0.0211308, skewness -0.0959668, kurtosis 32.6009 ) 
  DG_BW   ( min -1, max 1, mean 0.000104767, stddev 0.0359046, skewness -0.540897, kurtosis 190.132 ) 
  DC_BW*  ( min -4.99463, max 15.6942, mean 0.000462857, stddev 0.182134, skewness 5.30686, kurtosis 409.573 ) 
  DH_BW   ( min -2.43087, max 2.02033, mean 0.00043282, stddev 0.0878224, skewness 0.0450263, kurtosis 32.9342 ) 
  DM_BW   ( min -2.51148, max 3.42656, mean 0.00259419, stddev 0.23727, skewness 0.0163644, kurtosis 5.2992 ) 
  DR_BW   ( min -1.02784, max 0.970676, mean -2.78639e-05, stddev 0.0884638, skewness -0.0128188, kurtosis 3.61053 ) 
Component 2 : <Tanh>, 
Component 3 : <AffineTransform>, 
  linearity_grad ( min -169.073, max 180.107, mean -3.22917e-08, stddev 2.625, skewness 1.14119, kurtosis 637.226 ) , lr-coef 1, max-norm 0
  bias_grad ( min -299.991, max 412.228, mean 2.86102e-07, stddev 14.931, skewness 9.82492, kurtosis 579.904 ) , lr-coef 1
Component 4 : <Softmax>, 
### END GRADIENT

LOG (ali-to-pdf[5.5.1074~1-71f3]:main():ali-to-pdf.cc:68) Converted 2919 alignments to pdf sequences.
LOG (ali-to-post[5.5.1074~1-71f3]:main():ali-to-post.cc:73) Converted 2919 alignments.
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:168) MC05_98, missing targets
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:334) ### After 755062 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:335) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.408399, max 0.419209, mean 0.00376648, stddev 0.0769677, skewness 0.0150702, kurtosis 0.0456359 ) 
  f_w_gifo_r_   ( min -0.445386, max 0.400828, mean -0.000575929, stddev 0.0766374, skewness 0.000994921, kurtosis -0.00418234 ) 
  f_bias_   ( min -0.351857, max 1.32066, mean 0.215869, stddev 0.456511, skewness 1.07022, kurtosis -0.655579 ) 
  f_peephole_i_c_   ( min -0.389615, max 0.413185, mean -0.00394479, stddev 0.12002, skewness 0.0245806, kurtosis 0.741832 ) 
  f_peephole_f_c_   ( min -0.692214, max 0.710466, mean 0.00228086, stddev 0.157115, skewness 0.0824935, kurtosis 4.18139 ) 
  f_peephole_o_c_   ( min -0.529181, max 0.458228, mean -0.0111839, stddev 0.167957, skewness 0.240416, kurtosis -0.0548913 ) 
  f_w_r_m_   ( min -0.53702, max 0.471381, mean 0.000575746, stddev 0.0976943, skewness -0.00156689, kurtosis -0.0299637 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -0.5708, max 0.581206, mean 0.00620702, stddev 0.0802532, skewness -0.0292889, kurtosis 0.937816 ) 
  b_w_gifo_r_   ( min -0.345889, max 0.304109, mean -0.00024402, stddev 0.0680733, skewness 0.00161323, kurtosis -0.403567 ) 
  b_bias_   ( min -0.298454, max 1.16775, mean 0.210175, stddev 0.448265, skewness 1.06132, kurtosis -0.683011 ) 
  b_peephole_i_c_   ( min -0.369135, max 0.261902, mean 0.00537656, stddev 0.0887678, skewness -0.172002, kurtosis 1.10869 ) 
  b_peephole_f_c_   ( min -0.543131, max 0.596355, mean 0.0111786, stddev 0.143537, skewness 0.474962, kurtosis 3.56313 ) 
  b_peephole_o_c_   ( min -0.538749, max 0.384202, mean -0.0169933, stddev 0.169263, skewness -0.184098, kurtosis 0.157635 ) 
  b_w_r_m_   ( min -0.360523, max 0.35442, mean -0.000146544, stddev 0.0849152, skewness -0.00220094, kurtosis -0.131654 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.855551, max 0.712344, mean -0.000155919, stddev 0.104017, skewness 0.00653466, kurtosis 0.0661545 ) , lr-coef 1, max-norm 0
  bias ( min -0.0701939, max 2.08181, mean 7.45058e-10, stddev 0.0669258, skewness 24.2658, kurtosis 733.809 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:336) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -14.8312, max 13.8231, mean 0.00769689, stddev 0.940803, skewness 0.731578, kurtosis 12.6034 ) 
[1] output of <BlstmProjected> ( min -4.16277, max 3.91109, mean -0.000620369, stddev 0.652948, skewness -0.0130639, kurtosis 3.1053 ) 
[2] output of <Tanh> ( min -0.999516, max 0.999199, mean 6.0349e-05, stddev 0.431705, skewness -0.00617365, kurtosis 0.121511 ) 
[3] output of <AffineTransform> ( min -12.5975, max 19.8631, mean 0.00584544, stddev 1.88672, skewness 0.916551, kurtosis 5.41644 ) 
[4] output of <Softmax> ( min 4.56903e-12, max 0.99865, mean 0.000780998, stddev 0.0152334, skewness 41.8133, kurtosis 1994.98 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:338) 
### BACKWARD PROPAGATION BUFFER CONTENT :
[0] diff of <Input>  ( min -2.53172, max 2.43595, mean 0.0178603, stddev 0.224277, skewness 1.08288, kurtosis 12.1514 ) 
[1] diff-output of <BlstmProjected> ( min -0.457564, max 0.427129, mean 0.000147052, stddev 0.0471203, skewness -0.00248561, kurtosis 7.14779 ) 
[2] diff-output of <Tanh> ( min -0.45885, max 0.506067, mean 0.000202068, stddev 0.0617339, skewness 0.0187068, kurtosis 4.46046 ) 
[3] diff-output of <AffineTransform> ( min -0.999505, max 0.960773, mean -1.58236e-08, stddev 0.0180135, skewness -28.5849, kurtosis 1828.6 ) 
[4] diff-output of <Softmax> ( min -0.999505, max 0.960773, mean -1.58236e-08, stddev 0.0180135, skewness -28.5849, kurtosis 1828.6 ) 
### END BACKWARD


LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:339) 
### GRADIENT STATS :
Component 1 : <BlstmProjected>, ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  ### Gradients 
  f_w_gifo_x_corr_   ( min -35.4057, max 46.7022, mean 0.097408, stddev 3.71319, skewness 0.187549, kurtosis 7.57993 ) 
  f_w_gifo_r_corr_   ( min -44.5971, max 35.1634, mean -0.00710349, stddev 2.70801, skewness -0.0751362, kurtosis 6.43732 ) 
  f_bias_corr_   ( min -31.0528, max 23.9915, mean -0.484093, stddev 4.76075, skewness -0.30361, kurtosis 3.94996 ) 
  f_peephole_i_c_corr_   ( min -21.9304, max 31.0581, mean 0.550266, stddev 5.01119, skewness 1.41488, kurtosis 9.77643 ) 
  f_peephole_f_c_corr_   ( min -82.6755, max 56.2267, mean -0.0106281, stddev 11.6124, skewness -0.652436, kurtosis 12.009 ) 
  f_peephole_o_c_corr_   ( min -132.476, max 77.1171, mean -1.59372, stddev 17.4281, skewness -2.07966, kurtosis 17.36 ) 
  f_w_r_m_corr_   ( min -44.1979, max 40.8438, mean -0.00426871, stddev 4.71884, skewness 0.0402065, kurtosis 3.01144 ) 
  ---
  b_w_gifo_x_corr_   ( min -81.8252, max 126.518, mean 0.253971, stddev 5.47615, skewness -0.181546, kurtosis 28.0387 ) 
  b_w_gifo_r_corr_   ( min -46.0418, max 49.6562, mean -0.00236203, stddev 3.412, skewness 0.0611624, kurtosis 7.98395 ) 
  b_bias_corr_   ( min -53.6725, max 85.7771, mean -1.10334, stddev 8.62389, skewness 1.12444, kurtosis 15.3392 ) 
  b_peephole_i_c_corr_   ( min -55.9639, max 17.2785, mean -0.441605, stddev 6.01836, skewness -2.81934, kurtosis 23.6797 ) 
  b_peephole_f_c_corr_   ( min -71.4942, max 127.17, mean 0.0768262, stddev 15.1798, skewness 2.12405, kurtosis 19.3478 ) 
  b_peephole_o_c_corr_   ( min -83.2182, max 82.9734, mean -0.619856, stddev 16.8655, skewness -0.263741, kurtosis 5.48055 ) 
  b_w_r_m_corr_   ( min -50.9539, max 48.203, mean 0.047102, stddev 5.52571, skewness -0.0252689, kurtosis 1.75332 ) 

  ### Activations (mostly after non-linearities)
  YI_FW(0..1)^   ( min 0, max 1, mean 0.328473, stddev 0.350035, skewness 0.705146, kurtosis -0.967869 ) 
  YF_FW(0..1)^   ( min 0, max 1, mean 0.470285, stddev 0.386162, skewness 0.0151835, kurtosis -1.57972 ) 
  YO_FW(0..1)^   ( min 0, max 1, mean 0.274508, stddev 0.340073, skewness 1.03341, kurtosis -0.431687 ) 
  YG_FW(-1..1)   ( min -1, max 1, mean 0.0164359, stddev 0.751462, skewness -0.0277166, kurtosis -1.40014 ) 
  YC_FW(-R..R)*  ( min -50, max 50, mean 0.262774, stddev 10.2722, skewness 0.300991, kurtosis 16.7731 ) 
  YH_FW(-1..1)   ( min -1, max 1, mean 0.0241708, stddev 0.586816, skewness -0.0285155, kurtosis -0.634614 ) 
  YM_FW(-1..1)   ( min -1, max 1, mean 0.00214906, stddev 0.281487, skewness -0.0980277, kurtosis 4.69012 ) 
  YR_FW(-R..R)   ( min -3.39434, max 3.91109, mean 0.0105587, stddev 0.626786, skewness 0.121248, kurtosis 2.96681 ) 
  ---
  YI_BW(0..1)^   ( min 0, max 1, mean 0.343051, stddev 0.344871, skewness 0.610735, kurtosis -1.07802 ) 
  YF_BW(0..1)^   ( min 0, max 1, mean 0.481287, stddev 0.366392, skewness -0.103014, kurtosis -1.46086 ) 
  YO_BW(0..1)^   ( min 0, max 1, mean 0.285858, stddev 0.345235, skewness 0.954781, kurtosis -0.60683 ) 
  YG_BW(-1..1)   ( min -1, max 1, mean 0.00814367, stddev 0.738903, skewness -0.0123347, kurtosis -1.3695 ) 
  YC_BW(-R..R)*  ( min -50, max 50, mean 0.684293, stddev 9.11073, skewness 1.17184, kurtosis 19.0002 ) 
  YH_BW(-1..1)   ( min -1, max 1, mean 0.0291105, stddev 0.596736, skewness -0.0151647, kurtosis -0.706976 ) 
  YM_BW(-1..1)   ( min -0.999993, max 0.999996, mean 0.00335852, stddev 0.295009, skewness -0.00938844, kurtosis 3.90525 ) 
  YR_BW(-R..R)   ( min -4.16277, max 3.85351, mean -0.0117921, stddev 0.674125, skewness -0.114348, kurtosis 3.20706 ) 

  ### Derivatives (w.r.t. inputs of non-linearities)
  DI_FW^  ( min -1, max 1, mean -0.000110836, stddev 0.0308216, skewness -0.39291, kurtosis 226.905 ) 
  DF_FW^  ( min -0.958079, max 1, mean -0.000111837, stddev 0.0218734, skewness 0.0505975, kurtosis 325.747 ) 
  DO_FW^  ( min -1, max 1, mean -0.000138032, stddev 0.0560863, skewness -0.490103, kurtosis 113.911 ) 
  DG_FW   ( min -1, max 1, mean 0.00012936, stddev 0.0418971, skewness 1.23487, kurtosis 259.386 ) 
  DC_FW*  ( min -7.20973, max 10.4575, mean 0.00184886, stddev 0.289732, skewness 1.20732, kurtosis 127.103 ) 
  DH_FW   ( min -10.4669, max 9.04445, mean 0.000584809, stddev 0.23498, skewness 1.00967, kurtosis 215.852 ) 
  DM_FW   ( min -12.2314, max 14.0361, mean 0.00085239, stddev 0.729954, skewness -0.0353681, kurtosis 42.8486 ) 
  DR_FW   ( min -2.0245, max 2.37423, mean 0.000123802, stddev 0.149952, skewness 0.270207, kurtosis 29.1781 ) 
  ---
  DI_BW^  ( min -1, max 1, mean 0.000481193, stddev 0.0226509, skewness 4.82669, kurtosis 544.732 ) 
  DF_BW^  ( min -1, max 1, mean 9.37013e-05, stddev 0.0156404, skewness -0.818614, kurtosis 583.103 ) 
  DO_BW^  ( min -0.472941, max 0.5057, mean 0.000693782, stddev 0.0204191, skewness 0.815849, kurtosis 65.6969 ) 
  DG_BW   ( min -1, max 1, mean 4.79929e-05, stddev 0.0321555, skewness 1.28473, kurtosis 245.019 ) 
  DC_BW*  ( min -5.75964, max 23.5401, mean 0.00496182, stddev 0.348681, skewness 38.0179, kurtosis 2272.11 ) 
  DH_BW   ( min -1.70853, max 2.23331, mean 9.0403e-05, stddev 0.0946252, skewness 0.483784, kurtosis 43.8306 ) 
  DM_BW   ( min -2.84459, max 2.64258, mean 0.000274804, stddev 0.26555, skewness 0.123941, kurtosis 8.05929 ) 
  DR_BW   ( min -0.863043, max 0.874359, mean 0.00012368, stddev 0.0857524, skewness -0.0258217, kurtosis 5.39355 ) 
Component 2 : <Tanh>, 
Component 3 : <AffineTransform>, 
  linearity_grad ( min -88.2689, max 113.995, mean -4.72567e-08, stddev 2.23182, skewness 0.224796, kurtosis 130.118 ) , lr-coef 1, max-norm 0
  bias_grad ( min -140.932, max 211.29, mean 2.38419e-08, stddev 8.9655, skewness 4.26331, kurtosis 325.396 ) , lr-coef 1
Component 4 : <Softmax>, 
### END GRADIENT

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:346) PER-CLASS PERFORMANCE:
@@@ Frames per-class: [ 181212 144 127 606 532 43 173 190 515 703 138 211 211 608 45 289 271 572 372 153 368 892 307 454 467 612 374 177 314 165 203 169 193 120 162 350 506 100 142 40889 89709 154 463 1777 286 231 687 352 545 459 79 394 481 237 200 663 1037 475 2210 428 257 167 4192 1910 93 230 151 223 4363 199 339 612 931 689 168 53 430 425 404 147 72 468 398 184 543 242 103 448 1376 60 779 717 668 146 541 163 358 830 561 408 249 268 103 1105 608 477 208 629 257 136 769 185 359 2082 165 69 303 74 247 336 172 147 117 128 443 612 117 125 52 316 956 165 305 574 202 297 222 1167 208 209 172 556 182 606 267 121 534 323 470 239 776 321 812 179 268 1139 537 191 554 5133 217 193 202 128 264 242 282 141 453 190 390 248 274 208 168 117 250 72 227 832 107 208 74 120 577 200 74 173 93 435 313 308 638 462 201 239 312 172 59 151 330 221 210 68 77 424 248 789 348 126 86 566 208 142 331 485 286 372 234 328 637 273 293 132 41 150 57 102 173 655 328 426 861 178 245 87 533 72 131 265 150 790 182 357 233 69 187 112 206 273 85 62 302 205 411 179 278 260 193 525 723 141 288 139 513 788 215 97 150 236 273 455 166 615 208 63 196 43 120 342 911 206 87 1156 192 146 131 166 31 221 252 417 373 499 401 233 179 409 182 439 579 96 189 251 1812 272 55 134 245 227 120 170 454 69 211 126 232 303 701 89 59 87 90 418 122 334 282 91 119 125 379 59 189 157 163 620 120 203 224 199 71 381 86 544 80 247 220 94 114 76 256 175 1186 420 378 336 135 140 331 666 306 356 196 203 291 152 221 543 214 149 360 365 670 264 264 692 216 616 923 524 408 234 395 301 375 105 525 80 359 98 456 204 309 258 288 286 181 90 101 263 53 253 167 354 150 174 194 393 579 494 437 273 142 236 159 226 115 606 198 255 508 136 282 304 327 234 1467 199 113 213 747 267 237 238 267 223 423 127 334 351 773 400 668 269 762 507 73 172 259 944 420 60 318 817 125 150 155 430 99 155 360 316 425 603 146 124 301 210 170 102 140 531 288 716 282 161 233 181 349 120 133 242 135 51 141 111 147 205 551 427 131 99 109 133 445 7274 204 271 135 128 125 377 120 372 449 808 108 557 45 280 47 596 57 276 400 121 347 315 193 77 146 96 105 216 193 341 1125 236 219 105 417 109 338 639 130 396 177 211 138 121 275 91 104 194 51 542 461 346 197 315 121 109 444 210 64 135 84 52 96 283 268 315 144 187 498 569 470 194 79 151 71 285 409 404 165 215 321 466 69 209 585 117 83 141 150 364 478 225 439 81 192 156 773 344 306 243 112 146 203 501 258 143 117 231 316 156 83 53 468 163 539 465 176 116 79 157 350 139 86 253 155 101 132 342 440 324 189 390 153 111 192 365 208 291 263 103 139 134 307 402 452 451 5943 88 406 111 189 488 786 548 86 632 99 382 216 141 365 388 65 90 187 430 300 463 604 247 79 314 305 463 45 473 718 330 223 226 1312 201 160 62 170 227 141 90 90 193 68 539 610 241 156 182 257 224 114 99 209 178 32 193 174 235 238 150 123 308 261 195 275 142 111 371 821 518 831 306 124 432 87 357 932 174 327 225 300 36 190 272 120 358 461 397 462 306 105 348 166 390 216 520 219 260 71 179 258 429 201 53 142 256 406 60 128 173 26 88 191 83 60 155 179 1000 125 44 191 230 383 687 216 257 321 233 417 202 1061 342 327 163 248 165 412 229 303 517 124 815 539 79 306 155 255 89 321 245 73 308 228 196 192 269 176 228 101 8591 469 337 966 237 430 177 258 334 85 304 414 487 401 57 178 99 260 178 151 248 261 926 126 248 230 264 241 147 220 155 139 646 219 209 393 1097 85 160 936 605 267 175 213 208 155 166 207 194 260 180 542 294 745 650 41 235 409 387 268 281 172 798 467 256 138 171 316 378 542 197 77 199 253 693 506 326 151 543 272 142 171 242 241 191 161 274 304 47 226 320 127 248 43 388 236 233 244 332 111 261 196 246 68 466 707 315 146 315 282 30 427 201 153 510 1026 278 186 316 45 464 106 316 404 415 543 346 257 452 1347 138 153 366 558 585 171 109 855 530 221 155 160 161 111 196 131 341 863 116 777 103 428 210 393 95 260 221 295 129 320 516 185 97 126 343 492 405 270 645 150 141 159 382 1153 426 252 535 332 205 417 575 597 292 372 180 236 213 343 301 154 349 528 290 243 378 616 213 266 230 676 309 338 165 324 314 639 310 148 365 184 119 116 199 264 1367 367 467 1442 213 598 641 335 156 122 351 114 129 98 489 213 162 386 116 74 274 197 265 173 129 167 753 211 214 1711 286 746 207 83 123 191 204 353 783 57 223 151 323 285 99 1197 111 116 189 130 401 488 428 96 280 248 468 506 117 284 315 328 129 334 124 557 265 243 383 268 66 97 474 153 221 139 391 194 634 156 322 748 309 195 326 651 462 370 172 385 490 392 610 304 249 70 501 324 335 228 370 206 587 560 108 850 256 209 153 168 206 131 204 232 340 200 284 174 301 124 107 220 67 370 1390 1253 311 434 127 136 104 493 416 778 103 335 338 242 66 208 459 231 96 291 160 220 368 145 728 458 351 255 350 348 137 125 1039 1466 285 221 232 361 605 549 4192 543 376 361 3105 243 319 117 225 232 542 309 859 113 244 913 235 271 853 881 252 243 274 129 90 371 274 176 453 107 172 140 172 411 267 778 147 558 2509 132 95 258 162 203 87 21 467 192 231 265 665 552 160 225 162 279 3235 297 303 235 317 297 155 692 494 82 179 192 734 178 103 204 87 225 63 351 225 260 282 352 367 219 77 182 384 386 147 340 273 260 234 61 251 141 663 194 320 94 144 ]
@@@ Loss per-class: [ 0.44193 1.92497 2.24809 1.52439 1.34217 5.093 2.84783 1.7549 1.86229 2.48813 1.84059 2.92313 2.81737 1.9421 6.91954 1.51134 1.71225 2.49611 2.50363 3.17267 1.43811 1.6359 2.07325 1.21604 1.37291 1.85378 1.45022 3.41723 1.8039 3.13222 1.90068 1.77565 2.21834 3.50991 1.04478 2.54338 2.46236 3.02109 3.34284 1.84057 0.792999 2.11681 1.8987 1.55135 2.56362 1.63415 0.928725 2.4261 2.68166 2.22371 3.66227 1.88748 2.00849 2.65836 3.10407 1.89742 1.94597 1.13511 1.30955 1.67117 2.71704 1.61666 0.683553 0.496411 3.56683 2.16517 3.15666 2.25379 1.58258 1.61986 1.16995 1.27781 1.51228 1.87191 2.27796 4.53689 0.881238 0.590263 1.48848 2.57025 3.96208 2.15679 1.186 1.64035 2.22835 2.78405 3.68194 2.09302 1.84508 6.02391 1.70444 1.75456 1.39954 3.47218 1.9794 2.93073 1.67301 0.873525 1.12592 2.10459 2.94035 2.1407 3.22117 1.43551 1.1058 1.65228 2.56234 2.65252 2.26285 2.13819 2.65374 3.31452 3.16509 1.09562 3.41466 5.77223 2.23575 4.10547 3.70545 2.21677 2.87467 4.89727 3.13541 3.1541 1.61132 1.00872 4.56365 3.5649 5.81593 1.42303 0.881887 2.83794 1.21548 1.44043 2.86729 1.57312 1.81585 0.748863 2.14986 2.50049 2.56091 1.20099 1.97162 1.35798 2.20301 3.4148 2.16226 3.11182 1.71427 1.0272 0.581615 2.12938 2.85215 3.39414 2.13263 1.0569 0.692979 1.70858 1.18145 0.774174 1.96449 2.41857 3.22389 5.92667 1.77526 2.0862 1.51796 2.56656 1.18042 2.74943 1.024 2.75559 2.46221 2.65346 3.70729 1.61564 1.56439 3.25273 3.4002 0.739725 2.82209 2.92199 3.36483 2.67386 0.941302 2.20714 6.96008 2.4083 3.5604 1.68085 1.671 1.52244 0.80736 1.38823 2.32357 1.92529 2.25924 2.83232 3.83664 1.82321 2.99821 2.78219 2.67224 4.08671 3.98094 1.67502 2.79871 1.48327 1.73701 2.73372 4.71856 1.9996 1.3933 12.8804 2.40065 1.54359 2.18233 1.41088 1.57521 2.58038 1.66982 1.57246 3.10044 2.6695 5.112 2.98983 5.31587 3.17119 2.23156 1.792 2.67941 1.42815 1.6703 3.4369 1.8035 4.8473 1.92397 4.5913 2.68998 2.18753 3.69415 0.940076 2.48801 1.77858 2.77188 4.86769 2.74661 5.75043 2.82818 2.9002 5.34227 2.7844 2.71817 2.24472 2.38603 1.55712 2.91462 2.74128 1.76822 1.73375 1.54629 2.79511 2.43424 3.09414 2.15202 1.45525 2.42656 3.8238 2.34867 2.61469 1.53816 2.24058 3.79978 2.50809 3.66743 5.10002 2.73201 6.30177 1.84864 2.55719 1.9367 3.20808 3.69707 1.50058 2.30119 2.60998 3.87451 2.23327 9.42587 3.34332 2.37981 2.93166 1.78052 2.12646 2.73733 2.53586 2.02193 1.7659 3.17133 3.1206 1.04767 3.0623 1.84572 1.35547 0.53523 1.8572 6.65438 2.97948 3.39801 3.54842 2.50898 2.38071 1.98753 3.46995 2.70292 3.69167 1.66341 2.32135 1.0651 3.67871 5.77788 2.66457 5.13794 0.932916 3.82241 3.17153 2.58732 5.99756 3.88227 3.32858 1.68958 4.2512 3.20466 2.54443 2.5224 1.23424 4.51392 2.73817 2.23091 3.05855 2.64133 1.43747 3.99179 1.55778 2.65299 2.53326 2.60408 3.62871 4.04961 3.73156 2.49836 2.38559 1.53984 0.682513 1.80495 1.87445 2.20614 2.85683 2.91264 0.884904 1.97332 3.26511 2.65614 3.7188 2.66637 3.20461 1.43924 2.04653 1.62608 3.47341 2.00224 2.56676 1.09082 2.57575 1.7963 1.62388 1.94128 1.79764 1.73666 2.17567 2.20057 2.05658 1.76186 2.56348 2.19075 3.44671 1.72273 3.30073 2.13492 3.50363 2.17126 2.8911 1.92347 1.78365 2.033 2.06991 3.1993 2.87654 3.14092 1.38459 6.74123 1.4463 2.56138 2.36295 3.7018 2.44132 2.12803 2.1455 1.8629 3.05402 4.11503 3.95379 3.41963 2.86163 2.90109 1.62289 2.89404 1.07898 3.51269 1.78566 1.46689 2.64531 1.71652 1.66367 1.53714 2.7544 0.725164 1.08069 4.20784 1.49944 1.90413 3.0469 2.74307 2.47513 1.9951 1.79995 2.78931 2.19458 1.83254 1.9663 0.862648 2.24784 1.12966 2.37977 1.84024 1.2296 5.46884 2.26888 2.4463 0.620226 1.62405 2.86587 2.13856 1.39585 3.00474 8.41615 1.49669 2.33119 4.00755 3.21598 1.24247 1.79581 2.91556 2.41353 3.53592 4.69433 3.1085 2.20348 2.71554 3.47904 1.68366 0.904381 2.4151 1.6626 2.06511 1.33264 1.38522 2.09405 2.28328 2.68139 6.20403 2.88292 2.53931 3.6969 2.91408 2.26121 3.30778 2.47193 1.02574 2.4619 2.62517 2.99171 4.58646 3.1258 1.47152 0.944658 1.38959 2.93449 3.78245 2.4973 2.64738 2.50509 2.81165 3.18343 1.71119 2.66142 5.40407 1.12536 6.45835 2.00442 6.53851 2.4632 3.70144 1.94224 1.76658 3.10363 2.74748 1.23805 3.8432 3.83009 4.46441 3.3404 2.57866 5.54411 3.44853 2.22015 1.45345 3.00835 2.31085 4.06434 3.03641 4.27719 2.00722 1.51728 3.90064 2.7141 2.0635 2.15486 2.59683 3.48713 2.4261 3.96157 4.09119 2.71333 6.21918 0.682206 1.23598 2.0269 1.65401 2.5155 3.42783 5.3072 1.88269 3.75108 3.25952 2.05227 4.5057 4.87122 3.35281 3.33223 4.86341 3.10385 3.51008 3.05411 1.13675 1.64907 1.22775 1.86369 3.81484 2.84077 4.58692 1.8792 1.97096 1.36316 3.23366 3.30851 1.75751 1.47906 4.61717 2.05407 2.48179 4.36202 4.12909 4.55301 2.44543 2.08425 2.46388 2.38234 3.07164 4.29534 3.12187 3.02019 1.98663 1.17707 2.86199 2.86392 4.86428 2.82773 3.51514 1.55596 3.18121 3.96445 4.09711 3.43626 2.25603 3.1745 4.23246 5.46906 1.10503 2.9898 1.69557 1.4982 2.35469 3.21172 4.79544 3.46397 3.59368 3.9524 2.65514 3.36852 2.69354 2.80492 2.9431 1.71519 2.22263 1.98687 2.47889 3.15022 2.58678 3.29609 4.09548 4.00084 3.14537 2.76592 1.56097 4.18899 2.58734 2.55193 2.38162 2.35879 1.82327 1.39659 1.30002 2.74763 1.65983 2.95447 3.01115 1.66709 1.45363 1.86613 7.53242 0.84175 5.72896 2.37189 3.27846 3.14844 2.85663 1.91871 8.33679 3.5436 2.62088 2.00391 1.49536 1.58402 1.14427 3.87159 4.56571 2.39593 2.57827 2.63239 8.8609 1.84113 1.58071 2.33789 3.90642 4.28862 2.03994 2.73093 3.35602 6.98909 3.14956 3.33654 2.46483 3.84863 5.18847 2.83936 5.30861 1.74494 1.44413 3.66872 3.21672 1.94102 3.16124 2.28555 4.59104 4.28047 2.88421 4.29352 5.68528 2.51787 2.59807 1.40172 2.33323 2.76783 3.0697 2.13724 2.08461 3.39764 1.5662 3.09324 5.94409 1.49 0.857173 2.01564 1.22137 2.06645 3.54752 2.45025 4.92312 4.04378 3.0117 3.68808 2.46835 4.14706 2.59004 8.10859 2.60268 3.22134 3.08972 1.11662 2.17237 2.27301 1.83525 3.31132 3.55011 2.45149 3.45682 1.83657 2.67683 1.90944 2.38201 2.38796 4.58407 2.86649 1.91456 2.19066 2.52682 7.38714 2.07376 2.1183 2.2487 4.86251 3.52342 2.21603 4.74235 3.52235 2.74748 6.06218 4.61165 2.39224 3.01474 1.61707 4.06108 6.38749 3.61929 3.54682 2.70827 1.43927 3.1879 2.08054 2.02716 2.19022 2.11869 2.02725 2.31369 2.30632 2.92206 4.99525 3.14083 3.31695 1.3621 2.36353 2.84951 1.892 4.96383 0.831837 2.00096 2.78253 2.64302 1.79088 1.96348 5.02203 2.39852 2.63486 5.40351 1.52495 2.65855 3.67674 2.86335 2.37954 3.52435 5.89558 4.71957 0.403967 1.72144 3.90286 0.965606 3.07964 2.43371 3.70491 1.69914 2.45305 6.52148 1.84608 2.55234 1.96629 2.51983 5.8665 3.71682 4.17002 1.13386 2.52737 2.5431 3.50432 2.60293 1.60875 4.34557 2.90584 2.95775 2.81081 2.61206 4.14539 3.54807 2.43804 2.92541 1.72507 2.52484 4.34971 3.11411 1.68693 4.60934 2.70452 1.16983 1.8763 1.66183 2.16547 2.76745 3.06005 3.45609 2.70933 3.16913 3.13926 3.76982 2.86202 2.09894 2.24453 1.53631 2.38696 6.50106 2.57606 2.60395 1.90132 2.75337 2.37412 3.78086 1.73036 1.85086 2.98489 5.01457 1.55524 2.57478 2.42247 1.76267 2.95317 5.00753 4.42017 2.01949 1.29952 1.87301 2.20441 2.26147 1.30155 1.62086 4.34414 3.25603 2.78295 2.78646 3.01452 1.65299 2.02318 1.74126 5.22205 2.18379 1.78148 3.68468 1.3361 6.15702 1.6486 2.4541 2.60376 3.22887 2.33583 3.14366 2.3504 3.20566 2.50395 5.18668 1.15357 1.35136 1.81977 4.79264 1.49139 1.67734 8.74218 2.23706 3.35092 2.86508 2.22085 2.37943 2.34003 4.11626 2.40896 7.31525 2.65674 3.00894 2.02286 1.76821 2.12015 2.67897 2.82332 2.80444 1.3633 1.88888 3.44712 4.51505 3.68048 2.69497 2.31855 3.25321 3.86057 1.72467 2.93191 2.58879 3.0923 2.4952 2.6021 4.53684 1.92369 3.82237 2.20888 1.51123 2.58346 1.80422 3.17571 1.79575 1.32213 1.80148 2.93024 2.959 2.25919 4.55256 4.11476 3.45039 2.50042 2.96898 3.72956 6.32617 2.96247 1.36464 1.67744 2.75453 1.74777 2.57052 3.92794 3.72571 3.11793 3.12915 2.57646 3.53424 2.27636 1.50149 3.21859 1.13582 1.49218 2.95691 2.32257 2.21912 2.63147 2.72619 3.17019 2.58374 2.34906 2.83039 2.47147 2.12693 1.52324 2.53917 1.72869 2.41014 2.12449 2.78887 2.73111 1.6752 1.96653 1.94027 2.63082 2.29028 1.9032 2.2558 3.09707 4.27516 2.52774 2.18414 2.11734 4.26523 2.973 2.55985 2.08518 3.30766 2.18528 0.733356 1.66525 2.62887 1.05515 2.28415 1.77079 3.59016 2.67133 3.20146 3.91307 4.54739 2.18845 2.63133 2.85704 2.20028 4.28868 4.57106 2.6254 3.48996 3.65002 3.73753 2.73971 2.85338 3.17874 2.07081 4.21335 0.986656 3.77522 1.304 4.10929 3.78115 4.88213 3.3012 2.73127 2.3444 0.908137 5.10568 1.96637 3.40947 2.36946 2.44619 4.73049 1.48753 4.26229 2.84022 2.39533 3.43217 1.5892 1.86593 2.05699 4.50276 1.98039 2.90542 2.25142 1.50506 2.96283 2.55688 2.87246 1.97533 3.65899 2.4515 3.99268 3.13493 3.55581 2.36071 1.15149 1.72238 4.44974 5.29929 1.83155 3.59011 3.44589 3.09106 1.86314 2.17588 1.95927 3.8381 2.00568 2.09536 2.08582 4.14098 2.03514 2.02829 2.87547 2.0432 2.72802 1.63314 2.48459 1.78404 1.31652 2.63886 4.33445 5.84374 2.67265 2.58462 3.51122 2.86982 1.98404 3.90804 1.8774 1.7371 3.3043 2.14355 3.06129 2.24852 2.21373 2.07405 2.75001 3.85891 2.71126 3.1106 2.08943 3.15132 1.93763 3.52881 2.01592 3.96832 5.36492 2.86121 3.85822 2.32296 0.717479 0.724022 1.37012 2.79095 4.46268 3.47561 3.55179 2.28126 2.46924 1.26401 3.10879 3.35529 2.84179 1.85785 7.92245 2.40159 1.79748 2.2358 3.38523 2.39333 3.02856 1.46832 3.27693 3.24694 1.66445 1.37535 1.63875 2.84758 3.058 1.38842 1.96397 2.19863 1.42678 1.15149 2.87848 2.95398 2.73249 2.00925 1.50026 1.60811 0.952816 2.48688 2.63057 1.70108 1.06986 2.66446 2.9416 3.07358 2.97004 3.05656 2.51578 3.0525 1.34796 3.88517 2.34034 1.63522 2.77819 2.40751 2.41776 1.40773 2.12068 3.9453 2.83123 2.34273 3.97907 3.13451 2.82885 3.41875 2.04224 4.22578 2.54066 3.99448 1.89774 2.16616 3.59099 2.30265 3.17183 1.71249 1.55238 3.39248 3.73748 3.37799 2.30544 3.33693 4.92315 11.5959 2.14354 1.95837 3.58272 3.50964 3.07954 3.6725 3.17685 3.55169 2.45222 3.46786 1.20469 2.1953 2.73445 2.09308 3.18081 3.91902 4.03249 1.85268 2.65027 4.97653 5.03449 3.06045 1.65811 2.69526 3.66557 2.94752 4.16347 2.40447 6.19038 2.3221 2.89702 3.04008 3.22693 2.10227 2.95197 1.50903 5.34628 1.54808 2.42689 1.85483 4.47136 2.23502 2.04506 3.58183 4.09662 4.43819 2.68789 4.10129 2.11143 2.14679 3.24136 2.88501 4.02509 ]
@@@ Frame-accuracy per-class: [ 83.3005 51.9031 33.7255 61.6653 68.9202 0 29.9712 45.6693 41.7071 30.4193 59.2058 33.0969 31.2057 37.4692 0 66.3212 54.1436 30.917 23.8926 11.7264 64.5862 48.8515 43.5772 71.9472 69.3048 42.449 67.8238 3.38028 52.1463 29.6073 38.3292 51.3274 44.4444 4.14938 75.6923 24.8217 27.0484 18.9055 26.6667 31.8052 68.5936 36.246 31.4995 52.5457 23.7347 59.1793 77.9636 32.9078 14.4821 34.6028 11.3208 53.9924 46.3136 21.4737 11.4713 51.5448 24.2892 65.8254 66.3198 53.909 31.8447 48.3582 80.644 85.475 4.27807 22.5597 15.8416 42.953 54.7038 54.1353 69.514 66.1224 56.2534 47.2806 35.6083 3.73832 75.9582 86.7215 60.8158 35.9322 4.13793 48.666 67.2522 56.9106 34.9586 12.7835 9.66184 51.9509 49.7639 0 47.0815 54.7735 64.7719 10.2389 49.6768 15.9021 58.2985 72.4865 67.854 37.4541 20.8417 47.2998 4.83092 60.787 73.788 46.911 41.7266 28.9118 23.6893 52.7473 18.0637 15.0943 16.1335 70.2041 19.3353 0 32.29 0 13.7374 50.5201 21.4493 0 20.4255 7.00389 57.4972 71.0204 0 11.1554 0 62.5592 74.8562 16.9184 65.1391 59.53 31.1111 63.5294 53.4831 80.6852 43.6451 36.7542 20.8696 64.69 49.3151 59.357 45.9813 12.3457 46.5856 20.711 50.797 69.7286 86.9285 37.6361 15.3846 4.45682 51.0242 71.7859 82.6047 59.0078 69.9729 76.8676 42.7586 35.6589 26.1728 0 55.1985 49.0722 67.6106 34.629 71.6648 17.3228 74.7759 35.4125 33.1512 32.1343 15.4303 50.2128 61.8762 17.931 18.022 78.5586 14.8837 27.8177 10.7383 32.3651 78.2684 33.4165 0 30.5476 9.62567 52.8129 64.7528 55.7536 81.5975 64.8649 39.7022 40.9186 34.88 31.8841 11.7647 60.066 17.8517 22.5734 23.753 1.45985 1.29032 61.2485 30.5835 67.5111 60.8321 22.9249 2.31214 41.8358 59.4724 0 42.5339 58.9083 34.555 67.1141 53.3049 38.3562 49.4118 55.5759 34.4123 29.434 0 15.9468 3.47826 25.3659 50.1441 48.2075 31.0502 60.9613 54.2078 15.1261 59.8778 0 43.1115 1.37931 11.4068 46.3277 21.2625 67.0462 38.3562 51.1888 28.2655 0 26.1333 0.888889 34.8668 35.1005 0 17.6 37.3554 41.8491 37.181 63.5097 26.2118 23.4165 51.1628 58.9914 62.8887 23.3216 44.0208 32.2581 32.3272 66.7089 40.3712 0 16.6113 45.2431 57.7697 31.6136 15.6156 29.5695 14.8681 0 32.0611 0 43.9834 26.2774 50.9051 18.4019 9.14286 64.2456 31.6883 38.2253 9.88593 28.2282 0 22.1219 36.0396 31.6168 54.083 46.2462 35.1183 32.5482 49.5822 55.6777 20.274 22.0705 75.4098 25.9067 50.1319 61.6302 89.269 55.7798 0 19.3309 11.4053 18.9011 31.5353 41.0557 43.7844 2.8777 27.896 13.4387 60.6452 40.8567 71.4184 7.82123 0 45.7143 0 75.7467 4.08163 18.2362 24.4248 0 10.0418 23.1076 54.2819 0 34.8285 23.4921 39.1437 69.9436 1.65975 39.312 40.5345 23.0576 27.972 64.4823 2.31214 56.0147 16.1491 29.899 27.6644 4.2328 8.73362 7.84314 29.2398 42.1652 53.8559 89.1795 59.181 54.0862 35.4244 43.4164 29.8643 78.4696 46.9821 24.6844 40.7125 11.7936 35.3345 16.3934 69.9774 56.1178 60.6061 18.0602 45.215 42.9549 71.5884 38.5633 42.344 59.639 60.5081 52.717 58.4732 38.1316 39.4125 37.5267 56.3843 31.5091 44.2077 7.58294 60.1332 4.96894 55.0765 23.3503 32.8587 23.9609 51.6963 42.94 29.1161 45.7243 22.0386 29.8343 25.6158 59.962 0 71.0059 38.209 40.3385 17.2757 45.2722 45.2442 44.9809 54.7023 21.4358 11.8857 18.2815 18.9474 27.907 33.8558 56.5121 19.9134 72.5474 9.06801 55.9687 63.9135 19.0476 58.7611 54.5156 64.7328 38.806 85.4514 79.198 7.04846 58.0796 47.7592 25.7944 36.6316 33.1237 49.3458 58.1655 12.0425 32.9412 52.0179 52.9161 79.3794 44.4444 66.8661 47.4954 54.9508 75.468 0 43.4783 43.1599 83.6421 60.1665 8.26446 48.9796 58.3486 33.4661 0 49.5177 33.6818 0 19.9357 68.516 55.9242 20.2115 32.3115 6.14334 2.40964 15.9204 39.4299 31.085 12.6829 56.2278 79.0216 33.6222 64.0614 43.5398 66.8731 68.5225 36.9146 26.6094 31.5353 0 32.5773 39.8524 1.94175 21.2014 46.6368 12.2034 43.7956 80.1451 38.5965 12.1673 19.0955 0 20.2247 59.7082 67.5648 70.4156 27.256 10.3321 20.2335 21.5139 26.4901 18.2573 27.651 62.5139 29.1899 0 73.722 0 47.0588 0 34.1995 1.73913 50.6329 57.4282 26.3374 26.7626 66.2441 5.68475 0 3.41297 31.0881 31.2796 0.461894 16.5375 24.5974 60.3287 20.7188 42.8246 26.5403 18.6826 1.82648 47.8582 55.5121 6.89655 46.1538 49.0141 43.4988 31.769 2.46914 27.2232 6.55738 4.78469 29.8201 0 81.106 65.6555 37.2294 70.3797 34.8653 12.3457 0 59.6175 3.80048 17.0543 43.5424 0 0 10.3627 22.2222 1.11732 21.2361 10.3806 15.4667 69.6088 57.4188 71.6259 51.928 6.28931 22.4422 11.1888 55.3415 38.5836 66.9963 14.5015 15.7773 51.3219 60.4502 1.43885 52.506 23.5696 2.55319 7.18563 3.53357 28.5714 48.011 14.629 31.4856 20.2503 3.68098 23.8961 26.1981 53.3937 71.1176 33.9315 23.8193 6.22222 32.0819 25.5528 57.8265 25.1451 6.27178 0 12.095 33.1754 15.3355 0 1.86916 73.6393 20.1835 64.1335 67.884 41.9263 21.4592 3.77358 18.4127 12.5535 9.319 20.8092 14.9901 21.2219 26.601 33.9623 56.9343 28.6039 47.1495 37.467 22.2791 23.4528 8.07175 8.31169 15.8687 25.4197 37.0497 53.8899 3.86473 38.7097 10.4089 39.0244 39.2547 46.1878 66.8882 54.4124 27.1186 52.3985 18.8341 27.9683 61.2078 60.0127 47.5843 0 77.4704 1.00503 46.7974 21.2471 8.48057 38.0301 38.3526 0 18.7845 20.8 48.0836 57.9035 56.3107 71.6294 4.44444 0 39.1097 40.5892 20.9277 0 51.1088 57.62 45.9909 6.26398 8.83002 41.9048 28.2878 14.9533 0 23.4604 14.0659 35.3357 9.94475 0 17.5711 0 57.0899 67.1581 2.89855 14.6965 46.5753 22.1359 36.0802 0.873362 2.01005 15.7518 6.16246 0 33.0749 36.1032 71.7622 25.9958 21.9269 12.1457 35.6564 53.5373 12.7877 58.4392 15.4386 0 54.6433 79.8539 41.4658 70.9561 54.8124 4.01606 38.6127 0 10.6294 19.4102 21.2034 41.8321 4.43459 19.9667 0 38.8451 16.1468 27.3859 70.2929 39.6533 39.2453 52.3243 22.5122 10.4265 37.5897 13.8138 54.5455 33.7182 47.8386 36.9021 39.5393 5.59441 33.9833 45.648 42.142 26.799 0 37.193 56.5302 43.5424 0 14.0078 38.6167 0 11.2994 14.6214 0 1.65289 28.9389 36.7688 60.5697 2.39044 0 4.17755 6.07375 37.2881 59.0545 23.5566 45.0485 44.79 43.6831 43.5928 36.0494 34.4795 45.5474 22.5954 1.22324 24.5473 13.8973 64.7273 42.2658 25.7002 60.2899 0.803213 81.5451 52.0853 30.1887 40.4568 50.8039 43.8356 4.46927 24.5723 32.5866 0 65.4781 33.698 14.2494 31.6883 32.6531 14.1643 1.31291 0 88.9251 47.4973 4.74074 80.3932 27.3684 40.6504 15.7746 52.9981 24.8132 0 53.8588 27.503 51.4872 37.3599 0 13.4454 0 66.4107 29.1317 28.3828 21.328 29.8279 55.3697 11.8577 29.7787 31.6703 35.1607 26.501 8.81356 15.4195 28.9389 25.0896 57.6953 25.0569 1.90931 22.1093 53.0296 0 23.053 69.8345 53.5095 45.6075 45.584 27.1663 29.7362 7.71704 32.4324 22.6506 23.6504 7.67754 27.1468 46.4516 38.7097 55.9356 32.1291 0 24.2038 34.9206 40.2581 19.7393 35.8792 9.85507 53.35 59.0374 24.1715 3.61011 55.9767 34.7551 26.1559 51.6129 15.6962 1.29032 16.0401 42.998 67.7722 49.9506 40.7351 28.3828 72.6771 57.6147 4.21053 15.1603 21.4433 27.3292 24.0209 52.6316 53.5519 50.2463 0 49.8896 61.1544 3.13725 66.3984 0 60.2317 25.37 32.9764 26.1759 46.015 12.5561 41.6826 17.8117 33.2657 0 73.3119 60.4947 56.1014 1.36519 59.588 53.0973 0 35.5556 16.3772 31.2704 30.3624 30.9791 43.088 8.0429 46.7615 0 30.5705 31.9249 52.4487 60.5686 41.8773 35.5106 29.7258 25.2427 65.4144 42.5232 7.22022 0.651466 8.18554 36.5264 38.2579 18.0758 7.30594 55.9906 31.2912 35.2144 16.7203 28.6604 29.7214 2.69058 45.2926 3.04183 31.918 64.6207 30.9013 50.418 14.4928 57.4096 70.3088 61.4994 26.178 30.7102 34.3115 0.338409 10.0386 15.6006 30.7841 30.1887 1.02564 0 20.3785 66.3959 50.3083 19.5933 58.7142 25.9136 15.5477 9.40439 27.1895 14.2176 29.3083 10.6931 35.1074 61.9549 36.0097 59.1617 58.384 21.7573 30.4274 30.3356 28.2548 27.0613 23.8876 16.885 37.4793 25.89 34.0486 35.3832 65.4045 39.8357 56.539 38.4428 47.3068 28.1426 33.4056 53.2151 48.4653 41.3589 32.6284 39.4453 51.8283 47.5371 25.1208 6.73401 32.8317 53.1165 38.4937 8.58369 26.5664 39.3195 38.1718 19.8639 52.8342 75.4939 55.2693 27.9031 75.1364 44.7094 42.1725 17.1429 41.2518 16.5939 6.17761 0 38.4065 29.5082 24 50.9702 0 1.34228 31.694 18.2278 11.6761 8.64553 20.8494 13.1343 22.1632 31.2057 6.99301 74.7298 9.42408 64.7019 10.1205 9.58084 1.61943 22.9765 34.7188 44.9788 75.0479 0 44.7427 16.5017 37.4034 35.3765 0 60.3758 4.4843 20.6009 34.8285 19.9234 59.2777 63.4596 39.4399 2.07254 59.893 27.3642 41.6222 58.8351 13.617 21.7926 30.7448 53.5769 7.72201 38.864 1.60643 22.6009 16.9492 47.6386 69.1004 40.9683 3.00752 5.12821 52.4763 8.46906 14.8984 10.0358 50.0639 52.9563 56.1072 5.7508 48.9922 43.0194 44.9111 8.69565 32.7718 41.2893 26.8108 52.3617 28.4058 54.2153 34.8624 57.5796 67.4857 37.4384 8.01603 0 30.9073 32.0493 21.1624 26.6958 47.7733 0 47.8298 47.2792 12.9032 44.0917 12.4756 37.2315 30.6189 45.1039 16.4649 12.1673 31.7848 16.7742 53.7445 19.9501 51.3181 15.4728 35.1575 7.22892 0 18.1406 13.3333 47.5034 82.2726 83.207 58.748 23.015 3.13725 8.05861 9.56938 38.5005 31.4526 71.1625 16.4251 11.3264 18.3161 51.5464 0 35.012 57.0185 42.7646 15.544 42.1955 22.4299 58.9569 26.5943 24.7423 58.6136 67.6118 52.6316 20.7436 26.2482 65.9971 44.3636 31.0757 64.8389 69.417 32.9247 7.22348 34.4086 47.5795 66.3914 60.7825 76.8277 27.9669 28.1541 55.6017 64.5307 34.0862 34.1158 36.5957 24.3902 6.02151 27.0968 11.9548 52.1233 1.76211 42.1268 54.5156 28.0255 34.9908 41.2419 66.2507 48.3168 14.7844 30.2368 36.2934 8.83978 31.2248 29.8725 11.3314 45.645 0 41.7391 14.9466 44.058 35.966 20.1869 40.7193 12.2034 57.4754 52.4806 12.8302 6.28272 15.087 38.7692 14.742 1.14286 0 44.0642 43.1169 15.1188 12.4294 19.2337 7.42081 14.9533 4.43459 51.6923 16.458 64.1323 48.7395 29.3245 55.6263 35.5906 13.4454 10.2894 57.1841 26.2892 0 3.89972 13.5065 56.7733 27.451 18.3575 22.9829 4.57143 35.0333 0 46.9417 40.7982 26.8714 12.3894 36.5957 38.3673 66.0592 1.29032 65.7534 38.7516 50.7115 4.74576 43.7592 39.4881 11.5163 3.83795 4.87805 29.4235 8.48057 54.4084 50.3856 25.585 24.3386 9.68858 ]

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:347) Done 2622 files, 1 with no tgt_mats, 0 with other errors. [TRAINING, 0.331527 min, fps37958.8]
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:353) AvgLoss: 1.5242 (Xent), [AvgXent: 1.5242, AvgTargetEnt: 0]
progress: []
FRAME_ACCURACY >> 57.1772% <<

