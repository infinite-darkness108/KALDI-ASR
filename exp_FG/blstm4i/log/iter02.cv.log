speech-HP-Z2-Tower-G9
nnet-train-multistream-perutt --cross-validate=true --randomize=false --verbose=0 --num-streams=10 --max-frames=15000 --feature-transform=exp_FG/blstm4i/final.feature_transform 'ark:copy-feats scp:exp_FG/blstm4i/cv.scp ark:- | apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_cv10/utt2spk scp:data-fbank/train_cv10/cmvn.scp ark:- ark:- | add-deltas --delta-order=2 ark:- ark:- |' 'ark:ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- |' exp_FG/blstm4i/nnet/nnet_iter02 
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:243) Not in compute-exclusive mode.  Suggestion: use 'nvidia-smi -c 3' to set compute exclusive mode
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:438) Selecting from 1 GPUs
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:453) cudaSetDevice(0): NVIDIA RTX A2000 12GB	free:11515M, used:515M, total:12031M, free/total:0.957153
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:501) Device: 0, mem_ratio: 0.957153
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:382) Trying to select device: 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:511) Success selecting device 0 free mem ratio: 0.957153
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:FinalizeActiveGpu():cu-device.cc:338) The active GPU is [0]: NVIDIA RTX A2000 12GB	free:11001M, used:1029M, total:12031M, free/total:0.91443 version 8.6
copy-feats scp:exp_FG/blstm4i/cv.scp ark:- 
apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_cv10/utt2spk scp:data-fbank/train_cv10/cmvn.scp ark:- ark:- 
add-deltas --delta-order=2 ark:- ark:- 
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:144) CROSS-VALIDATION STARTED
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Read():nnet/nnet-matrix-buffer.h:191) Read() started... Buffer size in MB: 0, max 3072, having 0 utterances.
ali-to-post ark:- ark:- 
ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl 'ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |' ark:- 
LOG (copy-feats[5.5.1074~1-71f3]:main():copy-feats.cc:143) Copied 296 feature matrices.
LOG (apply-cmvn[5.5.1074~1-71f3]:main():apply-cmvn.cc:159) Applied cepstral mean and variance normalization to 296 utterances, errors on 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:303) ### After 0 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:304) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.355673, max 0.336965, mean 0.00350909, stddev 0.0736455, skewness 0.0134472, kurtosis -0.132953 ) 
  f_w_gifo_r_   ( min -0.445196, max 0.40056, mean -0.000572445, stddev 0.0761557, skewness 0.00094158, kurtosis -0.0106089 ) 
  f_bias_   ( min -0.353702, max 1.28243, mean 0.218177, stddev 0.4533, skewness 1.07433, kurtosis -0.655898 ) 
  f_peephole_i_c_   ( min -0.3764, max 0.369109, mean -0.00579472, stddev 0.11422, skewness -0.0541228, kurtosis 0.69791 ) 
  f_peephole_f_c_   ( min -0.493632, max 0.789383, mean 0.00208594, stddev 0.141345, skewness 0.325635, kurtosis 4.38515 ) 
  f_peephole_o_c_   ( min -0.495588, max 0.423398, mean -0.0151365, stddev 0.159437, skewness 0.243382, kurtosis -0.177994 ) 
  f_w_r_m_   ( min -0.485306, max 0.481097, mean 0.000477928, stddev 0.0957113, skewness 0.000162791, kurtosis -0.0423553 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -0.475277, max 0.44022, mean 0.00600088, stddev 0.0751442, skewness -0.0226455, kurtosis 0.312667 ) 
  b_w_gifo_r_   ( min -0.339587, max 0.30893, mean -0.000269862, stddev 0.0668413, skewness 0.00193539, kurtosis -0.46802 ) 
  b_bias_   ( min -0.293218, max 1.16403, mean 0.211775, stddev 0.446312, skewness 1.06906, kurtosis -0.679125 ) 
  b_peephole_i_c_   ( min -0.341314, max 0.265661, mean 0.00596035, stddev 0.0855359, skewness -0.0249465, kurtosis 0.689641 ) 
  b_peephole_f_c_   ( min -0.420732, max 0.56097, mean 0.0115235, stddev 0.131142, skewness 0.627342, kurtosis 3.35085 ) 
  b_peephole_o_c_   ( min -0.524298, max 0.34616, mean -0.0175643, stddev 0.155377, skewness -0.215479, kurtosis 0.133178 ) 
  b_w_r_m_   ( min -0.358498, max 0.336653, mean -0.000147236, stddev 0.0816128, skewness -0.00302681, kurtosis -0.174974 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.778583, max 0.676899, mean -0.000155921, stddev 0.102461, skewness 0.00622605, kurtosis 0.0582926 ) , lr-coef 1, max-norm 0
  bias ( min -0.0638544, max 1.93261, mean -1.11759e-09, stddev 0.0611704, skewness 25.1519, kurtosis 778.218 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:305) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -11.004, max 12.2048, mean 0.00682255, stddev 0.969149, skewness 0.287893, kurtosis 3.83541 ) 
[1] output of <BlstmProjected> ( min -4.03156, max 4.25375, mean -0.00087773, stddev 0.723652, skewness 0.0361558, kurtosis 0.991023 ) 
[2] output of <Tanh> ( min -0.99937, max 0.999596, mean -0.00149691, stddev 0.503153, skewness 0.00398645, kurtosis -0.822493 ) 
[3] output of <AffineTransform> ( min -18.7152, max 19.37, mean 0.0126475, stddev 2.04515, skewness 0.576117, kurtosis 2.03141 ) 
[4] output of <Softmax> ( min 4.40973e-13, max 0.997158, mean 0.000779897, stddev 0.0129504, skewness 39.9296, kurtosis 1930.15 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:334) ### After 79212 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:335) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.355673, max 0.336965, mean 0.00350909, stddev 0.0736455, skewness 0.0134472, kurtosis -0.132953 ) 
  f_w_gifo_r_   ( min -0.445196, max 0.40056, mean -0.000572445, stddev 0.0761557, skewness 0.00094158, kurtosis -0.0106089 ) 
  f_bias_   ( min -0.353702, max 1.28243, mean 0.218177, stddev 0.4533, skewness 1.07433, kurtosis -0.655898 ) 
  f_peephole_i_c_   ( min -0.3764, max 0.369109, mean -0.00579472, stddev 0.11422, skewness -0.0541228, kurtosis 0.69791 ) 
  f_peephole_f_c_   ( min -0.493632, max 0.789383, mean 0.00208594, stddev 0.141345, skewness 0.325635, kurtosis 4.38515 ) 
  f_peephole_o_c_   ( min -0.495588, max 0.423398, mean -0.0151365, stddev 0.159437, skewness 0.243382, kurtosis -0.177994 ) 
  f_w_r_m_   ( min -0.485306, max 0.481097, mean 0.000477928, stddev 0.0957113, skewness 0.000162791, kurtosis -0.0423553 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -0.475277, max 0.44022, mean 0.00600088, stddev 0.0751442, skewness -0.0226455, kurtosis 0.312667 ) 
  b_w_gifo_r_   ( min -0.339587, max 0.30893, mean -0.000269862, stddev 0.0668413, skewness 0.00193539, kurtosis -0.46802 ) 
  b_bias_   ( min -0.293218, max 1.16403, mean 0.211775, stddev 0.446312, skewness 1.06906, kurtosis -0.679125 ) 
  b_peephole_i_c_   ( min -0.341314, max 0.265661, mean 0.00596035, stddev 0.0855359, skewness -0.0249465, kurtosis 0.689641 ) 
  b_peephole_f_c_   ( min -0.420732, max 0.56097, mean 0.0115235, stddev 0.131142, skewness 0.627342, kurtosis 3.35085 ) 
  b_peephole_o_c_   ( min -0.524298, max 0.34616, mean -0.0175643, stddev 0.155377, skewness -0.215479, kurtosis 0.133178 ) 
  b_w_r_m_   ( min -0.358498, max 0.336653, mean -0.000147236, stddev 0.0816128, skewness -0.00302681, kurtosis -0.174974 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.778583, max 0.676899, mean -0.000155921, stddev 0.102461, skewness 0.00622605, kurtosis 0.0582926 ) , lr-coef 1, max-norm 0
  bias ( min -0.0638544, max 1.93261, mean -1.11759e-09, stddev 0.0611704, skewness 25.1519, kurtosis 778.218 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:336) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -7.66434, max 7.26584, mean -0.0147177, stddev 0.961893, skewness 0.577064, kurtosis 2.62679 ) 
[1] output of <BlstmProjected> ( min -4.09937, max 4.19221, mean -0.00035445, stddev 0.709235, skewness -0.00668691, kurtosis 1.64644 ) 
[2] output of <Tanh> ( min -0.99945, max 0.999543, mean 0.00026664, stddev 0.482778, skewness -0.00933868, kurtosis -0.601742 ) 
[3] output of <AffineTransform> ( min -12.2573, max 17.2564, mean 0.0112383, stddev 1.98985, skewness 0.755873, kurtosis 3.32464 ) 
[4] output of <Softmax> ( min 4.89299e-12, max 0.991668, mean 0.000780802, stddev 0.0155844, skewness 36.226, kurtosis 1535.25 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:346) PER-CLASS PERFORMANCE:
@@@ Frames per-class: [ 1670 30 8 79 45 6 24 17 70 220 22 36 34 39 10 53 43 120 22 20 34 66 38 35 87 73 52 35 43 17 86 33 19 32 16 75 106 10 5 1873 21251 22 63 135 22 30 80 98 44 43 9 67 66 27 41 51 107 54 285 40 48 24 308 222 21 35 16 33 248 15 54 49 139 81 14 4 45 35 47 26 17 66 4 14 44 25 18 49 111 11 94 64 72 34 66 27 32 33 78 42 15 34 9 135 37 103 31 64 37 21 67 0 93 69 23 2 34 10 32 37 25 34 18 19 28 43 46 13 4 26 88 25 22 43 33 62 12 98 32 25 22 78 11 62 24 13 88 37 45 30 117 35 60 38 37 129 16 23 64 982 29 24 10 20 31 33 34 16 52 44 64 25 22 25 3 9 30 11 42 35 13 12 11 8 32 32 9 20 32 48 32 42 56 36 35 41 27 16 5 29 35 31 12 7 9 45 27 81 21 16 19 72 29 0 46 70 68 37 26 33 50 37 32 9 5 26 23 15 14 128 52 51 59 20 20 8 73 5 17 32 22 16 4 39 60 11 16 7 19 20 8 7 52 22 37 15 32 12 39 94 72 13 59 17 69 35 25 11 13 18 58 26 24 119 16 14 9 5 13 35 18 40 17 83 10 20 20 32 21 19 33 104 48 60 79 47 20 14 27 49 56 7 20 34 199 37 4 3 21 22 15 27 39 6 10 11 22 25 47 21 6 14 6 60 3 43 26 14 8 10 70 4 15 29 11 133 11 35 22 51 21 57 14 109 15 36 29 5 21 20 18 19 114 55 69 35 21 21 64 44 39 43 15 21 37 31 46 70 32 25 50 48 101 37 32 62 34 53 184 59 33 24 47 16 35 9 79 14 45 12 20 16 36 46 8 52 28 13 12 31 17 29 40 53 13 20 9 73 85 128 108 21 7 44 30 32 12 46 15 32 90 31 20 32 52 21 237 45 11 21 215 49 25 30 31 29 71 13 57 48 98 46 27 41 19 60 21 4 38 48 50 5 63 71 0 0 25 47 11 16 42 44 49 71 27 15 49 22 22 11 30 67 41 56 15 28 24 35 7 14 15 34 7 8 38 0 9 28 57 50 12 13 39 28 40 746 35 13 19 19 18 27 12 16 52 60 21 82 1 33 49 89 6 38 42 18 51 39 26 11 19 7 23 28 18 51 98 35 37 10 34 11 56 273 7 39 31 18 21 15 24 22 15 25 6 21 53 59 24 25 15 4 49 28 14 9 8 3 8 29 34 0 8 18 51 52 48 23 3 60 16 24 35 46 16 52 37 72 3 38 59 15 5 40 14 54 52 32 51 4 14 21 187 40 33 16 15 18 19 52 32 28 37 29 41 52 10 6 95 18 46 53 17 8 5 18 31 15 14 27 7 8 17 7 93 28 14 64 22 15 8 48 40 29 32 15 10 26 38 92 13 54 242 3 84 0 22 55 226 137 6 121 19 57 20 16 55 85 4 17 33 53 42 81 83 26 30 36 55 64 9 66 84 47 14 13 91 43 19 13 23 37 17 19 27 24 0 59 88 26 15 20 111 16 50 7 32 22 5 17 15 28 38 22 27 28 48 52 41 28 16 114 105 37 112 43 4 69 17 87 95 11 45 15 22 24 24 65 3 27 29 73 63 47 17 34 18 53 17 37 17 20 6 14 36 78 20 8 14 23 47 6 6 22 3 11 89 3 5 18 20 146 25 8 25 44 29 28 22 26 34 24 73 34 85 42 37 35 79 40 39 34 32 23 18 104 65 9 45 19 21 17 48 42 21 40 28 25 27 73 23 34 2 186 119 39 90 27 23 23 38 48 0 28 66 71 45 13 24 7 25 19 16 23 24 129 50 34 21 69 32 14 26 16 14 79 10 15 35 83 24 21 61 28 128 42 66 19 12 25 28 26 60 26 36 30 128 60 8 15 24 59 47 2 19 63 42 22 4 29 30 49 65 12 8 20 32 71 48 32 17 59 28 24 21 20 22 26 24 15 35 0 17 35 13 29 14 47 49 7 13 6 17 31 19 38 15 53 143 34 68 37 23 6 34 29 7 46 92 30 48 31 3 30 16 40 24 44 69 43 19 65 170 29 5 28 134 73 20 11 82 86 29 13 21 12 10 24 12 81 163 12 193 6 28 39 45 31 23 36 65 11 39 113 30 11 28 125 45 52 34 87 20 29 16 97 42 97 26 45 39 15 109 53 18 59 52 33 29 52 31 45 25 45 136 11 24 33 61 26 28 29 106 30 33 18 48 56 86 92 23 37 23 14 2 14 36 346 138 60 357 25 129 51 39 29 11 53 15 14 13 44 38 41 41 8 18 32 16 20 21 17 8 96 16 23 156 45 91 21 6 43 48 25 45 29 8 22 23 31 42 3 198 10 14 29 25 39 64 39 11 26 28 39 96 13 53 5 34 9 11 21 67 28 24 50 19 10 11 22 23 15 13 30 30 176 37 84 80 124 21 74 32 81 43 23 25 68 52 83 27 20 0 33 56 14 23 30 22 132 113 9 71 18 15 16 5 38 61 32 70 35 44 36 82 12 19 25 56 12 24 164 178 24 42 24 19 11 41 67 112 11 27 62 39 5 26 82 58 18 39 25 15 60 20 122 54 35 24 51 28 24 13 180 152 39 59 14 42 123 70 441 14 35 36 425 22 33 13 41 32 48 41 161 16 18 110 56 36 262 66 33 47 21 15 29 84 58 48 47 2 22 14 22 35 120 98 20 59 274 17 15 22 15 59 16 0 27 31 40 47 63 69 16 68 32 39 426 46 55 29 28 47 14 42 42 14 17 28 110 23 16 27 12 36 25 38 21 57 45 37 40 25 7 21 46 54 29 44 49 16 46 13 48 16 103 37 35 12 34 ]
@@@ Loss per-class: [ 0.783982 3.18666 3.2062 1.93192 1.13554 5.94865 3.78424 2.7257 2.91009 3.55039 3.67531 4.63638 2.54975 3.03168 5.47323 3.45514 2.00946 3.46508 5.28838 1.45031 2.01961 2.53187 4.32731 2.49431 2.27803 2.86084 2.38745 6.11754 2.29249 3.09794 5.19071 4.38293 2.9358 6.49864 0.7964 1.2184 3.54847 2.93923 4.16309 0.675676 1.37548 4.00145 5.25364 2.15652 1.84911 2.84439 1.70214 6.43027 2.107 2.27914 4.87492 3.08787 3.4555 4.11068 4.05315 2.63934 3.7301 2.01529 2.15043 2.24309 4.90856 2.28386 2.33265 2.9146 5.47538 1.87214 2.58016 3.59426 2.37302 2.49501 1.81552 2.62898 2.23184 2.62925 2.91239 6.33539 3.28009 0.523444 2.85787 2.51376 4.27122 3.38559 5.54328 2.14471 3.34768 4.1334 4.62536 5.04452 2.26975 5.27171 4.98531 3.40513 2.14337 5.26082 4.7727 4.06647 2.05848 3.2664 1.78525 1.76774 5.76396 2.78462 4.59952 1.78632 1.29235 5.24 2.55647 3.5586 3.43183 2.82853 3.55635 0 4.21369 2.1365 3.82919 5.50528 2.40555 4.57988 2.89727 4.24417 3.357 5.38971 4.26313 4.12093 1.67611 2.55078 5.54501 4.35358 6.69394 2.0065 1.89387 2.98901 1.60195 2.75982 2.61212 2.62684 3.17693 0.715493 3.00201 3.50311 4.47984 2.08755 1.8094 1.88398 3.95653 3.419 3.35027 4.64072 2.80237 1.87572 1.68075 2.46597 4.12003 3.99691 2.14081 0.912414 1.73888 3.36513 1.72934 2.50819 3.11991 2.77216 3.89907 6.60518 2.39237 3.24795 3.19846 3.79393 1.43936 4.51054 2.79132 4.00316 2.94091 2.85204 5.1847 1.86415 2.62022 4.61688 4.03431 4.7085 3.50635 4.86421 9.05388 3.52725 1.03319 3.18288 7.12767 2.81484 4.07447 3.17863 2.10739 2.31681 2.81699 3.15851 3.00963 1.9265 2.04652 3.98 4.0295 3.90495 3.579 3.58714 2.97966 4.51641 3.80388 2.15224 3.40258 2.1443 2.79567 3.46006 5.35356 2.71014 1.32453 0 3.92185 1.8088 3.20395 2.1679 2.14795 3.24545 1.5904 2.80963 3.94467 2.26714 5.65853 3.29777 6.35189 5.21024 2.52115 3.83929 3.16522 2.75812 3.16201 2.76989 5.33976 5.43334 1.71639 4.44869 2.48882 2.44792 7.50827 2.23325 3.03006 1.98824 5.23958 6.52097 3.71369 6.81949 3.51013 3.30016 6.32583 2.79731 4.2259 2.20634 3.21665 1.84772 1.87073 2.37443 2.86332 1.75264 2.13986 3.02578 3.26908 2.41968 3.08808 0.501029 3.05187 5.03972 0.825796 2.39503 2.50914 1.79229 5.37473 3.85296 3.08768 4.99205 2.64771 7.19323 2.57067 3.15698 2.72834 3.56161 4.74744 3.30791 2.19352 2.82811 2.29667 2.72985 9.15417 2.17555 2.227 3.60368 2.968 0.598749 3.98699 3.71025 2.945 3.13643 6.11809 4.90721 2.83739 3.69236 2.63263 2.58353 1.0637 2.55441 6.17779 2.52793 3.76124 3.89208 3.97843 3.07056 1.9452 3.32251 3.61178 4.56796 2.91133 2.54299 2.90968 6.50054 6.64896 3.73753 5.72241 1.88962 4.57287 4.17747 3.28994 6.0729 4.03954 4.6602 2.98592 5.44521 3.87952 3.9316 1.9866 2.84956 6.27506 2.71744 2.31981 4.44969 4.17261 1.6811 3.98165 2.2271 3.79962 4.16872 2.30241 4.70678 4.54614 4.61639 3.21758 2.91822 2.23473 1.18679 3.5563 2.85302 3.46334 5.10905 3.76203 3.02125 2.11178 3.223 3.95586 3.5851 3.41358 5.55193 2.20598 2.50384 2.58338 5.15198 3.25575 2.75576 1.6268 3.70828 1.39187 2.84513 2.18627 1.60804 2.35379 2.99999 3.45521 2.51563 1.60438 2.43503 2.11355 2.49852 3.47024 5.32659 2.59689 4.9693 2.68599 3.82316 3.3999 2.66216 3.98661 1.84947 4.21703 5.36664 3.11503 3.27783 8.17667 2.59467 2.73474 1.84255 3.67582 1.63498 2.30997 3.80594 2.46817 3.9456 6.04477 4.39411 2.65208 3.89794 3.41981 2.10269 3.11896 0.794804 4.64077 2.45045 2.09039 3.11966 1.44038 1.48956 4.92546 2.96094 1.47672 1.97979 5.76723 2.10306 4.25089 3.69917 3.24598 4.07725 3.63086 1.86202 4.31844 4.05357 3.759 2.57418 1.54404 3.25859 3.46204 3.44386 7.10391 1.55054 5.54673 3.08269 2.71135 2.46685 2.87402 3.55114 3.55565 3.30484 0 0 0.437759 2.20007 3.85973 4.32594 0.920707 2.69293 4.10271 3.31733 4.73216 4.86076 4.97977 1.12208 1.62233 3.49718 1.27443 1.83129 2.93376 2.00506 3.90702 2.12412 1.71056 3.70752 4.39867 3.53492 6.04713 2.39985 4.52319 4.32167 3.85156 0 3.16641 3.98085 1.97626 3.08486 3.68431 4.73176 4.80506 7.13693 1.25212 2.11572 2.84103 4.39693 4.46118 3.49194 2.81854 1.93512 3.17015 5.37515 1.7479 2.8381 5.09363 1.08308 7.0795 2.1686 7.38014 3.84337 5.28375 2.37028 3.46178 3.01333 3.75805 2.83305 4.71324 5.01034 5.18781 4.78936 4.20549 6.13266 3.26936 2.4948 2.73523 3.91735 4.27282 3.19624 5.71039 5.45857 4.58535 5.02126 4.51646 2.88422 2.81058 1.87865 3.26024 3.43218 2.32375 6.26362 5.1601 2.18144 7.54404 2.47898 1.8745 3.64097 2.72599 3.19701 3.52872 5.64477 2.5505 3.39034 4.83239 1.45237 4.59517 4.15366 3.51605 3.32469 3.80189 0 4.53428 4.5815 2.28092 2.65469 2.18062 2.42447 3.49963 6.74804 4.95381 2.67919 2.42942 1.32772 4.63723 6.88633 4.32617 1.67925 4.98223 5.16598 4.62196 5.18365 5.2591 4.70327 3.502 2.10432 5.90767 4.48936 2.69939 4.22051 3.25558 3.95574 1.76672 2.37457 3.90706 3.33382 3.82445 3.14679 4.55031 2.35835 3.04191 5.41845 3.92349 3.72084 2.42059 4.23311 5.8992 6.54643 1.93326 3.20043 3.28143 2.89782 3.92396 3.6672 5.22625 2.68333 5.22827 5.47843 4.18327 4.73721 2.84375 3.17663 3.7657 1.6865 6.10751 3.40421 3.88607 4.24765 2.88463 4.47099 3.73364 5.37143 3.60387 4.13024 2.72008 5.09153 3.59648 2.30517 3.99321 4.27219 3.95191 3.14656 1.09778 2.77003 1.64255 0 3.70331 2.90803 6.2904 1.93678 5.5959 2.70051 5.14088 3.16818 3.92277 5.22481 4.08986 6.02562 5.35683 3.11977 3.16202 2.29152 3.22615 3.22975 1.6345 4.30905 6.45624 2.70629 4.26891 3.17834 8.50651 2.62304 1.60551 2.51666 4.59845 5.91546 4.32528 5.32085 5.07079 6.4065 3.48488 5.02681 3.52343 5.74426 8.62456 3.96187 0 2.97967 2.27654 3.81114 2.52951 4.03583 5.37546 2.27569 6.75474 5.4292 2.02896 5.33197 5.66828 3.02847 2.70914 4.2461 2.96087 3.75626 5.15229 3.74952 3.27167 4.33067 1.32221 4.003 8.01479 2.47502 1.14167 2.2835 0.873064 2.20035 4.72538 4.15985 6.28882 5.13801 3.30974 7.02837 2.46757 3.8863 4.66198 7.98458 2.76368 5.79327 4.39245 1.07296 3.35824 2.10048 2.67873 5.53366 4.23646 1.23474 5.55364 2.43978 2.25644 2.57256 3.00979 3.42205 4.62758 4.51614 2.57894 3.37209 2.12699 7.68436 2.33037 2.53019 2.6848 4.58615 4.50297 2.77242 5.1122 3.48798 7.05733 6.47417 4.74175 3.14149 3.33919 2.15177 4.58395 5.65103 4.13967 6.04989 3.71665 5.68457 3.33604 2.64283 2.2484 2.43948 5.04225 4.32085 3.18559 1.73288 4.01111 5.23671 4.39567 4.25151 2.93874 3.91025 4.24637 2.61331 6.37693 1.37317 3.05242 4.53593 3.04432 2.55943 2.0215 5.87138 4.11342 4.18603 5.5837 2.78687 3.0887 3.9158 3.12875 2.00432 5.25902 5.92981 4.7186 1.27837 3.49403 5.93824 1.55711 2.76166 3.5288 3.68843 2.61359 3.60001 0 4.35117 2.61449 3.18835 3.49018 6.12579 4.85741 3.97404 2.15992 3.87035 2.85505 3.88397 4.88857 3.40877 6.67883 3.79634 3.56244 3.88213 3.42816 5.6599 5.65484 4.34152 4.55887 1.96782 3.51824 5.0921 2.46803 1.0517 4.97008 2.64515 1.85169 1.95487 5.17248 3.71538 5.90874 2.62595 3.98196 2.85426 4.72997 5.57245 4.91592 3.12331 2.02584 3.00974 3.41952 2.26852 7.04895 3.01184 2.87604 3.18284 4.18235 4.74219 4.97369 2.66891 2.4145 3.87003 5.4983 3.9294 3.16207 2.22007 2.70849 3.42651 6.31443 6.55298 3.39205 1.98465 2.54853 2.47313 2.54258 1.72966 2.04172 5.47465 5.51263 4.87238 4.30114 1.96231 1.74322 3.47435 1.84583 0 2.88309 1.97777 4.48047 2.42043 5.58662 2.51153 3.25594 3.02892 4.18085 2.77603 3.18835 3.39586 3.84168 3.85864 5.40162 1.59388 1.76835 1.57721 6.90467 3.04793 2.40244 9.40592 2.67958 4.58929 3.17725 3.13168 3.46305 3.1118 5.98657 3.82892 7.50153 3.08841 4.97079 3.33533 1.9096 4.78331 3.22234 3.75739 3.83052 4.57457 2.21237 4.36244 3.42831 4.75003 4.36046 2.97477 3.52387 4.54443 3.51937 3.42722 4.06324 3.68603 3.83861 4.37325 4.85248 4.10975 3.6838 3.70824 2.10302 3.31726 4.12682 1.49259 2.31239 2.11517 3.83196 5.53267 2.41549 2.98201 4.87419 5.23085 5.10458 3.84104 4.48469 3.98163 6.64656 3.59447 1.01735 2.30293 3.72647 1.88549 3.2547 5.98869 3.31263 4.68108 3.16442 3.63147 5.83525 2.56662 1.31118 3.30809 5.13718 1.65216 4.76094 3.38199 2.05928 5.12417 2.75159 4.02786 4.66902 3.84497 2.86208 3.04102 3.90756 2.69244 3.39874 4.33158 2.90527 2.64171 3.70497 1.74733 2.33488 2.2887 3.30747 1.52038 3.32166 5.00204 2.19004 5.2065 5.65151 2.59213 2.98941 2.19328 3.41868 4.58996 3.86154 3.10515 4.93173 2.97985 3.98948 1.75327 3.24355 2.02904 2.76203 2.22412 4.18085 3.63057 3.81313 5.16809 5.58428 1.53347 3.50132 4.46245 2.75266 3.92677 5.68452 3.08384 4.46013 5.58862 4.9192 3.21723 3.38409 4.29363 2.96799 4.43088 3.33146 4.61831 2.17901 5.13773 4.36183 6.76886 4.85742 4.55957 2.74142 2.80592 5.06971 2.55398 3.97141 2.0714 2.72401 4.95869 2.46368 3.82632 4.39314 2.82684 4.29732 2.09341 3.50344 2.30096 5.08702 1.23097 2.73375 3.6156 2.05985 3.72834 3.17509 4.33179 3.0185 4.15909 4.79883 4.5542 3.5927 2.99725 3.66146 1.31013 1.86169 5.79859 5.16261 2.80092 4.61959 2.97441 3.94805 2.44805 2.73033 3.37532 5.41373 2.43589 2.62088 4.4787 5.84362 3.24523 3.45636 3.88832 2.65359 3.43885 1.91642 4.24047 3.11386 1.48687 3.38385 5.09383 0 3.94708 3.87869 3.86982 2.89514 3.32987 6.01579 3.00267 3.83828 6.01215 3.36169 3.79961 2.27369 4.63759 2.52632 3.77693 5.94241 4.16848 3.90641 2.94656 3.81061 2.3745 8.51088 2.51016 6.08109 6.56695 5.7852 6.50193 1.72404 2.75725 3.40959 1.30712 3.59092 5.4134 3.98564 3.59444 2.32913 3.92263 1.97051 4.59453 3.92647 3.73363 3.00091 8.28002 3.18381 2.40557 5.23846 6.39981 3.81868 4.43956 3.00188 4.34643 4.02928 1.41074 1.3192 2.81154 4.05773 4.58951 3.04966 3.71022 2.9188 2.62059 1.9271 5.24524 4.54387 5.34453 1.87978 4.63572 1.89947 2.45004 5.19829 2.40151 1.42016 1.211 3.65548 3.41066 3.52745 3.54662 4.45209 3.60819 4.7232 2.3386 5.33073 4.35655 2.75428 4.50797 3.13308 6.38071 2.25609 2.05669 5.30045 2.79216 2.97742 4.36265 4.20233 2.99173 4.47299 2.41974 4.53127 3.82747 4.77484 2.30295 2.42744 3.91462 3.26689 3.73042 1.31099 3.20636 4.84421 4.42314 4.07503 3.96671 5.97458 5.0076 0 3.73575 2.54861 7.20189 3.65256 3.13417 4.70217 3.42518 4.4504 3.95241 3.71811 2.01611 3.74105 2.37172 4.08507 5.92531 4.14878 5.72289 1.74896 2.23202 5.2887 5.33563 4.85946 2.28653 2.94652 6.74433 3.79031 5.23407 2.72222 7.01712 1.94253 3.40164 4.47484 4.57496 1.28421 3.59677 1.55039 7.24118 2.10375 1.58181 2.4776 4.22977 2.24669 3.00548 3.11993 3.02313 4.45846 3.83613 5.04411 1.71706 1.88076 3.09154 4.25111 4.84096 ]
@@@ Frame-accuracy per-class: [ 75.9054 6.55738 0 57.8616 76.9231 0 0 11.4286 39.7163 9.97732 13.3333 13.6986 40.5797 10.1266 0 20.5607 45.977 23.2365 13.3333 68.2927 43.4783 34.5865 0 36.6197 42.2857 31.2925 62.8571 0 34.4828 28.5714 0 8.95522 0 0 96.9697 91.3907 10.3286 9.52381 0 84.2808 23.8995 8.88889 9.44882 46.4945 53.3333 32.7869 38.5093 0 47.191 32.1839 0 38.5185 13.5338 0 0 29.1262 20.4651 55.0459 39.5797 49.3827 0 20.4082 38.8979 46.2921 0 45.0704 24.2424 14.9254 2.8169 45.1613 62.3853 36.3636 42.2939 41.7178 20.6897 0 19.7802 98.5915 4.21053 56.6038 0 22.5564 0 41.3793 2.24719 0 0 4.0404 46.6368 0 0 0 30.3448 0 36.0902 0 46.1538 11.9403 54.7771 58.8235 6.45161 23.1884 0 61.9926 74.6667 4.83092 57.1429 1.55039 2.66667 27.907 0 0 10.6952 69.0647 0 0 28.9855 0 6.15385 5.33333 23.5294 0 0 0 45.614 45.977 0 0 0 56.6038 51.9774 23.5294 62.2222 13.7931 50.7463 4.8 24 88.3249 24.6154 43.1373 0 43.3121 43.4783 44.8 0 0 24.8588 0 10.989 59.0164 55.3191 28.169 3.30579 0 53.3333 79.5367 36.3636 17.0213 58.9147 29.5165 0 40.8163 19.0476 0 34.9206 20.8955 26.087 12.1212 60.9524 0 12.4031 35.2941 35.5556 3.92157 0 31.5789 42.623 0 9.41176 19.7183 0 0 0 0 92.3077 15.3846 0 14.6341 0 4.12371 67.6923 35.2941 26.5487 32.8767 30.9859 38.5542 36.3636 6.06061 0 0 11.2676 6.34921 16 0 0 50.5495 29.0909 63.8037 46.5116 0 0 27.5862 64.4068 0 0 51.0638 7.29927 45.3333 37.7358 14.9254 61.3861 26.6667 27.6923 31.5789 0 15.0943 0 0 34.4828 10.8949 24.7619 33.0097 6.72269 29.2683 0 0 47.619 0 0 46.1538 0 18.1818 0 43.038 0 0 0 0 10.2564 9.7561 0 13.3333 0 35.5556 26.6667 45.1613 61.5385 0 20.2532 75.1323 48.2759 0 30.2521 51.4286 14.3885 95.7746 7.84314 0 96.2963 48.6486 23.9316 64.1509 8.16327 18.41 24.2424 0 0 0 29.6296 14.0845 32.4324 4.93827 0 14.3713 19.0476 24.3902 24.3902 21.5385 0 35.8974 26.8657 14.3541 14.433 92.562 8.80503 25.2632 34.1463 13.7931 0 8.08081 26.5487 0 34.1463 28.9855 75.188 32 0 0 13.9535 17.7778 0 21.8182 30.3797 0 0 0 4.44444 23.5294 18.9474 0 0 0 0 49.5868 0 2.29885 22.6415 0 23.5294 0 38.2979 0 32.2581 3.38983 52.1739 15.7303 0 30.9859 31.1111 1.94175 0 57.3913 0 43.8356 12.9032 0 23.7288 0 0 0 16.2162 41.0256 29.6943 63.0631 30.2158 36.6197 13.9535 0 15.5039 38.2022 50.6329 16.092 0 13.9535 21.3333 0 36.5591 42.5532 49.2308 0 25.7426 32.9897 66.9951 8 52.3077 3.2 57.971 39.2523 40.1084 33.6134 17.9104 28.5714 65.2632 42.4242 50.7042 31.5789 21.3836 0 39.5604 8 19.5122 18.1818 30.137 17.2043 0 55.2381 7.01754 0 24 25.3968 0 27.1186 34.5679 56.0748 0 73.1707 21.0526 20.4082 38.5965 7.7821 0 0 0 2.24719 26.2295 49.2308 8 77.4194 0 40 51.9337 0 58.5366 70.7692 7.61905 46.5116 67.3684 48.3516 0 46.5116 4.64037 8.08081 15.6863 0 22.2222 61.0169 2.7972 37.037 12.1739 51.5464 57.868 30.1075 25.4545 12.0482 0 71.0744 0 0 38.961 39.1753 35.6436 0 17.3228 36.3636 0 0 98.0392 42.1053 0 6.06061 89.4118 20.2247 0 0 0 0 0 84.4444 75.5556 8.69565 85.2459 53.3333 16.8675 56.6372 0 35.0877 73.4694 22.5352 0 13.7931 0 43.4783 0 0 20.7792 0 0 3.50877 64.3478 0 0 0 0 0 64.1975 39.7857 22.5352 0 0 0 10.8108 61.8182 0 6.06061 60.9524 23.1405 0 70.303 0 53.7313 0 3.35196 0 25.974 25.8824 32.4324 9.70874 15.1899 0 0 0 26.6667 12.766 0 32.4324 1.94175 21.3198 19.7183 8 47.619 0 0 5.30973 8.77514 0 37.9747 25.3968 48.6486 4.65116 0 36.7347 0 0 54.902 0 46.5116 54.2056 6.72269 57.1429 0 0 0 62.6263 0 0 63.1579 0 0 0 27.1186 0 0 0 0 50.4854 15.2381 41.2371 42.5532 0 0 0 40.8163 11.2676 70.9677 0 0 13.3333 62.069 0 5.19481 5.04202 0 0 0 0 42.2018 20.9524 3.07692 17.4757 0 0 9.30233 63.4667 37.037 17.9104 0 19.3548 27.027 0 13.3333 12.3077 0 0 6.77966 21.6867 0 0 0 55.4974 5.40541 21.5054 26.1682 11.4286 0 0 21.6216 0 0 0 0 13.3333 0 5.71429 40 1.06952 0 6.89655 10.8527 13.3333 0 0 10.3093 19.7531 20.339 0 0 0 7.54717 2.5974 2.16216 7.40741 16.5138 74.6392 0 46.1538 0 4.44444 23.4234 14.128 50.9091 0 18.107 0 12.1739 4.87805 0 7.20721 5.84795 0 22.8571 0 37.3832 28.2353 41.7178 64.6707 0 0 19.1781 7.20721 3.10078 0 25.5639 55.6213 65.2632 0 0 2.18579 0 0 0 21.2766 0 5.71429 0 0 0 0 23.5294 46.3277 3.77358 25.8065 24.3902 0 30.303 0 0 36.9231 0 0 11.4286 51.6129 3.50877 15.5844 0 0 3.50877 30.9278 1.90476 72.2892 0 0 23.5808 74.8815 42.6667 83.5556 57.4713 0 2.8777 0 3.42857 19.8953 0 59.3407 0 0 0 53.0612 0 0 76.3636 10.1695 47.619 39.3701 10.5263 0 72.4638 0 31.7757 51.4286 34.6667 5.71429 24.3902 0 0 16.4384 22.9299 43.9024 0 41.3793 29.7872 46.3158 0 0 31.1111 0 0 3.35196 0 0 0 24.3902 49.8294 0 0 0 0 10.1695 10.5263 44.4444 22.6415 40.5797 24.4898 1.36054 2.89855 4.67836 49.4118 18.6667 0 1.25786 0 30.3797 23.1884 3.07692 25.5319 0 70.8134 12.2137 0 24.1758 15.3846 27.907 0 0 4.70588 0 46.9136 24.5614 3.92157 25.4545 40.8163 4.25532 0 0 70.2413 10.0418 0 61.8785 36.3636 0 4.25532 28.5714 0 0 0 30.0752 26.5734 21.978 0 0 0 35.2941 0 6.06061 25.5319 0 1.5444 0 23.1884 4.65116 0 18.4615 0 0 12.1212 0 59.1195 0 0 5.6338 80.2395 0 13.9535 43.9024 38.5965 14.0078 4.70588 10.5263 35.8974 0 11.7647 0 0 3.30579 15.0943 35.6164 13.1148 11.6732 36.3636 0 25.8065 20.4082 3.36134 2.10526 0 5.12821 33.0709 42.3529 0 0 0 13.1148 32.3232 32.0611 8 0 0 24.6154 47.5524 16.4948 36.9231 17.1429 62.1849 52.6316 0 0 19.5122 0 49.0566 57.1429 0 59.1549 0 57.1429 56.338 0 27.1186 0 40 18.1818 0 0 15.3846 5.71429 15.873 20.5128 2.5974 0 67.2897 64.1115 60.8696 0 18.6667 42.5532 0 14.4928 10.1695 0 0 7.56757 26.2295 0 25.3968 0 29.5082 0 17.284 53.0612 4.49438 28.777 9.1954 0 0 62.7566 0 0 0 7.43494 17.6871 0 0 7.27273 18.4971 13.5593 37.037 0 0 0 4.08163 0 11.0429 53.8226 0 8.26873 92.3077 45.614 50.6329 43.956 0 42.5532 30.137 0 0 2.53165 10.5727 3.27869 0 0 3.98406 74.7253 34.2857 2.89855 62.8571 0 0 0 11.2821 4.70588 7.17949 0 8.79121 83.5443 12.9032 2.73973 63.5514 0 3.36134 24.7619 0 30.5085 5.71429 0 8.79121 31.3725 26.3736 8.05861 17.3913 24.4898 8.95522 17.8862 26.4151 7.01754 57.6271 39.4366 29.5082 11.9403 75.6757 32.9897 3.53982 45.0867 4.32432 0 45.3333 25.5319 0 0 0 10.9589 14.43 1.44404 28.0992 19.3007 62.7451 3.0888 50.4854 40.5063 37.2881 8.69565 22.4299 0 0 0 71.9101 12.987 7.22892 45.7831 0 0 12.3077 0 4.87805 0 5.71429 0 16.5803 0 12.766 42.8115 2.1978 42.623 0 0 0 0 3.92157 43.956 47.4576 0 8.88889 0 53.9683 18.8235 0 44.3325 9.52381 0 23.7288 0 48.1013 27.907 43.038 0 83.0189 45.614 35.443 54.9223 0 9.34579 0 14.4928 0 0 0 5.92593 17.5439 4.08163 71.2871 35.8974 0 0 31.1111 0 0 0 52.459 3.27869 30.0283 0 56.8047 44.7205 0 0 9.39597 18.4615 24.5399 52.8736 4.25532 54.902 1.45985 34.2857 81.4371 14.5455 14.6341 0 20.8955 0 34.4828 21.2766 9.83607 0 17.3585 22.9075 0 39.1608 0 25.8065 6.06061 0 2.5974 0 0 0 19.7183 17.9775 46.5753 0 0 0 0 5.30973 0 69.3878 25.5319 22.9692 69.3878 9.41176 0 15.3846 0 33.7349 13.3333 64 0 0 9.6 7.59494 0 15.0943 49.697 0 0 20.2532 3.92157 25.8065 9.91736 4.87805 68.5714 62.3853 28.169 16.3265 0 7.01754 0 14.8148 29.3629 57.0492 0 0 0 42.3529 17.004 63.8298 36.9196 0 14.0845 87.6712 54.7591 4.44444 5.97015 7.40741 4.81928 0 8.24742 4.81928 28.483 0 0 32.5792 0 24.6575 5.33333 37.594 65.6716 0 27.907 12.9032 6.77966 15.3846 22.2222 2.06186 27.3684 0 8.88889 0 13.3333 39.4366 17.4274 22.335 0 78.9916 18.9435 0 0 0 0 0 0 0 7.27273 44.4444 0 12.6316 14.1732 0 0 0 0 0 56.7409 12.9032 50.4505 13.5593 0 10.5263 0 70.5882 30.5882 0 0 0 26.2443 34.0426 0 0 0 21.9178 0 59.7403 18.6047 0 0 61.3333 12.3457 58.8235 0 55.814 55.914 33.0275 0 44.9438 16.1616 6.06061 15.0538 0 0 0 61.8357 64 19.7183 0 0 ]

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:347) Done 295 files, 0 with no tgt_mats, 0 with other errors. [CROSS-VALIDATION, 0.0208483 min, fps63324.1]
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:353) AvgLoss: 2.66164 (Xent), [AvgXent: 2.66164, AvgTargetEnt: 0]
progress: []
FRAME_ACCURACY >> 28.1801% <<

WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Close():kaldi-io.cc:515) Pipe ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- | had nonzero return status 36096
