speech-HP-Z2-Tower-G9
nnet-train-multistream-perutt --cross-validate=false --randomize=true --verbose=0 --num-streams=10 --max-frames=15000 --learn-rate=2e-05 --momentum=0.9 --l1-penalty=0 --l2-penalty=0 --feature-transform=exp_FG/blstm4i/final.feature_transform 'ark:copy-feats scp:exp_FG/blstm4i/train.scp ark:- | apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_tr90/utt2spk scp:data-fbank/train_tr90/cmvn.scp ark:- ark:- | add-deltas --delta-order=2 ark:- ark:- |' 'ark:ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- |' exp_FG/blstm4i/nnet/nnet_iter08_learnrate0.00004_tr0.6686_cv1.8011 exp_FG/blstm4i/nnet/nnet_iter10 
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:243) Not in compute-exclusive mode.  Suggestion: use 'nvidia-smi -c 3' to set compute exclusive mode
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:438) Selecting from 1 GPUs
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:453) cudaSetDevice(0): NVIDIA RTX A2000 12GB	free:11620M, used:410M, total:12031M, free/total:0.96589
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:501) Device: 0, mem_ratio: 0.96589
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:382) Trying to select device: 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:511) Success selecting device 0 free mem ratio: 0.96589
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:FinalizeActiveGpu():cu-device.cc:338) The active GPU is [0]: NVIDIA RTX A2000 12GB	free:11106M, used:924M, total:12031M, free/total:0.923168 version 8.6
copy-feats scp:exp_FG/blstm4i/train.scp ark:- 
apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_tr90/utt2spk scp:data-fbank/train_tr90/cmvn.scp ark:- ark:- 
add-deltas --delta-order=2 ark:- ark:- 
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:144) TRAINING STARTED
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Read():nnet/nnet-matrix-buffer.h:191) Read() started... Buffer size in MB: 0, max 3072, having 0 utterances.
ali-to-post ark:- ark:- 
ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl 'ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |' ark:- 
LOG (copy-feats[5.5.1074~1-71f3]:main():copy-feats.cc:143) Copied 2624 feature matrices.
LOG (apply-cmvn[5.5.1074~1-71f3]:main():apply-cmvn.cc:159) Applied cepstral mean and variance normalization to 2624 utterances, errors on 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:303) ### After 0 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:304) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.753756, max 0.905735, mean 0.00394505, stddev 0.0902405, skewness 0.0115745, kurtosis 1.15679 ) 
  f_w_gifo_r_   ( min -0.421152, max 0.414786, mean -0.000591021, stddev 0.0792712, skewness -0.00320186, kurtosis 0.0371957 ) 
  f_bias_   ( min -0.349357, max 1.43767, mean 0.211129, stddev 0.465931, skewness 1.05942, kurtosis -0.64723 ) 
  f_peephole_i_c_   ( min -0.687627, max 0.491544, mean -0.00526894, stddev 0.135972, skewness -0.106508, kurtosis 2.50647 ) 
  f_peephole_f_c_   ( min -0.713426, max 0.972877, mean 0.00593392, stddev 0.182442, skewness 0.396659, kurtosis 4.62657 ) 
  f_peephole_o_c_   ( min -0.602303, max 0.506963, mean -0.0126824, stddev 0.196669, skewness 0.066137, kurtosis -0.148737 ) 
  f_w_r_m_   ( min -0.507913, max 0.509972, mean 0.000573339, stddev 0.105837, skewness 0.00224517, kurtosis 0.030292 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -1.70143, max 1.18589, mean 0.00602792, stddev 0.0981853, skewness -0.178587, kurtosis 5.58694 ) 
  b_w_gifo_r_   ( min -0.350056, max 0.375124, mean -0.000185719, stddev 0.0731486, skewness 0.000348028, kurtosis -0.205019 ) 
  b_bias_   ( min -0.388563, max 1.23293, mean 0.203743, stddev 0.455278, skewness 1.0271, kurtosis -0.695354 ) 
  b_peephole_i_c_   ( min -0.448861, max 0.318401, mean 0.00372949, stddev 0.105018, skewness -0.179953, kurtosis 1.7247 ) 
  b_peephole_f_c_   ( min -0.646749, max 0.759774, mean 0.0142013, stddev 0.185468, skewness 0.576903, kurtosis 3.27403 ) 
  b_peephole_o_c_   ( min -0.599004, max 0.618308, mean -0.0155664, stddev 0.20353, skewness -0.0625438, kurtosis 0.523699 ) 
  b_w_r_m_   ( min -0.423328, max 0.401554, mean -0.000398632, stddev 0.0958175, skewness 0.00117138, kurtosis -0.0204458 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.992554, max 0.778909, mean -0.000155912, stddev 0.109231, skewness 0.00547628, kurtosis 0.0577655 ) , lr-coef 1, max-norm 0
  bias ( min -0.0940032, max 2.55054, mean -3.23635e-09, stddev 0.0848657, skewness 22.5199, kurtosis 645.501 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:305) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -8.67671, max 9.02116, mean 0.0321104, stddev 1.01793, skewness 0.450612, kurtosis 3.20408 ) 
[1] output of <BlstmProjected> ( min -4.41773, max 4.62047, mean 0.00195482, stddev 0.783244, skewness -0.0170014, kurtosis 1.33984 ) 
[2] output of <Tanh> ( min -0.999709, max 0.999806, mean 0.00207943, stddev 0.513364, skewness -0.00870367, kurtosis -0.753898 ) 
[3] output of <AffineTransform> ( min -18.0794, max 21.9897, mean 0.00429088, stddev 2.4785, skewness 0.869051, kurtosis 3.4276 ) 
[4] output of <Softmax> ( min 4.00506e-17, max 0.999986, mean 0.000780259, stddev 0.0220397, skewness 36.9854, kurtosis 1460.64 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:307) 
### BACKWARD PROPAGATION BUFFER CONTENT :
[0] diff of <Input>  ( min -5.48785, max 6.83323, mean -0.00925056, stddev 0.284602, skewness -1.62686, kurtosis 37.5579 ) 
[1] diff-output of <BlstmProjected> ( min -0.59304, max 0.6813, mean -0.000280442, stddev 0.0469981, skewness -0.010624, kurtosis 10.4853 ) 
[2] diff-output of <Tanh> ( min -0.894184, max 0.834449, mean -0.000317607, stddev 0.0607958, skewness 0.00128845, kurtosis 6.45668 ) 
[3] diff-output of <AffineTransform> ( min -0.999997, max 0.960047, mean -1.02069e-07, stddev 0.0164695, skewness -19.7301, kurtosis 2017.74 ) 
[4] diff-output of <Softmax> ( min -0.999997, max 0.960047, mean -1.02069e-07, stddev 0.0164695, skewness -19.7301, kurtosis 2017.74 ) 
### END BACKWARD


LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:308) 
### GRADIENT STATS :
Component 1 : <BlstmProjected>, ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  ### Gradients 
  f_w_gifo_x_corr_   ( min -50.4407, max 41.649, mean 0.0818875, stddev 3.50361, skewness -0.0800133, kurtosis 12.645 ) 
  f_w_gifo_r_corr_   ( min -52.7843, max 42.785, mean -0.00831769, stddev 2.60216, skewness -0.0990614, kurtosis 10.7079 ) 
  f_bias_corr_   ( min -46.1556, max 34.9864, mean 0.0754122, stddev 4.69113, skewness -0.484474, kurtosis 15.5016 ) 
  f_peephole_i_c_corr_   ( min -20.7536, max 22.573, mean 0.0423229, stddev 4.10325, skewness 0.277142, kurtosis 6.03694 ) 
  f_peephole_f_c_corr_   ( min -250, max 75.2341, mean -0.71332, stddev 18.3216, skewness -7.97457, kurtosis 107.094 ) 
  f_peephole_o_c_corr_   ( min -157.607, max 250, mean -0.593155, stddev 20.6774, skewness 4.07028, kurtosis 75.8621 ) 
  f_w_r_m_corr_   ( min -38.3433, max 46.9838, mean 0.0129205, stddev 4.08345, skewness 0.0950935, kurtosis 6.22966 ) 
  ---
  b_w_gifo_x_corr_   ( min -84.5733, max 66.218, mean 0.203428, stddev 4.53889, skewness 0.120222, kurtosis 52.1516 ) 
  b_w_gifo_r_corr_   ( min -33.389, max 30.8186, mean -0.00746928, stddev 2.60618, skewness -0.0187834, kurtosis 6.26482 ) 
  b_bias_corr_   ( min -67.7547, max 74.7867, mean -0.757278, stddev 7.16469, skewness 0.452947, kurtosis 29.8636 ) 
  b_peephole_i_c_corr_   ( min -46.3502, max 19.1859, mean 0.162986, stddev 4.8638, skewness -3.05389, kurtosis 28.6139 ) 
  b_peephole_f_c_corr_   ( min -31.8649, max 79.5654, mean 0.337293, stddev 10.3605, skewness 1.58453, kurtosis 11.7134 ) 
  b_peephole_o_c_corr_   ( min -75.1752, max 54.3313, mean -0.360555, stddev 14.4227, skewness -0.971929, kurtosis 6.36283 ) 
  b_w_r_m_corr_   ( min -41.0657, max 35.1292, mean -0.00682775, stddev 4.04913, skewness -0.0156403, kurtosis 3.06491 ) 

  ### Activations (mostly after non-linearities)
  YI_FW(0..1)^   ( min 0, max 1, mean 0.423148, stddev 0.349488, skewness 0.384349, kurtosis -1.30397 ) 
  YF_FW(0..1)^   ( min 0, max 1, mean 0.644566, stddev 0.322596, skewness -0.554444, kurtosis -0.990597 ) 
  YO_FW(0..1)^   ( min 0, max 1, mean 0.369649, stddev 0.358505, skewness 0.624846, kurtosis -1.16955 ) 
  YG_FW(-1..1)   ( min -1, max 1, mean 0.0183943, stddev 0.867991, skewness -0.0380497, kurtosis -1.79986 ) 
  YC_FW(-R..R)*  ( min -50, max 50, mean 0.344858, stddev 13.2619, skewness 0.14915, kurtosis 9.23255 ) 
  YH_FW(-1..1)   ( min -1, max 1, mean 0.0336881, stddev 0.691065, skewness -0.0588501, kurtosis -1.25773 ) 
  YM_FW(-1..1)   ( min -1, max 1, mean 0.00223968, stddev 0.333068, skewness -0.0794835, kurtosis 2.78882 ) 
  YR_FW(-R..R)   ( min -4.2393, max 4.34023, mean 0.0176497, stddev 0.772515, skewness 0.0119588, kurtosis 1.30862 ) 
  ---
  YI_BW(0..1)^   ( min 0, max 1, mean 0.456435, stddev 0.344144, skewness 0.243279, kurtosis -1.39943 ) 
  YF_BW(0..1)^   ( min 0, max 1, mean 0.641932, stddev 0.294874, skewness -0.547385, kurtosis -0.782285 ) 
  YO_BW(0..1)^   ( min 0, max 1, mean 0.378132, stddev 0.363511, skewness 0.568778, kurtosis -1.25572 ) 
  YG_BW(-1..1)   ( min -1, max 1, mean 0.0120191, stddev 0.865247, skewness -0.0228779, kurtosis -1.7975 ) 
  YC_BW(-R..R)*  ( min -50, max 50, mean 1.29384, stddev 12.1238, skewness 1.05259, kurtosis 10.4693 ) 
  YH_BW(-1..1)   ( min -1, max 1, mean 0.044883, stddev 0.695356, skewness -0.0548952, kurtosis -1.29444 ) 
  YM_BW(-1..1)   ( min -0.999998, max 1, mean 0.00418138, stddev 0.334434, skewness 0.0256751, kurtosis 2.46495 ) 
  YR_BW(-R..R)   ( min -4.41773, max 4.62047, mean -0.0137602, stddev 0.789802, skewness -0.0415239, kurtosis 1.40214 ) 

  ### Derivatives (w.r.t. inputs of non-linearities)
  DI_FW^  ( min -1, max 1, mean 1.14047e-05, stddev 0.0252073, skewness -1.32576, kurtosis 347.641 ) 
  DF_FW^  ( min -1, max 1, mean 7.75691e-06, stddev 0.021445, skewness 0.358199, kurtosis 574.971 ) 
  DO_FW^  ( min -1, max 1, mean 9.61504e-05, stddev 0.0280539, skewness 0.354744, kurtosis 99.8696 ) 
  DG_FW   ( min -1, max 1, mean -3.83617e-05, stddev 0.0321684, skewness -0.820866, kurtosis 390.237 ) 
  DC_FW*  ( min -76.5635, max 53.405, mean -0.00994765, stddev 0.840448, skewness -37.7825, kurtosis 3868.51 ) 
  DH_FW   ( min -5.65935, max 5.87012, mean -0.000197068, stddev 0.128613, skewness 0.0343666, kurtosis 74.287 ) 
  DM_FW   ( min -6.00024, max 6.8406, mean -0.000143173, stddev 0.333769, skewness 0.0297457, kurtosis 19.8338 ) 
  DR_FW   ( min -1.69779, max 1.44935, mean -0.000214752, stddev 0.0912102, skewness -0.00853655, kurtosis 11.749 ) 
  ---
  DI_BW^  ( min -1, max 1, mean -0.000161807, stddev 0.0264022, skewness -0.188596, kurtosis 410.918 ) 
  DF_BW^  ( min -1, max 1, mean -8.51841e-05, stddev 0.0177957, skewness 0.280505, kurtosis 433.212 ) 
  DO_BW^  ( min -1, max 0.850268, mean -0.000458683, stddev 0.022769, skewness -1.06924, kurtosis 110.45 ) 
  DG_BW   ( min -1, max 1, mean -6.70561e-05, stddev 0.0368332, skewness -1.27988, kurtosis 272.037 ) 
  DC_BW*  ( min -60.4915, max 29.0797, mean -0.00938918, stddev 0.625428, skewness -52.9079, kurtosis 4402.72 ) 
  DH_BW   ( min -4.29795, max 3.86778, mean -0.000351851, stddev 0.110936, skewness -1.13628, kurtosis 78.3395 ) 
  DM_BW   ( min -5.83619, max 5.53243, mean -3.49668e-05, stddev 0.290678, skewness -0.239145, kurtosis 22.2484 ) 
  DR_BW   ( min -1.68808, max 1.66138, mean -0.000354163, stddev 0.0951658, skewness -0.104483, kurtosis 13.9031 ) 
Component 2 : <Tanh>, 
Component 3 : <AffineTransform>, 
  linearity_grad ( min -93.6335, max 78.2429, mean -4.41587e-08, stddev 1.44175, skewness -0.338911, kurtosis 428.175 ) , lr-coef 1, max-norm 0
  bias_grad ( min -184.48, max 191.655, mean -5.96046e-09, stddev 7.86466, skewness 0.98214, kurtosis 511.477 ) , lr-coef 1
Component 4 : <Softmax>, 
### END GRADIENT

LOG (ali-to-pdf[5.5.1074~1-71f3]:main():ali-to-pdf.cc:68) Converted 2919 alignments to pdf sequences.
LOG (ali-to-post[5.5.1074~1-71f3]:main():ali-to-post.cc:73) Converted 2919 alignments.
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:168) MC05_98, missing targets
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:334) ### After 755062 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:335) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.777469, max 0.928414, mean 0.00377631, stddev 0.090768, skewness 0.0132324, kurtosis 1.20399 ) 
  f_w_gifo_r_   ( min -0.422528, max 0.425772, mean -0.000579251, stddev 0.0793185, skewness -0.00330333, kurtosis 0.0386755 ) 
  f_bias_   ( min -0.350854, max 1.43578, mean 0.2118, stddev 0.466028, skewness 1.05924, kurtosis -0.647292 ) 
  f_peephole_i_c_   ( min -0.713937, max 0.47721, mean -0.00508839, stddev 0.136989, skewness -0.120386, kurtosis 2.62616 ) 
  f_peephole_f_c_   ( min -0.747354, max 0.948171, mean 0.00549824, stddev 0.182186, skewness 0.291555, kurtosis 4.50682 ) 
  f_peephole_o_c_   ( min -0.590781, max 0.51535, mean -0.0117647, stddev 0.196349, skewness 0.0939897, kurtosis -0.14291 ) 
  f_w_r_m_   ( min -0.513627, max 0.515774, mean 0.000598835, stddev 0.106355, skewness 0.00315993, kurtosis 0.0335686 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -1.70632, max 1.21681, mean 0.00593237, stddev 0.0988247, skewness -0.1756, kurtosis 5.75895 ) 
  b_w_gifo_r_   ( min -0.348945, max 0.381961, mean -0.000195101, stddev 0.0733887, skewness 0.000752165, kurtosis -0.196383 ) 
  b_bias_   ( min -0.404941, max 1.23176, mean 0.20444, stddev 0.455367, skewness 1.02693, kurtosis -0.694856 ) 
  b_peephole_i_c_   ( min -0.469378, max 0.326105, mean 0.0032962, stddev 0.106106, skewness -0.164644, kurtosis 1.902 ) 
  b_peephole_f_c_   ( min -0.627952, max 0.755993, mean 0.0164509, stddev 0.186839, skewness 0.598042, kurtosis 3.24216 ) 
  b_peephole_o_c_   ( min -0.594783, max 0.602041, mean -0.0153227, stddev 0.203649, skewness -0.0708237, kurtosis 0.566784 ) 
  b_w_r_m_   ( min -0.426877, max 0.407225, mean -0.000416145, stddev 0.0965273, skewness 0.0011581, kurtosis -0.0150232 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.994223, max 0.803241, mean -0.000155911, stddev 0.109613, skewness 0.00547685, kurtosis 0.0591156 ) , lr-coef 1, max-norm 0
  bias ( min -0.0940893, max 2.61175, mean -4.1211e-09, stddev 0.0865484, skewness 22.7343, kurtosis 655.559 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:336) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -14.8312, max 13.8231, mean 0.00769689, stddev 0.940803, skewness 0.731578, kurtosis 12.6034 ) 
[1] output of <BlstmProjected> ( min -4.35108, max 3.99377, mean 0.00321938, stddev 0.668217, skewness -0.0323113, kurtosis 2.95778 ) 
[2] output of <Tanh> ( min -0.999668, max 0.999321, mean 0.00297646, stddev 0.438571, skewness -0.00470102, kurtosis 0.062753 ) 
[3] output of <AffineTransform> ( min -11.3919, max 23.1914, mean 0.00635968, stddev 2.12673, skewness 1.0719, kurtosis 6.17917 ) 
[4] output of <Softmax> ( min 6.02301e-14, max 0.999954, mean 0.00078108, stddev 0.0201382, skewness 42.308, kurtosis 1869.58 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:338) 
### BACKWARD PROPAGATION BUFFER CONTENT :
[0] diff of <Input>  ( min -2.67027, max 2.61816, mean 0.00181257, stddev 0.190904, skewness -0.0328253, kurtosis 26.1207 ) 
[1] diff-output of <BlstmProjected> ( min -0.654776, max 0.820227, mean -2.96627e-05, stddev 0.0327445, skewness -0.0622835, kurtosis 25.2549 ) 
[2] diff-output of <Tanh> ( min -0.709273, max 0.820246, mean -5.47701e-05, stddev 0.0447094, skewness -0.0564632, kurtosis 14.8233 ) 
[3] diff-output of <AffineTransform> ( min -0.995095, max 0.946406, mean -6.14004e-09, stddev 0.0119224, skewness -27.7618, kurtosis 3923.26 ) 
[4] diff-output of <Softmax> ( min -0.995095, max 0.946406, mean -6.14004e-09, stddev 0.0119224, skewness -27.7618, kurtosis 3923.26 ) 
### END BACKWARD


LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:339) 
### GRADIENT STATS :
Component 1 : <BlstmProjected>, ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  ### Gradients 
  f_w_gifo_x_corr_   ( min -41.1329, max 71.8806, mean 0.0639609, stddev 3.62169, skewness 0.669784, kurtosis 14.9703 ) 
  f_w_gifo_r_corr_   ( min -37.388, max 37.7748, mean -0.0124825, stddev 2.93901, skewness -0.00764079, kurtosis 6.67947 ) 
  f_bias_corr_   ( min -50.8004, max 32.4271, mean -0.578662, stddev 4.87483, skewness -1.17346, kurtosis 16.875 ) 
  f_peephole_i_c_corr_   ( min -40.3735, max 33.8657, mean 0.286665, stddev 6.40115, skewness 0.00641558, kurtosis 11.1492 ) 
  f_peephole_f_c_corr_   ( min -124.6, max 44.5617, mean -1.08372, stddev 14.5391, skewness -3.03007, kurtosis 23.5502 ) 
  f_peephole_o_c_corr_   ( min -67.932, max 62.832, mean -0.227739, stddev 14.5453, skewness -0.315647, kurtosis 6.21643 ) 
  f_w_r_m_corr_   ( min -33.6721, max 33.7751, mean 0.00138468, stddev 4.17619, skewness 0.00964659, kurtosis 2.62179 ) 
  ---
  b_w_gifo_x_corr_   ( min -152.102, max 87.8246, mean -0.0200273, stddev 4.31025, skewness -5.31098, kurtosis 209.935 ) 
  b_w_gifo_r_corr_   ( min -83.4742, max 110.798, mean 0.000515263, stddev 2.96024, skewness -0.0180415, kurtosis 25.1853 ) 
  b_bias_corr_   ( min -70.0971, max 140.989, mean 0.0797857, stddev 7.30172, skewness 5.01528, kurtosis 117.882 ) 
  b_peephole_i_c_corr_   ( min -46.9215, max 31.6427, mean -0.600798, stddev 5.81717, skewness -1.67872, kurtosis 19.2203 ) 
  b_peephole_f_c_corr_   ( min -92.9153, max 106.932, mean -0.378278, stddev 11.8549, skewness 0.931561, kurtosis 31.5055 ) 
  b_peephole_o_c_corr_   ( min -96.7691, max 76.1156, mean -0.817002, stddev 14.2663, skewness -0.385873, kurtosis 12.717 ) 
  b_w_r_m_corr_   ( min -36.0877, max 39.5438, mean 0.0086202, stddev 4.22695, skewness -0.0135047, kurtosis 2.23538 ) 

  ### Activations (mostly after non-linearities)
  YI_FW(0..1)^   ( min 0, max 1, mean 0.317679, stddev 0.354237, skewness 0.765149, kurtosis -0.922832 ) 
  YF_FW(0..1)^   ( min 0, max 1, mean 0.474031, stddev 0.393013, skewness 0.0163716, kurtosis -1.61292 ) 
  YO_FW(0..1)^   ( min 0, max 1, mean 0.27199, stddev 0.345921, skewness 1.04624, kurtosis -0.45189 ) 
  YG_FW(-1..1)   ( min -1, max 1, mean 0.01468, stddev 0.751823, skewness -0.025619, kurtosis -1.39941 ) 
  YC_FW(-R..R)*  ( min -50, max 50, mean 0.202482, stddev 10.0695, skewness 0.241874, kurtosis 17.4073 ) 
  YH_FW(-1..1)   ( min -1, max 1, mean 0.0241673, stddev 0.589605, skewness -0.0322525, kurtosis -0.63539 ) 
  YM_FW(-1..1)   ( min -1, max 1, mean 0.00367522, stddev 0.282352, skewness -0.0438582, kurtosis 4.87467 ) 
  YR_FW(-R..R)   ( min -4.35108, max 3.99377, mean 0.00951783, stddev 0.651258, skewness 0.0393097, kurtosis 2.85084 ) 
  ---
  YI_BW(0..1)^   ( min 0, max 1, mean 0.337594, stddev 0.355706, skewness 0.651903, kurtosis -1.09031 ) 
  YF_BW(0..1)^   ( min 0, max 1, mean 0.475005, stddev 0.376648, skewness -0.0408532, kurtosis -1.52751 ) 
  YO_BW(0..1)^   ( min 0, max 1, mean 0.28033, stddev 0.353063, skewness 0.994017, kurtosis -0.587784 ) 
  YG_BW(-1..1)   ( min -1, max 1, mean 0.00937949, stddev 0.751477, skewness -0.0131975, kurtosis -1.40166 ) 
  YC_BW(-R..R)*  ( min -50, max 50, mean 0.805479, stddev 9.71128, skewness 1.36145, kurtosis 16.8952 ) 
  YH_BW(-1..1)   ( min -1, max 1, mean 0.0289113, stddev 0.596365, skewness -0.0102834, kurtosis -0.693816 ) 
  YM_BW(-1..1)   ( min -0.999999, max 0.999998, mean 0.00144453, stddev 0.288261, skewness -0.0371324, kurtosis 4.35577 ) 
  YR_BW(-R..R)   ( min -4.03335, max 3.89889, mean -0.00311742, stddev 0.680801, skewness -0.0926266, kurtosis 3.08806 ) 

  ### Derivatives (w.r.t. inputs of non-linearities)
  DI_FW^  ( min -1, max 1, mean 0.000132717, stddev 0.0213414, skewness 0.688069, kurtosis 686.172 ) 
  DF_FW^  ( min -1, max 1, mean -4.68243e-05, stddev 0.0165857, skewness -2.80961, kurtosis 770.387 ) 
  DO_FW^  ( min -0.812379, max 0.703303, mean 2.554e-05, stddev 0.0213079, skewness 0.540246, kurtosis 124.826 ) 
  DG_FW   ( min -1, max 1, mean 8.46708e-05, stddev 0.0265604, skewness 6.66491, kurtosis 700.554 ) 
  DC_FW*  ( min -8.95753, max 21.6085, mean 0.00372781, stddev 0.358579, skewness 25.6722, kurtosis 1349.9 ) 
  DH_FW   ( min -3.07484, max 3.67877, mean -9.37334e-05, stddev 0.105148, skewness 0.550021, kurtosis 121.862 ) 
  DM_FW   ( min -4.6641, max 5.37801, mean -0.000623682, stddev 0.251383, skewness -0.0453806, kurtosis 26.6373 ) 
  DR_FW   ( min -1.16282, max 1.13218, mean -0.000191528, stddev 0.0709216, skewness 0.00893779, kurtosis 20.0934 ) 
  ---
  DI_BW^  ( min -0.581187, max 0.605565, mean -6.10286e-05, stddev 0.0130816, skewness -0.0560018, kurtosis 210.089 ) 
  DF_BW^  ( min -0.3961, max 0.688838, mean -8.33149e-05, stddev 0.00947211, skewness 1.66149, kurtosis 314.198 ) 
  DO_BW^  ( min -0.488891, max 0.342375, mean -0.00010416, stddev 0.0127797, skewness -0.568822, kurtosis 76.8032 ) 
  DG_BW   ( min -1, max 1, mean 4.1471e-05, stddev 0.0219369, skewness 0.781915, kurtosis 454.089 ) 
  DC_BW*  ( min -10.7645, max 10.9048, mean 0.000818673, stddev 0.1973, skewness 5.38971, kurtosis 963.408 ) 
  DH_BW   ( min -1.81903, max 1.48034, mean 4.39466e-05, stddev 0.0681406, skewness -0.33093, kurtosis 65.5087 ) 
  DM_BW   ( min -4.02668, max 3.13587, mean -0.00102577, stddev 0.168215, skewness -0.192427, kurtosis 19.0218 ) 
  DR_BW   ( min -0.744649, max 0.901349, mean -0.000180879, stddev 0.0580181, skewness -0.0712906, kurtosis 10.3676 ) 
Component 2 : <Tanh>, 
Component 3 : <AffineTransform>, 
  linearity_grad ( min -58.5878, max 64.0338, mean -6.73841e-08, stddev 1.41173, skewness 0.161911, kurtosis 156.447 ) , lr-coef 1, max-norm 0
  bias_grad ( min -83.0891, max 89.4762, mean -4.17233e-08, stddev 4.65967, skewness -2.52707, kurtosis 252.527 ) , lr-coef 1
Component 4 : <Softmax>, 
### END GRADIENT

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:346) PER-CLASS PERFORMANCE:
@@@ Frames per-class: [ 181212 144 127 606 532 43 173 190 515 703 138 211 211 608 45 289 271 572 372 153 368 892 307 454 467 612 374 177 314 165 203 169 193 120 162 350 506 100 142 40889 89709 154 463 1777 286 231 687 352 545 459 79 394 481 237 200 663 1037 475 2210 428 257 167 4192 1910 93 230 151 223 4363 199 339 612 931 689 168 53 430 425 404 147 72 468 398 184 543 242 103 448 1376 60 779 717 668 146 541 163 358 830 561 408 249 268 103 1105 608 477 208 629 257 136 769 185 359 2082 165 69 303 74 247 336 172 147 117 128 443 612 117 125 52 316 956 165 305 574 202 297 222 1167 208 209 172 556 182 606 267 121 534 323 470 239 776 321 812 179 268 1139 537 191 554 5133 217 193 202 128 264 242 282 141 453 190 390 248 274 208 168 117 250 72 227 832 107 208 74 120 577 200 74 173 93 435 313 308 638 462 201 239 312 172 59 151 330 221 210 68 77 424 248 789 348 126 86 566 208 142 331 485 286 372 234 328 637 273 293 132 41 150 57 102 173 655 328 426 861 178 245 87 533 72 131 265 150 790 182 357 233 69 187 112 206 273 85 62 302 205 411 179 278 260 193 525 723 141 288 139 513 788 215 97 150 236 273 455 166 615 208 63 196 43 120 342 911 206 87 1156 192 146 131 166 31 221 252 417 373 499 401 233 179 409 182 439 579 96 189 251 1812 272 55 134 245 227 120 170 454 69 211 126 232 303 701 89 59 87 90 418 122 334 282 91 119 125 379 59 189 157 163 620 120 203 224 199 71 381 86 544 80 247 220 94 114 76 256 175 1186 420 378 336 135 140 331 666 306 356 196 203 291 152 221 543 214 149 360 365 670 264 264 692 216 616 923 524 408 234 395 301 375 105 525 80 359 98 456 204 309 258 288 286 181 90 101 263 53 253 167 354 150 174 194 393 579 494 437 273 142 236 159 226 115 606 198 255 508 136 282 304 327 234 1467 199 113 213 747 267 237 238 267 223 423 127 334 351 773 400 668 269 762 507 73 172 259 944 420 60 318 817 125 150 155 430 99 155 360 316 425 603 146 124 301 210 170 102 140 531 288 716 282 161 233 181 349 120 133 242 135 51 141 111 147 205 551 427 131 99 109 133 445 7274 204 271 135 128 125 377 120 372 449 808 108 557 45 280 47 596 57 276 400 121 347 315 193 77 146 96 105 216 193 341 1125 236 219 105 417 109 338 639 130 396 177 211 138 121 275 91 104 194 51 542 461 346 197 315 121 109 444 210 64 135 84 52 96 283 268 315 144 187 498 569 470 194 79 151 71 285 409 404 165 215 321 466 69 209 585 117 83 141 150 364 478 225 439 81 192 156 773 344 306 243 112 146 203 501 258 143 117 231 316 156 83 53 468 163 539 465 176 116 79 157 350 139 86 253 155 101 132 342 440 324 189 390 153 111 192 365 208 291 263 103 139 134 307 402 452 451 5943 88 406 111 189 488 786 548 86 632 99 382 216 141 365 388 65 90 187 430 300 463 604 247 79 314 305 463 45 473 718 330 223 226 1312 201 160 62 170 227 141 90 90 193 68 539 610 241 156 182 257 224 114 99 209 178 32 193 174 235 238 150 123 308 261 195 275 142 111 371 821 518 831 306 124 432 87 357 932 174 327 225 300 36 190 272 120 358 461 397 462 306 105 348 166 390 216 520 219 260 71 179 258 429 201 53 142 256 406 60 128 173 26 88 191 83 60 155 179 1000 125 44 191 230 383 687 216 257 321 233 417 202 1061 342 327 163 248 165 412 229 303 517 124 815 539 79 306 155 255 89 321 245 73 308 228 196 192 269 176 228 101 8591 469 337 966 237 430 177 258 334 85 304 414 487 401 57 178 99 260 178 151 248 261 926 126 248 230 264 241 147 220 155 139 646 219 209 393 1097 85 160 936 605 267 175 213 208 155 166 207 194 260 180 542 294 745 650 41 235 409 387 268 281 172 798 467 256 138 171 316 378 542 197 77 199 253 693 506 326 151 543 272 142 171 242 241 191 161 274 304 47 226 320 127 248 43 388 236 233 244 332 111 261 196 246 68 466 707 315 146 315 282 30 427 201 153 510 1026 278 186 316 45 464 106 316 404 415 543 346 257 452 1347 138 153 366 558 585 171 109 855 530 221 155 160 161 111 196 131 341 863 116 777 103 428 210 393 95 260 221 295 129 320 516 185 97 126 343 492 405 270 645 150 141 159 382 1153 426 252 535 332 205 417 575 597 292 372 180 236 213 343 301 154 349 528 290 243 378 616 213 266 230 676 309 338 165 324 314 639 310 148 365 184 119 116 199 264 1367 367 467 1442 213 598 641 335 156 122 351 114 129 98 489 213 162 386 116 74 274 197 265 173 129 167 753 211 214 1711 286 746 207 83 123 191 204 353 783 57 223 151 323 285 99 1197 111 116 189 130 401 488 428 96 280 248 468 506 117 284 315 328 129 334 124 557 265 243 383 268 66 97 474 153 221 139 391 194 634 156 322 748 309 195 326 651 462 370 172 385 490 392 610 304 249 70 501 324 335 228 370 206 587 560 108 850 256 209 153 168 206 131 204 232 340 200 284 174 301 124 107 220 67 370 1390 1253 311 434 127 136 104 493 416 778 103 335 338 242 66 208 459 231 96 291 160 220 368 145 728 458 351 255 350 348 137 125 1039 1466 285 221 232 361 605 549 4192 543 376 361 3105 243 319 117 225 232 542 309 859 113 244 913 235 271 853 881 252 243 274 129 90 371 274 176 453 107 172 140 172 411 267 778 147 558 2509 132 95 258 162 203 87 21 467 192 231 265 665 552 160 225 162 279 3235 297 303 235 317 297 155 692 494 82 179 192 734 178 103 204 87 225 63 351 225 260 282 352 367 219 77 182 384 386 147 340 273 260 234 61 251 141 663 194 320 94 144 ]
@@@ Loss per-class: [ 0.151732 0.537842 0.793128 0.428981 0.573012 0.733624 0.836287 0.556523 0.641823 0.833113 0.422393 0.999201 0.785089 0.571714 0.765051 0.558456 0.653605 0.797397 0.838588 0.559272 0.511576 0.74944 0.736747 0.475313 0.306441 0.622281 0.440681 1.55851 0.66601 1.15931 0.723829 0.365828 0.754104 0.980128 0.43619 0.464941 1.19797 1.14638 0.807948 0.41109 0.228759 0.803045 0.845883 0.580883 0.771459 0.691546 0.376258 1.02227 0.407346 0.408773 0.723547 1.08319 0.835884 0.999504 0.576169 0.648114 0.405439 0.378885 0.320436 0.992666 0.871469 0.714316 0.211902 0.282733 1.05794 1.1016 1.05616 0.896568 0.705518 0.442903 0.54024 0.375152 0.346058 0.794456 0.686878 0.788904 0.270444 0.197273 0.385184 0.452946 0.784075 0.808031 0.224362 0.54024 0.641428 0.818465 1.02179 0.875714 0.596284 0.439904 0.364172 0.362407 0.578905 1.43018 1.01318 1.54717 0.514884 0.368019 0.535428 0.30449 1.34847 0.642037 1.13722 0.334054 0.370177 0.53114 1.12207 0.681602 0.610485 0.59921 0.731495 0.630015 1.32983 0.335501 0.985177 2.06831 1.00639 0.585462 1.35905 0.866819 0.892854 1.13687 1.01222 0.864708 0.475354 0.436496 0.597801 1.31753 0.507628 0.449293 0.212596 0.837403 0.439413 0.621277 1.37965 0.427729 0.488318 0.289128 0.892656 1.02758 0.975228 0.661218 0.623435 0.564708 0.451435 0.333915 0.917618 0.592062 0.549523 0.446365 0.17563 0.68076 0.584666 1.03812 0.907478 0.483656 0.280392 0.458131 0.597504 0.294294 0.472861 0.876059 1.00709 2.10558 0.664851 1.08831 0.616243 0.89635 0.508935 1.2385 0.397419 0.644169 0.573358 0.752452 0.923792 0.378411 0.608769 0.51415 1.31449 0.242758 0.957589 0.905447 0.699667 0.682022 0.182399 0.897636 1.0972 1.02587 1.04006 0.533328 0.529831 0.792554 0.315256 0.477479 0.64843 0.814533 1.22298 0.999599 0.830192 0.388371 1.43212 1.22702 0.961122 1.56225 1.33686 0.69381 1.42891 0.734655 0.705414 0.934328 0.783549 0.94452 0.583929 11.4765 1.20616 0.52273 1.0174 0.474817 0.490999 1.05607 0.606583 0.458679 1.38379 0.533418 0.793673 1.05883 1.50884 0.435306 0.614682 0.595385 0.813326 0.600037 0.577467 1.89287 0.591151 1.41104 0.423784 1.77259 0.769862 0.813877 0.609718 0.367424 0.663962 0.599014 0.961778 1.82869 0.473658 3.06186 1.28841 0.972642 1.7361 0.82273 0.958064 0.963419 0.924246 0.590351 1.68512 0.48713 0.524493 0.390718 0.667716 1.03093 0.87137 2.0609 0.599232 0.520448 1.34106 0.736818 1.01513 0.896407 0.81194 0.730742 1.55348 1.24075 1.7416 0.940365 0.567358 1.7707 0.687202 1.43875 0.740689 1.04185 0.852197 0.465961 0.552149 1.23472 1.29795 0.7527 4.15112 1.7292 1.19352 1.381 0.417856 0.259372 1.47478 0.798063 0.724718 0.557392 0.76211 1.00864 0.495147 0.572193 0.787023 0.676871 0.221708 0.476708 1.20851 0.68536 1.05029 1.52371 0.877407 0.86986 0.691656 0.703378 1.14047 1.41167 0.25114 0.949468 0.351442 1.09539 1.44039 0.441117 1.66193 0.418986 1.09833 1.87936 1.17223 1.73034 0.941704 0.708811 0.660646 0.964361 1.1612 0.77068 0.926618 0.396539 1.38429 1.17235 0.94423 0.498292 0.807091 0.76338 0.419354 0.729706 0.863295 0.661884 0.889461 0.949229 0.594968 0.778611 0.781344 1.00063 0.247137 0.166032 0.91349 0.764309 0.73595 0.952663 1.25603 0.295756 0.532137 1.39041 0.80783 2.00017 0.439501 1.28603 0.312296 0.891103 0.505658 0.673425 1.17816 1.16978 0.54707 0.871962 0.736016 0.377342 0.565261 0.514308 0.517243 0.836283 0.882309 0.850833 0.388685 0.633699 0.73919 0.745913 0.714299 0.989032 0.753562 1.08379 0.703361 0.948368 0.844401 0.551585 0.317649 0.746077 1.04897 0.556357 0.974682 0.46004 1.63751 0.411387 0.660942 0.518462 1.34446 0.820542 0.726376 0.921937 0.813137 0.974156 1.74125 1.50322 0.692326 0.748188 0.995638 0.499054 0.837116 0.379074 1.80682 0.656369 0.308 0.4175 0.458502 0.490828 0.649306 0.953721 0.17541 0.409485 1.32569 0.503016 0.547757 1.401 1.1421 1.05626 0.691269 0.44997 0.518464 0.628146 0.659446 0.723701 0.379192 1.20662 0.337365 1.0997 0.806191 0.551154 1.27913 0.651636 0.778935 0.239166 0.746291 0.949462 0.881384 0.367534 0.438759 2.54824 0.26989 0.711729 0.706908 0.841147 0.504822 0.772044 1.19408 0.638581 0.724427 1.56831 1.07313 0.769662 0.838422 0.878513 0.243015 0.318679 0.69695 0.460831 0.73421 0.369623 0.465654 0.557401 0.747888 0.729157 2.31101 1.13811 0.586579 0.883662 0.715263 0.295265 1.00648 0.802257 0.275995 0.956448 0.798793 0.879014 1.36681 1.44497 0.534254 0.408882 0.515609 0.769572 1.72423 0.390257 1.20217 0.610718 0.93698 1.17738 0.309277 0.707973 1.53648 0.448662 1.45854 0.710434 1.86696 0.386681 1.15213 0.673963 0.692371 0.956649 1.13735 0.6214 1.53815 0.932379 1.59813 1.05063 0.816738 2.01392 1.28687 0.561492 0.283981 1.15917 0.931949 1.77711 1.52776 1.57331 0.927032 0.812723 0.968066 1.39327 0.745693 0.842605 0.713953 1.43236 0.96328 0.886426 0.95604 1.03682 1.17094 0.122808 0.588052 0.942303 0.62201 0.796497 1.16007 1.88647 0.779285 0.475716 1.16477 0.716955 1.01719 1.40101 1.39255 1.32087 1.66744 1.02236 1.05263 0.764858 0.438987 0.500253 0.389758 0.647106 1.24224 1.02273 0.705054 0.805798 0.597503 0.556895 1.05988 1.56397 0.714999 0.64788 0.987371 0.774241 0.65311 1.45515 0.996041 2.02994 0.89394 0.843429 0.442709 1.14277 0.856957 0.934357 0.577115 0.627333 0.453999 0.411184 1.30505 1.54373 1.70276 1.01037 1.53214 0.73419 1.26362 1.59026 1.51634 1.66055 0.875082 0.444685 0.489815 1.24922 0.318837 1.0709 0.688456 0.654695 0.841456 0.505274 0.683256 1.12974 1.56376 0.645247 0.777714 1.58188 1.1234 0.97826 1.06875 0.581388 0.76845 0.580777 0.89477 1.25072 0.621165 0.970575 1.62591 1.91388 0.827299 1.09414 0.494812 0.556798 0.392448 1.06183 0.835543 0.989135 0.493275 0.631752 0.412458 0.562583 0.607816 0.222395 1.42679 0.714749 0.509507 0.561784 1.97209 0.393953 0.678514 0.543416 1.19854 1.19181 0.602376 0.80006 2.88303 0.977512 0.646179 0.847658 0.382947 0.49925 0.579219 0.93513 1.04741 0.55862 1.24713 0.467925 3.38352 0.682368 0.395592 0.726691 1.61128 1.67625 0.618337 0.749849 1.4441 1.47568 1.31515 0.59973 0.776106 0.77679 0.630426 1.04442 1.31595 0.584912 0.550538 1.22603 1.3251 0.684426 0.99627 0.484149 1.02885 0.86017 1.24774 1.91636 1.79131 0.910248 0.53633 0.433685 0.625512 0.868105 0.897192 0.909364 0.842372 0.85506 0.488651 0.815826 2.41469 0.472893 0.268517 0.525461 0.26775 0.558944 0.753742 0.652528 0.438148 2.10637 1.29643 0.682748 0.767975 0.701169 0.827144 1.79154 1.23481 1.29864 0.890361 0.521896 0.672432 0.472697 0.894762 1.26472 0.758561 0.841622 1.19173 0.866517 1.23245 0.311538 0.992278 0.760071 0.66181 0.523806 0.814296 0.614809 0.732316 2.08286 0.709458 1.17398 0.821036 0.977775 1.38612 0.954842 1.03971 1.92922 1.11215 2.21358 1.33416 0.905535 1.11726 0.317702 1.40313 1.44619 1.71586 1.05613 0.981611 0.342231 1.07977 0.958703 0.761796 0.786661 0.75121 0.59639 0.837198 1.02789 1.63257 1.95766 0.524834 0.485793 0.602537 0.539111 1.05805 0.710321 1.65756 0.27232 0.484944 1.19442 0.877929 0.507436 0.75326 2.24378 0.535437 0.680112 2.08615 0.36781 1.0665 1.43097 0.937303 0.69288 1.45907 3.35765 0.965388 0.14852 0.38375 0.809099 0.432669 1.53618 0.783496 1.61294 0.696826 0.404452 2.7511 0.667425 0.802572 0.823872 1.05493 2.19856 1.68048 1.82006 0.437064 0.718776 1.15214 1.24698 0.750958 0.625873 1.97849 1.04713 0.999259 0.858224 0.756111 2.04089 0.983824 0.668476 1.30273 0.666509 0.863695 1.11571 0.793738 0.462946 2.00441 0.969845 0.488129 0.622175 0.730743 0.673394 1.24444 1.14857 1.1266 0.875793 1.40483 1.19663 1.70522 1.16694 0.728089 0.901575 0.327716 0.706112 1.35341 0.873114 1.16694 0.597489 0.725786 0.649605 1.45203 0.641973 0.753712 1.05713 1.2503 0.389985 0.937758 1.01848 0.588531 0.968502 1.46582 1.49715 0.85784 0.484997 0.655051 1.05957 0.604736 0.478319 0.549652 1.86537 1.33544 0.953291 0.829312 1.36124 0.511236 0.787282 0.610633 0.45337 0.887954 0.698015 1.17013 0.47282 0.777383 0.667887 0.888618 1.0678 1.11145 0.743224 1.31884 0.683906 1.35319 0.589351 0.862913 0.493339 0.412284 0.785866 1.78754 0.660639 0.668267 2.39079 0.728536 1.48122 0.955297 0.552856 0.894857 0.983493 0.498584 1.2508 2.56768 1.24406 1.03579 0.887834 0.635851 1.04757 1.39622 1.47418 0.391726 0.617886 0.724433 1.239 0.993463 0.605175 1.11735 0.951753 1.1417 1.47782 0.732711 0.547208 1.0431 1.37985 0.950582 1.12663 1.34713 0.703401 1.57603 0.68646 0.614197 0.53181 0.627057 0.993104 0.532058 0.546958 0.84674 0.787246 1.25019 0.885881 1.40876 1.12115 1.77158 0.78635 1.13045 0.682709 2.06973 0.781248 0.667317 0.761692 0.81615 0.673988 1.01446 1.28592 1.08753 1.40821 0.65901 0.85502 1.21509 0.714561 0.425288 0.766625 0.447111 0.464252 0.960324 0.700637 0.5347 0.847907 0.95307 0.771414 0.712163 0.708449 1.10154 0.867934 0.819153 0.486052 0.704212 0.691925 0.700398 0.788626 1.32289 0.905626 0.504415 0.608006 0.730683 0.806779 0.578735 0.872127 0.781234 1.58775 1.49641 0.95518 0.714171 0.57134 1.02671 1.18791 1.2594 0.67991 1.31608 0.962855 0.413037 0.515075 0.765273 0.352326 0.726798 0.510375 1.90175 1.18124 0.704704 1.41948 1.51673 0.660088 0.975053 0.852237 1.19417 1.72333 1.47043 1.15316 1.30998 0.860924 1.17487 1.19046 1.11891 1.29364 0.361511 1.51621 0.525243 1.59136 0.563137 1.5808 1.00629 1.47411 1.32869 1.28804 1.17771 0.280884 1.51372 0.536571 1.19426 0.96906 1.09022 1.9614 0.521005 1.32826 1.13561 0.705826 0.899523 0.587933 0.836836 0.395932 1.6802 0.825218 0.961891 0.948457 0.578798 1.32344 0.790171 0.888331 0.92186 1.43848 0.84936 1.43856 1.49555 1.33806 0.850441 0.484212 0.515331 1.04804 1.55288 0.519009 1.24317 1.44568 1.3736 0.874342 0.984595 0.668727 1.13557 0.390639 0.643478 0.670672 0.839474 0.975408 0.732561 0.904316 0.590639 1.27573 0.753242 1.20772 0.428962 0.46083 0.804962 1.54999 1.83154 1.20893 1.1439 1.26375 0.903452 0.506838 1.47375 0.529273 0.719051 0.714197 0.859948 1.41811 0.757001 0.848409 0.31803 0.911171 1.62852 0.41942 0.874807 0.949792 1.21162 0.836301 1.12959 0.758701 1.25029 2.03834 1.10254 1.15631 1.1323 0.225044 0.218949 0.484927 1.05983 1.21119 1.40172 0.946422 0.947906 1.0732 0.538787 0.879525 0.783141 0.644199 0.595941 2.33073 0.943813 0.525415 0.618625 1.01704 1.19132 1.15854 0.709725 1.67767 1.18621 0.579036 0.568505 0.387367 0.866397 0.794682 0.397867 0.669964 0.732328 0.658961 0.407603 1.20436 0.673811 1.0135 0.443829 0.607797 0.725884 0.343402 1.14173 0.981805 0.687964 0.373647 1.34641 0.850639 0.95241 1.08194 0.733454 1.18179 1.3331 0.271395 1.89761 0.265915 0.571811 0.924007 0.777099 1.0744 0.385406 0.667948 1.19873 1.32603 0.825528 2.07178 1.22275 0.633364 0.990494 1.12841 1.37651 1.12209 1.73984 0.60845 1.50197 1.60149 0.916065 0.896189 0.87595 0.457844 0.989012 1.04179 1.40991 0.557591 1.09211 1.14444 3.4834 0.888748 0.679085 1.21904 1.5086 0.890977 1.4176 1.58225 0.707293 0.565063 1.0257 0.463963 0.704883 1.29368 0.797805 1.17039 2.01191 1.12223 0.748958 1.09899 1.50222 1.66736 1.32682 0.46729 0.987766 1.13915 1.12723 1.85873 0.774929 2.0722 1.02813 1.09131 0.772833 1.01212 0.457399 0.802685 0.590532 1.62787 0.59824 0.434596 0.854839 0.772781 0.935314 0.756433 1.41441 0.581583 0.661634 0.699091 1.04821 0.533904 0.844811 1.28409 0.881173 0.97285 ]
@@@ Frame-accuracy per-class: [ 94.7208 85.8131 75.2941 89.3652 82.4413 75.8621 77.8098 81.3648 76.2367 69.936 88.8087 72.8132 77.5414 85.1274 85.7143 85.6649 81.0313 76.1572 70.604 83.3876 85.7531 73.6134 76.4228 86.0286 93.0481 81.7959 88.6515 37.7465 82.3529 63.4441 78.1327 85.5457 79.5866 62.2407 85.5385 84.1655 60.612 65.6716 70.8772 85.2835 91.8699 75.7282 72.4919 79.6062 70.8551 77.7538 86.5455 64.3972 86.8928 86.8335 81.761 62.6109 72.4818 66.5263 84.788 79.1258 84.4337 88.5384 91.5178 63.4772 72.233 77.0149 93.0233 90.4999 67.3797 55.9653 67.9868 73.3781 80.6692 85.7143 82.7688 87.8367 90.7139 72.9514 81.3056 84.1121 91.0569 95.8872 88.2571 96.9492 78.6207 74.2796 93.3501 85.0949 87.2125 72.9897 73.43 74.6934 81.2205 94.2149 88.39 90.8711 81.2266 51.1945 69.4367 53.211 87.0293 89.3438 83.7044 92.0441 52.1042 83.7989 58.9372 90.2759 89.7288 81.0471 68.5851 79.4281 81.165 80.5861 74.334 85.1752 63.6996 89.5078 65.2568 34.5324 65.5684 89.9329 55.7576 73.997 77.6812 75.9322 70.6383 69.2607 84.1037 84.2449 88.5106 64.5418 93.3333 89.0995 92.5248 76.1329 82.1604 80.4178 57.284 87.395 84.9438 90.5353 75.2998 69.6897 65.5072 77.4483 81.0959 80.4617 90.0935 93.8272 70.159 82.8439 80.3401 84.7599 95.9433 80.5599 84.0615 68.5237 73.3706 83.3699 92.093 85.6397 81.5149 90.1529 84.5977 77.5194 66.1728 28.0156 78.6389 68.4536 82.1239 74.9117 82.9107 54.0682 86.2996 80.4829 81.2386 80.5755 71.2166 88.5106 82.6347 82.7586 62.8571 92.8529 70.6977 66.6667 81.8792 78.8382 94.8918 66.8329 69.7987 67.4352 66.3102 83.5821 84.5295 75.8509 90.9945 85.8378 79.4045 75.1566 52.48 68.4058 82.3529 87.7888 53.5552 55.5305 69.8337 51.0949 58.0645 78.6808 45.4728 77.7707 77.7618 71.9368 82.0809 63.7246 82.494 0 61.2368 85.8908 66.3176 88.0537 83.5821 70.0152 79.6863 85.192 62.6917 83.0189 81.9277 63.7874 38.2609 95.6098 81.268 79.0236 76.4079 78.3118 83.6912 41.4566 83.9104 53.7143 87.3477 37.2414 71.4829 72.3164 84.3854 86.7805 81.0959 82.2378 68.5225 30.2158 88 28.4444 58.1114 69.8355 51.462 83.2 71.7355 73.9659 74.3621 87.4652 58.5278 82.9175 85.7881 91.1513 80.0276 67.8445 77.2964 52.3297 81.2074 84.4642 59.8608 87.1795 55.814 78.2241 75.3199 72.4479 48.6486 54.9147 47.482 70.8661 87.0229 45.977 79.668 56.0584 78.6615 65.8596 62.8571 86.4678 83.1169 66.2116 66.9202 78.6787 9.52381 56.4334 62.9703 59.1617 88.8889 92.8929 53.3001 74.0899 79.6657 82.5397 80 69.8521 86.1087 87.0466 72.8232 75.9443 92.9103 89.5413 72.0721 81.0409 72.9124 47.033 76.3485 69.7947 77.4477 79.1367 69.9764 60.0791 95.0538 74.4646 90.948 71.5084 53.7815 85.7143 57.4586 88.8889 54.6939 39.7608 58.7611 52.459 69.4561 84.4622 79.0514 57.1429 66.4908 78.7302 72.1713 84.9315 58.9212 69.7789 73.9421 88.2206 79.7203 73.9187 86.7052 78.9715 70.8075 81.6162 70.7483 70.8995 85.5895 82.3529 75.6335 66.0969 92.8782 95.3627 73.9762 76.9688 80.4428 77.5801 68.4766 91.0728 85.155 63.9551 74.3003 38.8206 88.8508 54.4262 93.0023 73.4131 85.3147 80.9365 63.2455 63.7483 82.4758 76.3705 77.8828 88.3755 81.7552 83.0495 83.3785 74.7378 72.4602 68.2303 90.7712 79.2703 75.6325 75.8294 78.7821 67.0807 77.3296 67.0051 78.4228 68.4597 79.483 82.0116 92.8943 79.5812 69.4215 87.2928 69.9507 87.666 50.4673 91.5187 81.791 87.4471 61.1296 78.51 74.5501 68.1067 77.6531 68.1496 50.7429 55.9415 80 79.0698 69.5925 89.1832 77.0563 89.3652 41.3098 79.4521 90.8555 85.7143 83.8938 88.3415 83.6641 70.7889 95.7411 87.218 57.2687 85.7143 82.5418 56.8224 65.2632 69.1824 79.6262 84.5638 89.2562 83.1373 83.4081 77.3826 88.0414 67.4157 89.454 67.1614 76.9836 85.1232 43.5374 80 80.9249 92.4299 75.8621 61.157 79.1209 88.0734 90.8367 13.9535 93.8907 81.7654 77.3869 77.1704 83.7725 75.1975 55.9342 80.0331 79.8635 51.4056 68.325 79.3349 73.9003 76.0976 95.3737 91.8156 75.5633 88.2066 75.0442 87.9257 83.0835 84.2975 76.1087 79.668 43.4457 69.2784 81.1808 79.6117 83.3922 93.2735 71.8644 81.2652 92.4751 68.7719 74.5247 73.3668 59.3607 56.9288 83.9506 84.4044 86.0636 80.663 51.6605 87.9377 55.7769 78.1457 68.8797 68.7248 90.99 78.9116 53.4562 86.6368 54.9451 77.0053 61.0526 88.8516 59.1304 75.226 75.6554 72.428 67.3381 76.7036 50.646 72.2581 54.6075 69.4301 76.7773 54.5035 59.4315 82.8697 90.5375 63.8478 76.9932 59.7156 52.2156 46.5753 72.0827 71.3057 68.1992 67.5914 78.3099 73.2861 78.7004 42.7984 65.6987 73.224 72.7273 69.9229 77.6699 97.1429 82.1235 72.1501 85.0633 80.8241 60.0823 42.0091 78.7402 88.8361 62.0155 77.4908 67.4556 57.1429 55.9585 59.612 45.0652 68.1458 69.8962 81.6 85.0552 84.6356 89.4793 80.2057 56.6038 76.5677 88.1119 73.9054 81.3187 82.3239 66.4653 59.3968 79.0047 81.2433 71.9424 76.8496 76.345 57.0213 69.4611 33.9223 67.7741 73.5254 82.1317 62.0843 75.3129 79.7546 88.8312 81.7891 84.0336 89.1147 60.6852 47.2279 58.6667 70.3072 59.9509 77.1685 66.5377 60.6272 54.4681 49.2441 72.3539 88.8179 88.6228 54.2056 88.3671 73.3945 79.3327 85.0698 70.255 90.1288 76.7296 66.0317 49.0728 88.172 72.8324 52.86 57.2347 76.8473 74.717 80.8759 76.731 83.5131 69.1293 60.1793 78.1759 78.9238 44.6753 47.0588 80.5755 65.8662 84.63 86.9565 92.4731 69.8885 75.7724 75.528 86.6298 80.1772 85.2864 88.1356 76.7528 96.861 57.5198 79.6315 85.6961 83.1358 21.9653 86.3241 74.3719 84.9673 63.2794 57.2438 85.3625 75.1609 16.7939 65.1934 74.6667 72.0093 88.1864 83.0636 81.555 73.9394 76.7296 85.5326 59.9018 85.8684 21.978 78.3527 88.6569 71.7095 46.9799 52.0971 79.7714 77.9156 47.9751 54.4 56.8915 84.3956 79.8587 80.663 79.558 62.0155 64.2336 84.5227 82.7191 55.0725 61.9808 78.9041 64.8544 88.196 68.1223 69.3467 53.9379 35.8543 36.9231 68.7339 86.533 87.0488 81.3417 74.4186 70.4453 69.3679 74.5698 75.7033 84.2105 75.7895 30.4933 85.5989 93.8527 85.053 93.0848 85.155 80.3213 80.6936 91.4286 43.6364 58.9812 87.6791 75.4198 83.3703 69.218 21.9178 64.042 66.422 72.1992 81.1715 78.2232 84.5283 74.3784 67.863 82.4645 74.0316 62.4625 73.7516 60.5081 95.4851 71.0706 76.0077 79.7203 86.351 73.8878 77.9977 79.9007 31.7757 76.4912 69.7856 78.2288 82.6446 56.8093 76.0807 60.3774 59.887 56.9191 37.1257 54.5455 70.0965 67.4095 90.3548 46.2151 56.1798 34.9869 62.4729 70.4042 89.0182 63.2794 71.8447 79.0047 76.6595 77.1257 84.9383 72.0678 74.7445 40.3053 42.2018 84.507 88.2175 83.1515 85.4031 73.8056 80.3865 58.6345 92.8265 86.5616 59.1195 77.6509 84.2444 75.5382 52.514 83.0482 81.0591 39.4558 91.7342 70.0219 55.9796 73.7662 78.2931 52.6912 37.6368 75.8621 94.803 87.5399 72.2963 88.9809 55.1579 81.0685 51.2676 79.3037 87.8924 29.2398 79.1461 68.7575 77.1282 65.2553 41.7391 50.9804 29.1457 83.3013 80.6723 67.9868 61.9718 78.7763 78.0356 45.0593 68.008 72.885 74.4802 79.089 46.1017 73.0159 80.3859 56.6308 76.2568 70.615 72.0764 75.4765 87.2893 33.9181 73.5202 83.8227 84.0628 74.3925 79.7721 67.4473 64.7482 59.8071 71.4715 60.241 63.2391 48.7524 58.1717 77.9724 74.0238 89.4702 77.1714 53.012 75.1592 64.2247 78.7097 76.3501 83.1261 56.2319 76.6437 78.9305 70.1754 62.8159 91.5452 72.3539 71.07 78.5253 60.2532 68.3871 59.1479 71.7949 86.9503 80.5528 66.1562 81.8482 89.9724 85.1376 48.4211 54.8105 71.7526 72.4638 57.9634 87.3065 77.5956 77.8325 88.4211 75.9382 81.7473 61.1765 84.507 85.0575 77.4775 76.5328 65.0964 65.4397 74.5865 61.8834 79.9235 54.9618 83.9757 80.292 85.5305 88.6219 77.0206 31.3993 81.141 81.0619 45.9016 79.0643 59.0571 75.57 81.6846 72.9664 68.9408 90.0804 67.6145 10.989 62.2174 76.0563 73.9336 83.3127 62.0939 55.5658 60.0289 91.6505 76.4641 76.6605 60.6498 76.2215 84.3111 67.5022 68.3177 65.8892 50.2283 75.3945 86.8992 70.8804 53.3762 72.8972 67.4923 66.3677 82.4427 42.5856 79.9414 82.6867 84.9785 80 68.599 85.881 81.2352 76.2389 81.6754 63.7236 72.2348 52.1151 65.6371 50.546 79.7677 66.8464 75.8974 41.1067 73.9447 78.7817 76.4488 75.0462 80.5577 65.7807 60.7774 63.3229 53.5948 81.8379 70.8089 66.1386 75.4435 89.9248 77.8589 82.8743 82.7107 74.6444 76.2393 81.6107 74.7922 69.3446 75.4098 79.476 79.9337 67.3139 75.2504 71.5232 85.3701 78.8501 78.996 76.7234 76.815 54.0338 71.5835 85.8832 82.0679 77.1049 77.9456 84.1294 72.814 77.4042 55.3945 60.6061 70.041 78.0488 82.0084 77.2532 60.6516 60.4915 77.9525 63.1293 73.7968 84.1594 88.0562 75.0209 90.8807 80.4769 85.623 52.2449 68.5633 80.3493 52.5097 45.6853 79.4688 71.6628 79.3846 67.7878 42.9185 53.6913 67.3953 61.2658 68.9266 62.2478 62.5483 67.4627 63.4373 90.3073 51.7483 81.3322 56.1955 83.858 58.3133 68.2635 52.6316 57.4413 64.5477 63.0835 93.2993 40 84.5638 63.3663 68.0062 69.0018 38.191 84.8434 61.8834 62.6609 83.905 75.8621 82.6899 79.0174 88.4481 46.6321 76.6488 72.0322 64.0342 80.7502 52.766 78.0316 71.6323 75.4947 57.9151 72.6457 57.8313 53.6323 63.2768 75.5647 86.8318 85.6611 60.1504 53.3333 85.1423 59.9349 48.307 54.4803 72.2861 69.9229 84.1608 65.1757 92.7132 82.1643 76.5751 80.8184 64.6248 75.6715 74.5946 87.1795 62.6087 72.8923 62.7931 88.9172 87.4693 81.445 59.7194 22.695 63.2104 68.7211 63.1893 77.0241 83.1309 44.0678 85.9574 75.8252 78.341 75.3674 51.462 69.6897 70.3583 93.1751 68.2809 48.6692 90.4645 75.6989 71.6593 65.3367 72.0562 64.7564 73.6318 61.8474 44.6512 66.6667 72.5926 67.2065 94.9299 95.1735 86.9984 60.7595 68.2353 51.2821 68.8995 67.2746 63.8655 84.1362 75.3623 75.4098 80.0591 82.8866 27.0677 72.4221 84.0044 86.3931 67.3575 65.5232 74.7664 78.0045 49.9322 65.9794 81.8119 82.0065 89.6159 70.0587 79.8859 90.1004 79.2727 77.2908 82.6359 86.8735 69.352 78.1038 72.2581 87.9668 81.5855 78.071 89.2069 59.0616 72.2444 75.7953 86.8459 62.0123 75.7433 76.5957 68.2927 78.2796 60.6452 60.7431 93.3101 40.5286 92.8425 83.4154 70.4883 78.453 69.7129 88.4855 76.0396 69.8152 57.1949 77.9923 34.2541 65.9489 82.6958 71.3881 61.742 60.4651 61.4493 45.5516 77.1014 42.7704 49.3458 73.7315 74.5763 72.3366 85.1564 72.4528 72.2513 61.5087 83.6923 59.4595 66.2857 0 77.0053 83.1169 65.6587 53.484 69.2712 55.3846 44.8598 79.3792 87.3846 65.1163 85.0873 79.6639 57.9901 80.2548 67.0866 49.4118 68.8103 79.5668 59.8584 59.3939 57.3816 52.987 86.1811 67.7871 64.7343 66.5037 33.1429 77.1619 34.6457 72.2617 70.0665 80.6142 69.7345 85.1064 76.4626 82.4601 50.3226 80 89.2068 73.9974 73.8983 72.8341 70.5667 51.8234 86.1407 79.6748 77.9324 65.0177 83.9488 73.5219 61.1544 77.2487 76.8166 ]

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:347) Done 2622 files, 1 with no tgt_mats, 0 with other errors. [TRAINING, 0.343851 min, fps36598.4]
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:353) AvgLoss: 0.513064 (Xent), [AvgXent: 0.513064, AvgTargetEnt: 0]
progress: []
FRAME_ACCURACY >> 84.0861% <<

