speech-HP-Z2-Tower-G9
nnet-train-multistream-perutt --cross-validate=true --randomize=false --verbose=0 --num-streams=10 --max-frames=15000 --feature-transform=exp_FG/blstm4i/final.feature_transform 'ark:copy-feats scp:exp_FG/blstm4i/cv.scp ark:- | apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_cv10/utt2spk scp:data-fbank/train_cv10/cmvn.scp ark:- ark:- | add-deltas --delta-order=2 ark:- ark:- |' 'ark:ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- |' exp_FG/blstm4i/nnet/nnet_iter08 
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:243) Not in compute-exclusive mode.  Suggestion: use 'nvidia-smi -c 3' to set compute exclusive mode
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:438) Selecting from 1 GPUs
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:453) cudaSetDevice(0): NVIDIA RTX A2000 12GB	free:11620M, used:410M, total:12031M, free/total:0.96589
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:501) Device: 0, mem_ratio: 0.96589
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:382) Trying to select device: 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:511) Success selecting device 0 free mem ratio: 0.96589
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:FinalizeActiveGpu():cu-device.cc:338) The active GPU is [0]: NVIDIA RTX A2000 12GB	free:11106M, used:924M, total:12031M, free/total:0.923168 version 8.6
copy-feats scp:exp_FG/blstm4i/cv.scp ark:- 
apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_cv10/utt2spk scp:data-fbank/train_cv10/cmvn.scp ark:- ark:- 
add-deltas --delta-order=2 ark:- ark:- 
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:144) CROSS-VALIDATION STARTED
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Read():nnet/nnet-matrix-buffer.h:191) Read() started... Buffer size in MB: 0, max 3072, having 0 utterances.
ali-to-post ark:- ark:- 
ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl 'ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |' ark:- 
LOG (copy-feats[5.5.1074~1-71f3]:main():copy-feats.cc:143) Copied 296 feature matrices.
LOG (apply-cmvn[5.5.1074~1-71f3]:main():apply-cmvn.cc:159) Applied cepstral mean and variance normalization to 296 utterances, errors on 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:303) ### After 0 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:304) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.753568, max 0.906113, mean 0.00394668, stddev 0.0902404, skewness 0.0115425, kurtosis 1.15702 ) 
  f_w_gifo_r_   ( min -0.421162, max 0.415142, mean -0.000591186, stddev 0.0792715, skewness -0.0031913, kurtosis 0.0371892 ) 
  f_bias_   ( min -0.349376, max 1.4383, mean 0.211131, stddev 0.465931, skewness 1.05943, kurtosis -0.647199 ) 
  f_peephole_i_c_   ( min -0.687176, max 0.491791, mean -0.00526809, stddev 0.135964, skewness -0.105526, kurtosis 2.50235 ) 
  f_peephole_f_c_   ( min -0.713593, max 0.972982, mean 0.00591965, stddev 0.182462, skewness 0.396163, kurtosis 4.62677 ) 
  f_peephole_o_c_   ( min -0.597303, max 0.506818, mean -0.0126943, stddev 0.196616, skewness 0.0679237, kurtosis -0.154991 ) 
  f_w_r_m_   ( min -0.507964, max 0.510147, mean 0.000573597, stddev 0.105838, skewness 0.00223551, kurtosis 0.0302627 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -1.70022, max 1.1857, mean 0.00603198, stddev 0.0981901, skewness -0.178156, kurtosis 5.58222 ) 
  b_w_gifo_r_   ( min -0.349673, max 0.374727, mean -0.000185871, stddev 0.0731482, skewness 0.000343579, kurtosis -0.205115 ) 
  b_bias_   ( min -0.388245, max 1.23297, mean 0.203728, stddev 0.455276, skewness 1.02718, kurtosis -0.695317 ) 
  b_peephole_i_c_   ( min -0.4486, max 0.318392, mean 0.00373275, stddev 0.105013, skewness -0.179622, kurtosis 1.72284 ) 
  b_peephole_f_c_   ( min -0.646684, max 0.759745, mean 0.014208, stddev 0.185454, skewness 0.576766, kurtosis 3.27424 ) 
  b_peephole_o_c_   ( min -0.599034, max 0.61779, mean -0.0155737, stddev 0.203541, skewness -0.0627148, kurtosis 0.522262 ) 
  b_w_r_m_   ( min -0.423328, max 0.401488, mean -0.000398769, stddev 0.0958159, skewness 0.00115021, kurtosis -0.0204606 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.992197, max 0.778834, mean -0.000155913, stddev 0.10923, skewness 0.0054761, kurtosis 0.0578189 ) , lr-coef 1, max-norm 0
  bias ( min -0.0940014, max 2.54685, mean -3.67872e-09, stddev 0.0848047, skewness 22.4854, kurtosis 643.798 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:305) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -11.004, max 12.2048, mean 0.00682255, stddev 0.969149, skewness 0.287893, kurtosis 3.83541 ) 
[1] output of <BlstmProjected> ( min -4.21886, max 4.68849, mean -0.000157063, stddev 0.763244, skewness -0.00675081, kurtosis 0.924896 ) 
[2] output of <Tanh> ( min -0.999567, max 0.999831, mean 0.000186979, stddev 0.517112, skewness -0.00392529, kurtosis -0.847634 ) 
[3] output of <AffineTransform> ( min -12.8399, max 20.355, mean 0.00611697, stddev 2.46873, skewness 0.687826, kurtosis 2.24965 ) 
[4] output of <Softmax> ( min 8.80717e-13, max 0.999582, mean 0.000780023, stddev 0.0180342, skewness 39.5307, kurtosis 1762.47 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:334) ### After 79212 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:335) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.753568, max 0.906113, mean 0.00394668, stddev 0.0902404, skewness 0.0115425, kurtosis 1.15702 ) 
  f_w_gifo_r_   ( min -0.421162, max 0.415142, mean -0.000591186, stddev 0.0792715, skewness -0.0031913, kurtosis 0.0371892 ) 
  f_bias_   ( min -0.349376, max 1.4383, mean 0.211131, stddev 0.465931, skewness 1.05943, kurtosis -0.647199 ) 
  f_peephole_i_c_   ( min -0.687176, max 0.491791, mean -0.00526809, stddev 0.135964, skewness -0.105526, kurtosis 2.50235 ) 
  f_peephole_f_c_   ( min -0.713593, max 0.972982, mean 0.00591965, stddev 0.182462, skewness 0.396163, kurtosis 4.62677 ) 
  f_peephole_o_c_   ( min -0.597303, max 0.506818, mean -0.0126943, stddev 0.196616, skewness 0.0679237, kurtosis -0.154991 ) 
  f_w_r_m_   ( min -0.507964, max 0.510147, mean 0.000573597, stddev 0.105838, skewness 0.00223551, kurtosis 0.0302627 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -1.70022, max 1.1857, mean 0.00603198, stddev 0.0981901, skewness -0.178156, kurtosis 5.58222 ) 
  b_w_gifo_r_   ( min -0.349673, max 0.374727, mean -0.000185871, stddev 0.0731482, skewness 0.000343579, kurtosis -0.205115 ) 
  b_bias_   ( min -0.388245, max 1.23297, mean 0.203728, stddev 0.455276, skewness 1.02718, kurtosis -0.695317 ) 
  b_peephole_i_c_   ( min -0.4486, max 0.318392, mean 0.00373275, stddev 0.105013, skewness -0.179622, kurtosis 1.72284 ) 
  b_peephole_f_c_   ( min -0.646684, max 0.759745, mean 0.014208, stddev 0.185454, skewness 0.576766, kurtosis 3.27424 ) 
  b_peephole_o_c_   ( min -0.599034, max 0.61779, mean -0.0155737, stddev 0.203541, skewness -0.0627148, kurtosis 0.522262 ) 
  b_w_r_m_   ( min -0.423328, max 0.401488, mean -0.000398769, stddev 0.0958159, skewness 0.00115021, kurtosis -0.0204606 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.992197, max 0.778834, mean -0.000155913, stddev 0.10923, skewness 0.0054761, kurtosis 0.0578189 ) , lr-coef 1, max-norm 0
  bias ( min -0.0940014, max 2.54685, mean -3.67872e-09, stddev 0.0848047, skewness 22.4854, kurtosis 643.798 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:336) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -7.66434, max 7.26584, mean -0.0147177, stddev 0.961893, skewness 0.577064, kurtosis 2.62679 ) 
[1] output of <BlstmProjected> ( min -3.76172, max 3.8749, mean 0.000574518, stddev 0.720117, skewness -0.0386117, kurtosis 1.65453 ) 
[2] output of <Tanh> ( min -0.99892, max 0.999139, mean 0.00191147, stddev 0.484017, skewness -0.0181557, kurtosis -0.556564 ) 
[3] output of <AffineTransform> ( min -13.1468, max 21.1452, mean 0.0110311, stddev 2.32834, skewness 0.868324, kurtosis 3.60185 ) 
[4] output of <Softmax> ( min 2.30914e-13, max 0.998703, mean 0.00078086, stddev 0.0203804, skewness 37.9336, kurtosis 1571.35 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:346) PER-CLASS PERFORMANCE:
@@@ Frames per-class: [ 1670 30 8 79 45 6 24 17 70 220 22 36 34 39 10 53 43 120 22 20 34 66 38 35 87 73 52 35 43 17 86 33 19 32 16 75 106 10 5 1873 21251 22 63 135 22 30 80 98 44 43 9 67 66 27 41 51 107 54 285 40 48 24 308 222 21 35 16 33 248 15 54 49 139 81 14 4 45 35 47 26 17 66 4 14 44 25 18 49 111 11 94 64 72 34 66 27 32 33 78 42 15 34 9 135 37 103 31 64 37 21 67 0 93 69 23 2 34 10 32 37 25 34 18 19 28 43 46 13 4 26 88 25 22 43 33 62 12 98 32 25 22 78 11 62 24 13 88 37 45 30 117 35 60 38 37 129 16 23 64 982 29 24 10 20 31 33 34 16 52 44 64 25 22 25 3 9 30 11 42 35 13 12 11 8 32 32 9 20 32 48 32 42 56 36 35 41 27 16 5 29 35 31 12 7 9 45 27 81 21 16 19 72 29 0 46 70 68 37 26 33 50 37 32 9 5 26 23 15 14 128 52 51 59 20 20 8 73 5 17 32 22 16 4 39 60 11 16 7 19 20 8 7 52 22 37 15 32 12 39 94 72 13 59 17 69 35 25 11 13 18 58 26 24 119 16 14 9 5 13 35 18 40 17 83 10 20 20 32 21 19 33 104 48 60 79 47 20 14 27 49 56 7 20 34 199 37 4 3 21 22 15 27 39 6 10 11 22 25 47 21 6 14 6 60 3 43 26 14 8 10 70 4 15 29 11 133 11 35 22 51 21 57 14 109 15 36 29 5 21 20 18 19 114 55 69 35 21 21 64 44 39 43 15 21 37 31 46 70 32 25 50 48 101 37 32 62 34 53 184 59 33 24 47 16 35 9 79 14 45 12 20 16 36 46 8 52 28 13 12 31 17 29 40 53 13 20 9 73 85 128 108 21 7 44 30 32 12 46 15 32 90 31 20 32 52 21 237 45 11 21 215 49 25 30 31 29 71 13 57 48 98 46 27 41 19 60 21 4 38 48 50 5 63 71 0 0 25 47 11 16 42 44 49 71 27 15 49 22 22 11 30 67 41 56 15 28 24 35 7 14 15 34 7 8 38 0 9 28 57 50 12 13 39 28 40 746 35 13 19 19 18 27 12 16 52 60 21 82 1 33 49 89 6 38 42 18 51 39 26 11 19 7 23 28 18 51 98 35 37 10 34 11 56 273 7 39 31 18 21 15 24 22 15 25 6 21 53 59 24 25 15 4 49 28 14 9 8 3 8 29 34 0 8 18 51 52 48 23 3 60 16 24 35 46 16 52 37 72 3 38 59 15 5 40 14 54 52 32 51 4 14 21 187 40 33 16 15 18 19 52 32 28 37 29 41 52 10 6 95 18 46 53 17 8 5 18 31 15 14 27 7 8 17 7 93 28 14 64 22 15 8 48 40 29 32 15 10 26 38 92 13 54 242 3 84 0 22 55 226 137 6 121 19 57 20 16 55 85 4 17 33 53 42 81 83 26 30 36 55 64 9 66 84 47 14 13 91 43 19 13 23 37 17 19 27 24 0 59 88 26 15 20 111 16 50 7 32 22 5 17 15 28 38 22 27 28 48 52 41 28 16 114 105 37 112 43 4 69 17 87 95 11 45 15 22 24 24 65 3 27 29 73 63 47 17 34 18 53 17 37 17 20 6 14 36 78 20 8 14 23 47 6 6 22 3 11 89 3 5 18 20 146 25 8 25 44 29 28 22 26 34 24 73 34 85 42 37 35 79 40 39 34 32 23 18 104 65 9 45 19 21 17 48 42 21 40 28 25 27 73 23 34 2 186 119 39 90 27 23 23 38 48 0 28 66 71 45 13 24 7 25 19 16 23 24 129 50 34 21 69 32 14 26 16 14 79 10 15 35 83 24 21 61 28 128 42 66 19 12 25 28 26 60 26 36 30 128 60 8 15 24 59 47 2 19 63 42 22 4 29 30 49 65 12 8 20 32 71 48 32 17 59 28 24 21 20 22 26 24 15 35 0 17 35 13 29 14 47 49 7 13 6 17 31 19 38 15 53 143 34 68 37 23 6 34 29 7 46 92 30 48 31 3 30 16 40 24 44 69 43 19 65 170 29 5 28 134 73 20 11 82 86 29 13 21 12 10 24 12 81 163 12 193 6 28 39 45 31 23 36 65 11 39 113 30 11 28 125 45 52 34 87 20 29 16 97 42 97 26 45 39 15 109 53 18 59 52 33 29 52 31 45 25 45 136 11 24 33 61 26 28 29 106 30 33 18 48 56 86 92 23 37 23 14 2 14 36 346 138 60 357 25 129 51 39 29 11 53 15 14 13 44 38 41 41 8 18 32 16 20 21 17 8 96 16 23 156 45 91 21 6 43 48 25 45 29 8 22 23 31 42 3 198 10 14 29 25 39 64 39 11 26 28 39 96 13 53 5 34 9 11 21 67 28 24 50 19 10 11 22 23 15 13 30 30 176 37 84 80 124 21 74 32 81 43 23 25 68 52 83 27 20 0 33 56 14 23 30 22 132 113 9 71 18 15 16 5 38 61 32 70 35 44 36 82 12 19 25 56 12 24 164 178 24 42 24 19 11 41 67 112 11 27 62 39 5 26 82 58 18 39 25 15 60 20 122 54 35 24 51 28 24 13 180 152 39 59 14 42 123 70 441 14 35 36 425 22 33 13 41 32 48 41 161 16 18 110 56 36 262 66 33 47 21 15 29 84 58 48 47 2 22 14 22 35 120 98 20 59 274 17 15 22 15 59 16 0 27 31 40 47 63 69 16 68 32 39 426 46 55 29 28 47 14 42 42 14 17 28 110 23 16 27 12 36 25 38 21 57 45 37 40 25 7 21 46 54 29 44 49 16 46 13 48 16 103 37 35 12 34 ]
@@@ Loss per-class: [ 0.796994 1.02997 1.75416 1.37732 0.982355 1.92332 1.28593 0.640192 3.02488 1.15743 1.93389 2.10719 1.09396 2.0978 0.704596 2.80093 1.86663 2.39936 5.42181 1.28061 2.08851 1.99677 3.43093 1.54945 1.1858 1.56669 1.50545 3.6849 1.0525 1.44177 4.06968 5.28049 1.03376 4.37415 0.14436 0.363563 2.32713 1.49601 4.32925 1.03551 0.815495 3.26415 4.54823 1.59527 2.23636 2.2615 1.94603 6.85764 1.87975 0.907888 1.95007 2.92673 1.68997 1.82021 1.55278 1.45769 1.67105 0.564835 1.08922 1.50892 3.26434 2.13048 0.725787 2.02479 2.86753 1.6447 1.57172 2.51635 0.748238 2.15212 1.27708 1.58706 2.21868 1.24037 1.05633 0.949379 2.24812 0.309571 2.49037 0.853909 1.64067 2.37706 4.2788 0.886007 1.756 1.94673 2.61303 3.78143 1.41465 1.65132 1.33258 0.44772 1.43956 2.56439 4.36874 3.34574 0.605185 1.64382 1.65433 0.364826 4.10772 2.40586 2.33597 2.07075 0.919955 2.94874 1.93222 2.23323 2.09874 1.01418 1.61496 0 3.70338 1.54836 1.7386 1.37703 1.58334 0.748499 1.6349 2.53374 2.59654 2.55487 2.28971 1.40813 0.396901 1.90745 2.69431 2.52062 0.173213 1.41291 1.10724 1.83199 1.61183 2.61529 2.91881 0.77487 1.69861 0.608816 1.87527 3.33639 3.62889 1.28038 1.97599 1.93776 2.66842 0.113528 1.44647 1.74379 2.30628 1.22284 1.02642 1.61168 4.09254 0.982342 1.63819 0.789388 1.18085 2.45623 1.49103 3.1375 0.380558 1.87312 2.36351 4.09831 0.886083 2.23905 2.30393 1.77768 0.638664 3.03984 0.925229 2.10588 2.14659 2.59906 4.46732 0.466689 1.38674 2.51688 2.53176 2.33987 1.14882 2.52287 4.88999 0.563979 0.722514 1.64385 0.879524 1.72719 1.63964 1.80589 1.08814 2.58149 1.59908 1.82272 1.5889 0.769332 1.65872 1.99591 1.97108 0.336048 2.66207 2.28193 1.09339 2.52099 3.02288 1.72835 1.74475 1.15389 2.60651 2.27953 2.1127 1.85783 0.76352 0 1.78907 0.899115 1.63724 1.40907 1.85407 1.18792 2.43163 2.06715 1.85024 0.870015 1.6652 2.61751 4.01362 1.86213 0.453507 2.75757 1.39734 1.78383 1.48647 1.82603 4.65623 3.02464 1.17216 2.83026 1.86579 1.03997 3.39169 2.46475 2.9821 0.758689 2.67027 3.90814 1.07218 4.90108 1.67994 3.60562 2.80662 1.74037 3.04645 0.786689 2.64189 0.55855 0.939023 0.0977361 1.68731 0.551361 1.52634 1.06146 1.94977 1.32376 2.25462 0.170886 1.20058 0.865619 0.666172 0.748949 2.80762 1.155 3.63019 3.64305 2.261 3.07809 0.215688 3.1564 1.22991 1.65601 1.28092 1.82363 1.11527 4.35624 1.27383 1.90571 0.398283 0.877669 6.0937 1.14178 1.84527 1.59649 0.631404 2.0032 3.9193 3.35565 1.13779 1.72089 2.67704 6.42864 1.56687 1.2815 3.20039 2.33947 0.393239 0.530067 3.66897 1.735 3.66751 4.12887 2.13297 1.55956 2.31831 0.493865 3.54691 2.45129 1.13583 1.39307 1.46029 6.32157 3.18547 1.87547 3.89085 0.855523 3.53718 2.78225 2.53438 3.07039 1.70293 1.32921 1.71849 1.66999 2.49556 2.51209 0.679514 1.74049 1.31176 2.49574 2.36977 1.91361 1.70625 1.78032 2.28731 1.47357 2.76033 2.35601 1.72568 1.87391 0.66651 2.39919 3.16471 1.23577 0.516268 2.1741 2.14321 1.86757 1.64538 1.57929 2.80776 2.74057 1.17626 2.71143 2.57375 1.91482 1.89692 4.16438 1.1214 1.44468 2.80175 2.50353 3.25457 2.45202 1.62175 1.40325 0.602342 0.441266 0.423043 0.483269 3.08991 1.86908 1.86672 1.61764 0.391851 0.727801 1.09137 0.913062 2.16676 2.31537 1.73012 4.57036 1.52621 1.81213 2.6781 2.74923 1.30897 1.16959 2.56393 3.37066 1.12487 2.7415 2.80859 0.593752 1.37981 0.927043 1.8783 0.727434 1.37241 2.49613 1.58762 3.4485 5.06957 2.1756 0.781206 2.11759 2.41295 0.74876 0.593727 0.501894 2.82343 1.53867 2.58145 0.818879 0.362298 0.566363 3.73232 1.53029 0.394004 0.745227 2.90029 1.05811 1.89705 2.07299 2.18684 1.86202 2.45753 2.07443 1.7119 1.69996 2.67603 1.52783 0.959504 1.31665 2.84816 2.48156 5.67614 0.781611 4.4283 4.13038 1.81534 2.14875 2.11721 1.25935 1.88283 2.01872 0 0 0.823862 1.2635 0.711732 2.24539 0.334969 1.4592 3.15155 1.44977 2.79564 1.54045 2.13246 0.500082 1.00691 2.75024 0.570414 0.893919 1.46301 0.375562 1.9542 0.701737 0.358917 2.69582 4.73045 1.81665 3.00428 1.40069 1.11457 2.47771 4.28572 0 1.40538 2.88729 0.733324 1.04029 0.740175 2.7963 1.95036 6.95151 1.19249 2.14433 1.2011 6.22608 2.98464 0.676302 1.75665 0.334583 2.44285 5.64389 1.55551 1.57347 2.29294 0.86602 4.43535 0.861166 5.95937 2.88844 1.74207 2.09367 2.03757 2.21138 2.7035 2.32325 2.08759 1.39995 3.75942 2.38528 2.6875 1.80684 2.82925 0.486261 1.86705 2.86382 2.83016 1.62228 4.15545 1.90234 4.16716 4.75169 1.71179 1.84267 1.23092 1.29978 1.92164 2.53916 1.02764 6.32965 2.53547 1.25331 2.6859 1.23425 0.985001 2.15704 1.40904 1.23238 2.74002 1.98731 1.36166 0.485983 3.62727 1.28236 0.876314 1.5822 1.44131 2.93874 2.99342 0 2.86718 1.85981 0.890552 2.54933 0.933044 1.90393 1.0934 6.55253 1.10644 1.88624 2.67088 0.753235 4.10161 6.07397 3.60994 1.15725 0.21253 4.25027 2.59654 1.93002 1.29514 4.10662 1.41487 1.2145 5.88845 3.84668 0.755806 0.743964 0.882573 1.26054 1.02155 2.68899 2.78225 2.21415 1.33978 1.7282 3.61951 1.68411 1.29362 3.59834 3.22756 2.83667 1.33197 1.85835 2.11925 1.85153 0.451388 1.61068 1.49377 1.04207 4.0663 0.617541 1.74461 1.14075 5.02024 0.957134 2.41379 4.17985 2.22051 0.80273 1.84767 1.27662 5.50867 0.617681 0.745312 3.09594 1.40685 1.45877 4.15937 3.65283 2.41273 2.77849 0.572404 1.5261 1.0868 1.34869 2.00377 4.27677 2.53937 2.59187 0.620282 1.0658 1.5703 0 2.0086 1.75037 5.66745 2.00472 4.86422 1.61908 1.30516 1.35569 1.0836 3.52509 1.71454 8.11328 3.71099 1.94412 2.5155 1.73212 2.36433 3.12553 1.07378 2.37435 5.31638 1.29538 2.2703 0.920721 5.46489 0.96357 1.57634 1.4837 5.22167 6.76296 4.76669 3.1611 2.31279 5.9132 2.69212 2.25884 1.12485 3.66455 6.66673 3.13997 0 1.68595 2.42117 2.28701 2.47973 2.65222 5.69587 1.03079 2.25855 1.19898 0.774656 4.5914 2.33181 1.7577 0.658406 2.55894 1.60995 2.16386 2.65581 1.95465 2.86529 2.55013 1.01902 2.26633 5.05822 1.17018 0.720371 2.20709 0.729002 0.836579 2.5278 3.48663 0.540019 4.3209 2.82012 5.38673 1.30048 0.333217 2.53167 3.44629 2.71366 4.10505 1.24781 1.14985 1.98868 1.81775 1.76599 5.12837 1.46558 1.66238 4.2243 1.88814 1.66347 3.06341 1.97741 2.49052 1.01595 4.05661 1.03622 1.87844 0.958189 1.50228 1.29277 2.2688 2.44452 1.02696 2.00018 1.94577 0.774354 1.25239 7.22596 2.75244 1.95141 1.59 1.51242 1.20304 1.80791 0.764469 1.85794 2.22201 1.56954 4.60805 1.96261 1.67556 1.29513 1.13101 4.38115 3.40994 1.97727 1.26559 3.1707 3.88225 2.35356 1.76104 1.77152 1.43275 1.25014 2.2091 2.95422 1.05549 1.96329 1.88412 1.40387 1.28995 2.35505 4.71182 0.743063 4.08659 3.14156 2.14633 2.92394 1.84728 2.00133 0.43707 6.17881 3.84089 5.03539 1.80439 0.707284 2.86727 1.62739 2.44199 1.73808 3.14665 1.60211 3.24316 0 2.66722 1.96046 1.42384 2.63182 3.61045 3.21226 2.38164 2.72069 1.98302 1.29713 2.00243 4.53984 2.52486 6.11887 2.35531 1.17867 2.19727 1.20356 1.99648 2.64737 6.37206 2.18544 1.16598 2.65791 1.98842 0.805851 0.468106 2.99134 0.894277 1.13635 0.634138 5.06905 2.27414 6.41759 1.38718 3.00864 1.83918 2.37814 2.17528 3.32148 2.89414 0.88804 1.56305 1.58244 1.34373 1.8345 1.37754 2.75357 1.73576 1.86273 2.98412 2.83493 1.91884 1.39213 2.41893 4.01015 1.71268 1.9653 1.864 0.802218 1.62698 1.82907 2.78226 2.44921 1.31922 1.25071 1.42126 0.76739 0.766801 0.797699 4.63924 1.93951 3.01844 3.11812 1.66543 1.13262 2.79297 1.47715 0 2.55493 0.990086 2.10118 1.72188 1.70226 1.69383 1.87 2.34694 1.5164 2.02734 2.36604 1.00317 2.5814 1.7189 0.379826 1.2564 1.04991 1.54191 5.12893 1.48902 1.12101 4.34677 1.66257 3.43623 1.23559 1.76664 2.89696 1.85286 1.91567 2.68007 3.24481 2.20008 3.30781 1.42221 0.986316 3.20346 2.03462 2.09981 0.766459 3.40072 2.45024 2.03573 1.44444 1.8975 3.11417 1.84555 2.05472 1.7928 1.93414 2.21661 2.27731 2.65965 2.64435 2.55343 1.39201 1.98659 2.25386 2.84253 1.7905 1.26374 5.26032 0.857589 3.12564 2.03189 2.61414 3.01045 1.34556 1.46648 3.9532 2.02292 3.04122 3.4427 2.75853 0.299243 4.60439 2.01078 0.407517 1.8909 2.91409 1.05439 1.21277 4.20106 1.27091 2.97156 4.1499 2.42742 4.39826 1.23997 0.629267 1.65183 3.38604 1.79846 2.82188 1.65118 2.20312 4.18721 1.3174 2.34087 1.93785 2.11729 2.18222 1.77614 4.33954 3.29022 1.73007 3.02482 2.19794 1.69785 1.678 1.02185 2.3594 1.77781 3.3332 0.497253 2.09856 4.61461 1.19127 4.27683 2.82378 1.56502 1.75506 1.25765 2.52333 3.01111 2.77974 2.90832 5.35852 2.77809 3.0744 0.715793 1.58148 1.90852 1.3293 1.60599 5.19136 1.84834 1.52164 4.20777 5.03542 0.450618 3.22466 4.12344 2.06293 1.92964 3.87905 2.21845 1.45845 3.39475 3.25125 1.42984 1.27394 5.06787 0.466538 2.11616 3.52597 2.13015 1.71497 4.88056 0.959125 5.09638 3.56982 1.59265 2.99438 1.21928 3.19127 1.08893 2.00585 1.20937 1.29322 2.28743 1.5643 1.1232 2.84789 2.75631 2.39026 1.20286 1.81799 0.925406 2.32696 0.885829 1.9983 2.16599 1.22519 2.89588 2.39425 2.90901 1.18184 1.43375 3.16017 3.84353 3.44139 3.45203 3.06035 1.24404 1.07833 3.57994 4.12085 0.850139 2.14705 2.11607 1.34662 1.9088 1.64615 2.58083 2.62857 0.491411 1.82743 2.5919 2.44469 2.74739 4.98566 3.04877 1.27592 2.6831 1.85013 2.83242 1.25976 0.762436 1.59608 4.35713 0 2.21067 3.02028 2.44933 1.25428 0.484141 4.82304 1.37993 2.67087 2.14217 2.62347 2.70426 0.912972 5.07775 2.13261 1.17701 4.53402 0.883186 3.94134 2.17448 2.22326 1.96805 5.97659 1.46759 2.41921 4.89732 5.79446 2.72763 1.30369 1.45385 2.20051 1.43975 2.4076 3.41518 2.21711 2.30965 0.958752 3.80185 1.02809 3.28142 0.56854 1.78646 1.33509 3.76946 2.59493 1.68689 3.80727 4.68777 3.06861 3.41985 3.12219 2.97771 2.84372 1.00263 0.974588 2.82359 3.19383 1.78963 3.00198 0.901932 1.70057 0.871785 2.61357 4.8501 3.43891 4.09874 0.440348 2.90982 0.914177 1.89344 5.80862 0.896785 0.659523 1.16118 2.51379 1.68945 1.76733 2.6207 2.09679 2.31394 3.83363 1.05132 3.13177 1.76293 1.63152 3.42878 1.49485 4.98183 3.2753 1.20181 3.20544 1.47664 2.01389 3.42855 3.16318 1.43837 2.96918 1.83558 3.13507 2.95795 2.96539 1.12524 1.90269 3.26191 3.27365 1.8935 0.657824 3.9637 1.77056 1.65941 2.60768 1.57327 4.93121 2.3542 0 2.57105 1.04432 3.40095 2.61711 2.28866 3.31313 1.59579 2.78417 1.92806 2.20064 1.4813 2.53566 1.40322 2.30176 5.44 2.38703 3.60725 1.07882 1.54377 1.59803 2.78289 3.41123 1.06614 2.1575 3.13523 1.85418 2.67522 1.30921 3.12471 1.04474 0.814422 2.81213 3.27525 0.999806 1.60909 0.811226 3.96386 0.686996 1.15221 1.3557 1.57473 1.38919 0.918598 1.47043 1.71289 2.31277 2.90417 2.26362 2.38638 1.88737 2.66039 1.91613 1.31337 ]
@@@ Frame-accuracy per-class: [ 76.7435 68.8525 47.0588 60.3774 81.3187 30.7692 61.2245 74.2857 25.5319 71.2018 31.1111 46.5753 72.4638 32.9114 76.1905 29.9065 52.8736 42.3237 17.7778 48.7805 55.0725 54.1353 31.1688 53.5211 58.2857 57.1429 70.4762 8.4507 64.3678 45.7143 13.8728 2.98507 61.5385 9.23077 90.9091 92.7152 32.8638 47.619 18.1818 70.1895 67.5435 26.6667 44.0945 64.2066 17.7778 26.2295 39.7516 4.06091 29.2135 73.5632 31.5789 10.3704 46.6165 43.6364 55.4217 56.3107 49.3023 86.2385 65.1489 61.7284 18.5567 32.6531 79.0924 55.7303 41.8605 39.4366 42.4242 32.8358 77.666 58.0645 62.3853 54.5455 35.1254 58.8957 62.069 44.4444 48.3516 92.9577 40 79.2453 51.4286 40.6015 0 75.8621 40.4494 43.1373 10.8108 14.1414 64.574 52.1739 66.6667 88.3721 56.5517 34.7826 39.0977 14.5455 80 53.7313 53.5032 94.1176 12.9032 46.3768 31.5789 42.0664 66.6667 36.715 69.8413 37.2093 40 69.7674 56.2963 0 12.8342 67.6259 42.5532 0 52.1739 76.1905 30.7692 26.6667 31.3725 26.087 16.2162 51.2821 84.2105 45.977 17.2043 37.037 88.8889 56.6038 81.3559 31.3725 35.5556 25.2874 26.8657 72 64 82.2335 46.1538 39.2157 8.88889 67.5159 26.087 38.4 32.6531 96.2963 55.3672 34.6667 28.5714 52.459 71.4894 45.0704 36.3636 62.3377 56 74.9035 54.5455 12.766 60.4651 31.2468 77.9661 61.2245 9.52381 4.87805 66.6667 35.8209 34.7826 48.4848 80 13.4831 72.8682 54.902 48.8889 23.5294 0 73.6842 72.1311 26.087 37.6471 45.0704 51.8519 16 17.3913 70.5882 83.0769 43.0769 73.6842 43.9024 27.6923 53.6082 70.7692 40 67.2566 60.274 56.338 69.8795 40 48.4848 36.3636 94.9153 8.4507 25.3968 80 26.6667 10.5263 41.7582 25.4545 76.0736 37.2093 24.2424 25.641 46.8966 74.5763 0 36.5591 70.922 43.7956 61.3333 26.4151 68.6567 27.7228 42.6667 55.3846 63.1579 54.5455 41.5094 21.2766 51.6129 82.7586 24.9027 68.5714 60.1942 55.4622 43.9024 4.87805 35.2941 66.6667 0 22.8571 73.8462 0 30.303 0 75.9494 34.7107 8.69565 66.6667 13.3333 30.7692 19.5122 23.5294 26.6667 19.0476 84.4444 34.6667 77.4194 73.8462 96 50.6329 80.4233 56.5517 74.0741 48.7395 57.1429 40.2878 95.7746 54.902 60.8696 74.0741 81.0811 30.7692 60.3774 28.5714 19.2469 24.2424 0 94.7368 0 74.0741 45.0704 59.4595 49.3827 45.7143 19.1617 66.6667 29.2683 92.6829 83.0769 0 51.2821 29.8507 57.4163 78.3505 59.5041 6.28931 23.1579 68.2927 41.3793 29.0909 2.0202 46.0177 40 24.3902 31.8841 85.7143 85.3333 0 28.5714 23.2558 0 25.8065 50.9091 30.3797 92.3077 0 34.7826 66.6667 62.7451 63.1579 0 15.3846 48.2759 0 84.2975 0 11.4943 33.9623 27.5862 23.5294 57.1429 55.3191 22.2222 25.8065 27.1186 86.9565 51.6854 60.8696 28.169 31.1111 50.4854 37.2093 41.7391 13.7931 62.1005 12.9032 46.5753 40.678 18.1818 93.0233 53.6585 21.6216 66.6667 82.0961 43.2432 35.9712 50.7042 37.2093 51.1628 26.3566 40.4494 63.2911 20.6897 25.8065 46.5116 45.3333 19.0476 73.1183 60.9929 36.9231 39.2157 15.8416 26.8041 56.1576 66.6667 80 91.2 89.8551 84.1121 18.9702 47.0588 50.7463 53.0612 86.3158 84.8485 59.1549 63.1579 56.6038 34.4828 59.3407 16 73.1707 48.4848 35.6164 23.6559 47.0588 60.9524 21.0526 7.40741 72 38.0952 0 91.5254 61.7284 65.4206 22.2222 68.2927 63.1579 40.8163 46.7836 13.2296 6.45161 51.1628 66.6667 44.9438 26.2295 73.8462 88 83.871 0 40 30.9392 66.6667 87.8049 83.0769 30.4762 55.814 88.8421 72.5275 0 69.7674 42.6914 32.3232 27.451 39.3443 38.0952 47.4576 50.3497 51.8519 33.0435 63.9175 63.9594 58.0645 29.0909 40.9639 0 80.9917 0 0 54.5455 24.7423 51.4851 36.3636 39.3701 57.3427 0 0 82.3529 56.8421 86.9565 30.303 89.4118 47.191 14.1414 51.7483 14.5455 58.0645 44.4444 88.8889 62.2222 26.087 85.2459 77.037 50.6024 90.2655 51.6129 70.1754 81.6327 36.6197 26.6667 34.4828 19.3548 57.971 53.3333 11.7647 0 0 63.1579 10.5263 86.9565 63.3663 72 22.2222 32.9114 7.01754 56.7901 53.5834 67.6056 0 5.12821 92.3077 43.2432 98.1818 8 0 49.5238 42.9752 46.5116 70.303 0 71.6418 2.0202 35.7542 61.5385 23.3766 42.3529 27.027 42.7184 37.9747 33.9623 43.4783 15.3846 26.6667 25.5319 24.5614 32.4324 89.3204 58.8832 16.9014 32 66.6667 14.4928 34.7826 12.3894 11.7002 53.3333 53.1646 63.4921 48.6486 46.5116 32.2581 69.3878 0 12.9032 58.8235 15.3846 65.1163 78.5047 45.3782 57.1429 62.7451 25.8065 44.4444 66.6667 91.2281 27.5862 52.6316 70.5882 57.1429 47.0588 33.8983 8.69565 0 0 54.0541 69.9029 32.381 80.4124 34.0426 57.1429 0 48.4848 61.2245 5.6338 64.5161 12.1212 5.71429 32 62.069 85.7143 12.987 23.5294 51.6129 36.3636 4.93827 62.069 56.8807 19.0476 21.5385 75.7282 66.6667 62.069 55.814 65.6 34.5679 32.8358 48.4848 64.5161 48.6486 15.3846 43.8095 67.6923 31.5789 13.3333 16.9492 48.1928 41.9048 28.5714 30.7692 89.0052 37.8378 62.3656 78.5047 17.1429 94.1176 54.5455 48.6486 9.52381 77.4194 27.5862 3.63636 0 70.5882 45.7143 53.3333 29.9465 87.7193 68.9655 23.2558 57.7778 58.0645 35.2941 24.7423 39.5062 13.5593 86.1538 45.1613 57.1429 45.283 46.7532 11.8919 37.037 29.3578 81.6495 0 46.1538 0 44.4444 57.6577 25.1656 32.7273 0 51.8519 46.1538 69.5652 82.9268 12.1212 48.6486 3.50877 0 34.2857 20.8955 48.5981 42.3529 29.4479 64.6707 22.6415 3.27869 63.0137 34.2342 75.969 0 63.1579 57.9882 42.1053 0 7.40741 15.3005 18.3908 41.0256 0 38.2979 48 68.5714 15.3846 0 44.898 0 62.1849 42.9379 33.9623 19.3548 43.9024 1.79372 48.4848 49.505 66.6667 80 0 36.3636 51.4286 90.3226 38.5965 57.1429 44.4444 36.3636 52.6316 32.9897 15.2381 67.4699 21.0526 0 59.3886 76.7773 29.3333 79.1111 78.1609 44.4444 11.5108 91.4286 11.4286 34.555 0 61.5385 96.7742 8.88889 24.4898 40.8163 9.16031 57.1429 58.1818 37.2881 39.4558 62.9921 18.9474 45.7143 28.9855 16.2162 37.3832 62.8571 21.3333 45.7143 34.1463 15.3846 0 57.5342 56.051 68.2927 47.0588 55.1724 42.5532 40 61.5385 46.1538 53.3333 85.7143 78.2609 6.70391 28.5714 18.1818 27.027 53.6585 75.0853 39.2157 70.5882 31.3725 29.2135 67.7966 17.5439 62.2222 56.6038 57.971 73.4694 4.08163 14.4928 42.1053 61.1765 13.3333 11.2676 32.7044 59.2593 45.5696 57.971 70.7692 29.7872 32.4324 67.9426 45.8015 52.6316 52.7473 66.6667 18.6047 5.71429 74.2268 9.41176 13.9535 24.6914 35.0877 47.0588 36.3636 88.4354 4.25532 0 0 62.7346 73.6402 17.7215 48.6188 18.1818 59.5745 21.2766 46.7532 20.6186 0 17.5439 39.0977 54.5455 24.1758 7.40741 24.4898 0 27.451 30.7692 48.4848 46.8085 4.08163 40.1544 11.8812 31.8841 60.4651 25.8993 58.4615 48.2759 22.6415 18.1818 27.5862 60.3774 19.0476 25.8065 70.4225 89.8204 8.16327 74.4186 65.0407 84.2105 9.33852 25.8824 16.5414 66.6667 8 47.0588 24.5614 41.5094 13.2231 26.4151 73.9726 62.2951 49.8054 52.8926 47.0588 45.1613 44.898 50.4202 40 0 30.7692 58.2677 54.1176 31.1111 0 23.7288 36.0656 42.4242 70.229 32 23.5294 39.0244 43.0769 68.5315 63.9175 49.2308 68.5714 80.6723 77.193 16.3265 27.907 34.1463 35.5556 37.7358 89.7959 25.8065 59.1549 0 40 67.6056 22.2222 37.2881 20.6897 56.8421 44.4444 26.6667 66.6667 15.3846 28.5714 69.8413 35.8974 41.5584 96.7742 69.1589 83.6237 57.971 1.45985 56 72.3404 0 52.1739 27.1186 53.3333 49.4624 24.8649 52.459 39.1753 38.0952 0 42.623 18.1818 54.321 69.3878 31.4607 43.1655 41.3793 82.0513 16.7939 52.1994 23.7288 72.7273 59.6491 35.6877 39.4558 39.0244 52.1739 64.2424 35.8382 47.4576 37.037 46.5116 64 47.619 40.8163 24 12.2699 44.6483 24 5.68475 61.5385 35.0877 25.3165 41.7582 28.5714 55.3191 54.7945 21.374 43.4783 43.038 21.1454 45.9016 86.9565 3.50877 31.8725 87.9121 51.4286 34.7826 67.4286 63.4146 3.38983 54.5455 20.5128 4.70588 32.8205 15.0943 43.956 75.9494 64.5161 26.484 46.729 37.8378 43.6975 30.4762 29.8507 61.0169 38.0952 50.7937 48.3516 35.2941 41.7582 8.05861 0 53.0612 29.8507 56.9106 30.1887 45.614 67.7966 38.4977 55.7377 11.9403 91.8919 20.6186 7.07965 76.3006 17.2973 25.5319 45.3333 51.0638 41.3793 0 27.5862 27.3973 30.5916 5.05415 23.1405 26.2937 70.5882 39.3822 44.6602 68.3544 44.0678 8.69565 61.6822 45.1613 0 7.40741 98.8764 23.3766 31.3253 53.012 35.2941 21.6216 43.0769 48.4848 24.3902 18.6047 51.4286 58.8235 3.10881 78.7879 38.2979 44.0895 46.1538 52.459 9.30233 76.9231 0 30.9278 35.2941 24.1758 71.1864 0 75.5556 38.2979 79.3651 70.5882 0 60.4534 66.6667 34.4828 16.9492 39.2157 60.7595 54.2636 73.4177 34.7826 79.2453 31.5789 25.3165 61.1399 14.8148 14.9533 0 66.6667 63.1579 17.3913 27.907 7.40741 7.01754 4.08163 63.3663 66.6667 9.52381 17.3913 71.1111 38.2979 25.8065 51.8519 52.459 45.9016 49.8584 18.6667 94.6746 42.236 32.9317 4.65116 25.5034 0 47.8528 62.069 25.5319 58.8235 29.1971 68.5714 81.4371 61.8182 24.3902 0 32.8358 12.3894 34.4828 51.0638 98.3607 0 59.6226 44.9339 10.5263 41.958 21.6216 70.9677 0 0 59.7403 1.62602 83.0769 17.0213 36.6197 44.9438 46.5753 7.27273 40 25.641 0 15.9292 24 44.898 63.8298 48.7395 44.898 28.2353 20.4082 25.641 26.087 69.8795 22.2222 70.2222 8.69565 72.7273 48 65.8228 0 30.1887 53.3333 10.2564 0 30.3797 39.2157 32.2581 24.7934 29.2683 73.4694 66.055 0 40.8163 38.835 45.614 69.3878 37.037 72.5762 45.9016 5.06329 11.7647 6.89655 91.7647 33.1984 79.4326 54.5866 6.89655 81.6901 79.4521 58.2844 31.1111 62.6866 66.6667 24.0964 30.7692 22.6804 24.0964 69.3498 12.1212 32.4324 54.2986 14.1593 57.5342 19.4286 18.0451 65.6716 31.5789 65.1163 32.2581 6.77966 14.2012 64.9573 32.9897 29.4737 0 22.2222 13.7931 62.2222 30.9859 23.2365 25.3807 43.9024 84.0336 21.8579 45.7143 38.7097 22.2222 51.6129 3.36134 24.2424 0 14.5455 85.7143 44.4444 18.9474 36.2205 11.5108 42.4242 8.75912 40 32.9114 58.3822 47.3118 63.0631 37.2881 3.50877 23.1579 6.89655 63.5294 49.4118 62.069 22.8571 7.01754 63.3484 25.5319 36.3636 36.3636 16 65.7534 15.6863 75.3247 79.0698 13.913 17.5824 80 56.7901 66.6667 13.3333 79.0698 64.5161 62.3853 54.2373 58.427 70.7071 42.4242 43.0108 37.037 8.24742 18.1818 39.6135 40 19.7183 24 57.971 ]

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:347) Done 295 files, 0 with no tgt_mats, 0 with other errors. [CROSS-VALIDATION, 0.0214853 min, fps61446.7]
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:353) AvgLoss: 1.80111 (Xent), [AvgXent: 1.80111, AvgTargetEnt: 0]
progress: []
FRAME_ACCURACY >> 52.7508% <<

WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Close():kaldi-io.cc:515) Pipe ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- | had nonzero return status 36096
