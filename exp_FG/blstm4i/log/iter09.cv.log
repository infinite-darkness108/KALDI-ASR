speech-HP-Z2-Tower-G9
nnet-train-multistream-perutt --cross-validate=true --randomize=false --verbose=0 --num-streams=10 --max-frames=15000 --feature-transform=exp_FG/blstm4i/final.feature_transform 'ark:copy-feats scp:exp_FG/blstm4i/cv.scp ark:- | apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_cv10/utt2spk scp:data-fbank/train_cv10/cmvn.scp ark:- ark:- | add-deltas --delta-order=2 ark:- ark:- |' 'ark:ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- |' exp_FG/blstm4i/nnet/nnet_iter09 
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:243) Not in compute-exclusive mode.  Suggestion: use 'nvidia-smi -c 3' to set compute exclusive mode
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:438) Selecting from 1 GPUs
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:453) cudaSetDevice(0): NVIDIA RTX A2000 12GB	free:11620M, used:410M, total:12031M, free/total:0.96589
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:501) Device: 0, mem_ratio: 0.96589
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:382) Trying to select device: 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:511) Success selecting device 0 free mem ratio: 0.96589
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:FinalizeActiveGpu():cu-device.cc:338) The active GPU is [0]: NVIDIA RTX A2000 12GB	free:11106M, used:924M, total:12031M, free/total:0.923168 version 8.6
copy-feats scp:exp_FG/blstm4i/cv.scp ark:- 
apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_cv10/utt2spk scp:data-fbank/train_cv10/cmvn.scp ark:- ark:- 
add-deltas --delta-order=2 ark:- ark:- 
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:144) CROSS-VALIDATION STARTED
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Read():nnet/nnet-matrix-buffer.h:191) Read() started... Buffer size in MB: 0, max 3072, having 0 utterances.
ali-to-post ark:- ark:- 
ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl 'ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |' ark:- 
LOG (copy-feats[5.5.1074~1-71f3]:main():copy-feats.cc:143) Copied 296 feature matrices.
LOG (apply-cmvn[5.5.1074~1-71f3]:main():apply-cmvn.cc:159) Applied cepstral mean and variance normalization to 296 utterances, errors on 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:303) ### After 0 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:304) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.813127, max 1.02813, mean 0.00383499, stddev 0.0924241, skewness 0.0196208, kurtosis 1.38729 ) 
  f_w_gifo_r_   ( min -0.417953, max 0.422586, mean -0.000591881, stddev 0.0797691, skewness -0.00326622, kurtosis 0.0449762 ) 
  f_bias_   ( min -0.358971, max 1.41457, mean 0.211323, stddev 0.467063, skewness 1.0564, kurtosis -0.647661 ) 
  f_peephole_i_c_   ( min -0.795716, max 0.479302, mean -0.00430439, stddev 0.14244, skewness -0.275116, kurtosis 3.52003 ) 
  f_peephole_f_c_   ( min -0.794434, max 0.917004, mean 0.00669835, stddev 0.191026, skewness 0.428515, kurtosis 4.22783 ) 
  f_peephole_o_c_   ( min -0.643463, max 0.53649, mean -0.0129355, stddev 0.197541, skewness 0.08604, kurtosis -0.0212903 ) 
  f_w_r_m_   ( min -0.507547, max 0.511603, mean 0.000567238, stddev 0.107093, skewness 0.000758426, kurtosis 0.0414588 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -1.78521, max 1.24475, mean 0.00614812, stddev 0.100926, skewness -0.196093, kurtosis 6.32091 ) 
  b_w_gifo_r_   ( min -0.37169, max 0.407157, mean -0.000166071, stddev 0.0740136, skewness 0.000132907, kurtosis -0.177203 ) 
  b_bias_   ( min -0.395846, max 1.23564, mean 0.203118, stddev 0.456732, skewness 1.02093, kurtosis -0.696836 ) 
  b_peephole_i_c_   ( min -0.492824, max 0.345471, mean 0.00178231, stddev 0.107968, skewness -0.249483, kurtosis 1.90944 ) 
  b_peephole_f_c_   ( min -0.669227, max 0.764493, mean 0.0167788, stddev 0.191197, skewness 0.585526, kurtosis 2.88105 ) 
  b_peephole_o_c_   ( min -0.601926, max 0.65114, mean -0.0161579, stddev 0.20846, skewness -0.0378945, kurtosis 0.609526 ) 
  b_w_r_m_   ( min -0.417634, max 0.413365, mean -0.000465865, stddev 0.0974354, skewness 0.00260483, kurtosis -0.00509405 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -1.00345, max 0.795666, mean -0.000155911, stddev 0.110019, skewness 0.00528258, kurtosis 0.0582955 ) , lr-coef 1, max-norm 0
  bias ( min -0.0954861, max 2.61578, mean -6.37956e-09, stddev 0.0871669, skewness 22.4336, kurtosis 641.638 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:305) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -11.004, max 12.2048, mean 0.00682255, stddev 0.969149, skewness 0.287893, kurtosis 3.83541 ) 
[1] output of <BlstmProjected> ( min -4.28926, max 4.16803, mean -0.00294051, stddev 0.765309, skewness -0.00381347, kurtosis 0.894479 ) 
[2] output of <Tanh> ( min -0.999624, max 0.999521, mean -0.0018085, stddev 0.519355, skewness 0.000340124, kurtosis -0.874228 ) 
[3] output of <AffineTransform> ( min -14.6494, max 20.9719, mean 0.00533286, stddev 2.49462, skewness 0.703242, kurtosis 2.26236 ) 
[4] output of <Softmax> ( min 1.85034e-13, max 0.999946, mean 0.000780172, stddev 0.0186676, skewness 39.4084, kurtosis 1735.39 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:334) ### After 79212 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:335) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.813127, max 1.02813, mean 0.00383499, stddev 0.0924241, skewness 0.0196208, kurtosis 1.38729 ) 
  f_w_gifo_r_   ( min -0.417953, max 0.422586, mean -0.000591881, stddev 0.0797691, skewness -0.00326622, kurtosis 0.0449762 ) 
  f_bias_   ( min -0.358971, max 1.41457, mean 0.211323, stddev 0.467063, skewness 1.0564, kurtosis -0.647661 ) 
  f_peephole_i_c_   ( min -0.795716, max 0.479302, mean -0.00430439, stddev 0.14244, skewness -0.275116, kurtosis 3.52003 ) 
  f_peephole_f_c_   ( min -0.794434, max 0.917004, mean 0.00669835, stddev 0.191026, skewness 0.428515, kurtosis 4.22783 ) 
  f_peephole_o_c_   ( min -0.643463, max 0.53649, mean -0.0129355, stddev 0.197541, skewness 0.08604, kurtosis -0.0212903 ) 
  f_w_r_m_   ( min -0.507547, max 0.511603, mean 0.000567238, stddev 0.107093, skewness 0.000758426, kurtosis 0.0414588 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -1.78521, max 1.24475, mean 0.00614812, stddev 0.100926, skewness -0.196093, kurtosis 6.32091 ) 
  b_w_gifo_r_   ( min -0.37169, max 0.407157, mean -0.000166071, stddev 0.0740136, skewness 0.000132907, kurtosis -0.177203 ) 
  b_bias_   ( min -0.395846, max 1.23564, mean 0.203118, stddev 0.456732, skewness 1.02093, kurtosis -0.696836 ) 
  b_peephole_i_c_   ( min -0.492824, max 0.345471, mean 0.00178231, stddev 0.107968, skewness -0.249483, kurtosis 1.90944 ) 
  b_peephole_f_c_   ( min -0.669227, max 0.764493, mean 0.0167788, stddev 0.191197, skewness 0.585526, kurtosis 2.88105 ) 
  b_peephole_o_c_   ( min -0.601926, max 0.65114, mean -0.0161579, stddev 0.20846, skewness -0.0378945, kurtosis 0.609526 ) 
  b_w_r_m_   ( min -0.417634, max 0.413365, mean -0.000465865, stddev 0.0974354, skewness 0.00260483, kurtosis -0.00509405 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -1.00345, max 0.795666, mean -0.000155911, stddev 0.110019, skewness 0.00528258, kurtosis 0.0582955 ) , lr-coef 1, max-norm 0
  bias ( min -0.0954861, max 2.61578, mean -6.37956e-09, stddev 0.0871669, skewness 22.4336, kurtosis 641.638 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:336) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -7.66434, max 7.26584, mean -0.0147177, stddev 0.961893, skewness 0.577064, kurtosis 2.62679 ) 
[1] output of <BlstmProjected> ( min -4.10659, max 4.11384, mean -0.00250556, stddev 0.711066, skewness -0.0384378, kurtosis 1.6706 ) 
[2] output of <Tanh> ( min -0.999458, max 0.999466, mean -0.000356477, stddev 0.481299, skewness -0.0141047, kurtosis -0.560615 ) 
[3] output of <AffineTransform> ( min -13.4204, max 21.6958, mean 0.00886576, stddev 2.33774, skewness 0.88432, kurtosis 3.64827 ) 
[4] output of <Softmax> ( min 2.32702e-13, max 0.999934, mean 0.000780878, stddev 0.0197154, skewness 36.729, kurtosis 1491.99 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:346) PER-CLASS PERFORMANCE:
@@@ Frames per-class: [ 1670 30 8 79 45 6 24 17 70 220 22 36 34 39 10 53 43 120 22 20 34 66 38 35 87 73 52 35 43 17 86 33 19 32 16 75 106 10 5 1873 21251 22 63 135 22 30 80 98 44 43 9 67 66 27 41 51 107 54 285 40 48 24 308 222 21 35 16 33 248 15 54 49 139 81 14 4 45 35 47 26 17 66 4 14 44 25 18 49 111 11 94 64 72 34 66 27 32 33 78 42 15 34 9 135 37 103 31 64 37 21 67 0 93 69 23 2 34 10 32 37 25 34 18 19 28 43 46 13 4 26 88 25 22 43 33 62 12 98 32 25 22 78 11 62 24 13 88 37 45 30 117 35 60 38 37 129 16 23 64 982 29 24 10 20 31 33 34 16 52 44 64 25 22 25 3 9 30 11 42 35 13 12 11 8 32 32 9 20 32 48 32 42 56 36 35 41 27 16 5 29 35 31 12 7 9 45 27 81 21 16 19 72 29 0 46 70 68 37 26 33 50 37 32 9 5 26 23 15 14 128 52 51 59 20 20 8 73 5 17 32 22 16 4 39 60 11 16 7 19 20 8 7 52 22 37 15 32 12 39 94 72 13 59 17 69 35 25 11 13 18 58 26 24 119 16 14 9 5 13 35 18 40 17 83 10 20 20 32 21 19 33 104 48 60 79 47 20 14 27 49 56 7 20 34 199 37 4 3 21 22 15 27 39 6 10 11 22 25 47 21 6 14 6 60 3 43 26 14 8 10 70 4 15 29 11 133 11 35 22 51 21 57 14 109 15 36 29 5 21 20 18 19 114 55 69 35 21 21 64 44 39 43 15 21 37 31 46 70 32 25 50 48 101 37 32 62 34 53 184 59 33 24 47 16 35 9 79 14 45 12 20 16 36 46 8 52 28 13 12 31 17 29 40 53 13 20 9 73 85 128 108 21 7 44 30 32 12 46 15 32 90 31 20 32 52 21 237 45 11 21 215 49 25 30 31 29 71 13 57 48 98 46 27 41 19 60 21 4 38 48 50 5 63 71 0 0 25 47 11 16 42 44 49 71 27 15 49 22 22 11 30 67 41 56 15 28 24 35 7 14 15 34 7 8 38 0 9 28 57 50 12 13 39 28 40 746 35 13 19 19 18 27 12 16 52 60 21 82 1 33 49 89 6 38 42 18 51 39 26 11 19 7 23 28 18 51 98 35 37 10 34 11 56 273 7 39 31 18 21 15 24 22 15 25 6 21 53 59 24 25 15 4 49 28 14 9 8 3 8 29 34 0 8 18 51 52 48 23 3 60 16 24 35 46 16 52 37 72 3 38 59 15 5 40 14 54 52 32 51 4 14 21 187 40 33 16 15 18 19 52 32 28 37 29 41 52 10 6 95 18 46 53 17 8 5 18 31 15 14 27 7 8 17 7 93 28 14 64 22 15 8 48 40 29 32 15 10 26 38 92 13 54 242 3 84 0 22 55 226 137 6 121 19 57 20 16 55 85 4 17 33 53 42 81 83 26 30 36 55 64 9 66 84 47 14 13 91 43 19 13 23 37 17 19 27 24 0 59 88 26 15 20 111 16 50 7 32 22 5 17 15 28 38 22 27 28 48 52 41 28 16 114 105 37 112 43 4 69 17 87 95 11 45 15 22 24 24 65 3 27 29 73 63 47 17 34 18 53 17 37 17 20 6 14 36 78 20 8 14 23 47 6 6 22 3 11 89 3 5 18 20 146 25 8 25 44 29 28 22 26 34 24 73 34 85 42 37 35 79 40 39 34 32 23 18 104 65 9 45 19 21 17 48 42 21 40 28 25 27 73 23 34 2 186 119 39 90 27 23 23 38 48 0 28 66 71 45 13 24 7 25 19 16 23 24 129 50 34 21 69 32 14 26 16 14 79 10 15 35 83 24 21 61 28 128 42 66 19 12 25 28 26 60 26 36 30 128 60 8 15 24 59 47 2 19 63 42 22 4 29 30 49 65 12 8 20 32 71 48 32 17 59 28 24 21 20 22 26 24 15 35 0 17 35 13 29 14 47 49 7 13 6 17 31 19 38 15 53 143 34 68 37 23 6 34 29 7 46 92 30 48 31 3 30 16 40 24 44 69 43 19 65 170 29 5 28 134 73 20 11 82 86 29 13 21 12 10 24 12 81 163 12 193 6 28 39 45 31 23 36 65 11 39 113 30 11 28 125 45 52 34 87 20 29 16 97 42 97 26 45 39 15 109 53 18 59 52 33 29 52 31 45 25 45 136 11 24 33 61 26 28 29 106 30 33 18 48 56 86 92 23 37 23 14 2 14 36 346 138 60 357 25 129 51 39 29 11 53 15 14 13 44 38 41 41 8 18 32 16 20 21 17 8 96 16 23 156 45 91 21 6 43 48 25 45 29 8 22 23 31 42 3 198 10 14 29 25 39 64 39 11 26 28 39 96 13 53 5 34 9 11 21 67 28 24 50 19 10 11 22 23 15 13 30 30 176 37 84 80 124 21 74 32 81 43 23 25 68 52 83 27 20 0 33 56 14 23 30 22 132 113 9 71 18 15 16 5 38 61 32 70 35 44 36 82 12 19 25 56 12 24 164 178 24 42 24 19 11 41 67 112 11 27 62 39 5 26 82 58 18 39 25 15 60 20 122 54 35 24 51 28 24 13 180 152 39 59 14 42 123 70 441 14 35 36 425 22 33 13 41 32 48 41 161 16 18 110 56 36 262 66 33 47 21 15 29 84 58 48 47 2 22 14 22 35 120 98 20 59 274 17 15 22 15 59 16 0 27 31 40 47 63 69 16 68 32 39 426 46 55 29 28 47 14 42 42 14 17 28 110 23 16 27 12 36 25 38 21 57 45 37 40 25 7 21 46 54 29 44 49 16 46 13 48 16 103 37 35 12 34 ]
@@@ Loss per-class: [ 0.68204 1.58654 1.49673 0.845426 1.21691 2.2367 1.4987 0.483505 2.31978 0.769407 2.3445 2.21907 0.521027 2.74141 0.845553 2.25085 1.64048 2.49362 3.64928 0.717382 1.46987 1.60765 3.54241 2.02336 0.9316 1.05521 2.12929 3.77054 0.98046 1.42999 4.03789 6.05714 1.57942 4.37241 0.125652 0.415014 2.17761 1.57351 5.23138 1.21725 1.48737 4.18577 4.78794 2.28679 2.03259 3.20569 2.40997 6.50556 1.12548 0.442542 1.33596 2.5111 2.11822 2.96625 0.817656 1.77129 1.4509 0.547605 0.886644 1.22132 3.35539 1.72057 0.886714 1.52493 2.54989 1.66233 1.41596 2.06265 1.03775 2.14825 1.42412 2.24104 2.4312 1.46404 0.842593 0.850195 2.88606 0.402612 2.97974 0.612935 1.56094 1.79533 2.77361 0.681493 2.49827 1.14064 1.8644 3.49696 1.52904 2.36165 2.45606 0.345043 1.36274 2.33159 4.31954 3.32436 0.699232 1.7376 1.59426 0.225258 4.17822 1.99675 2.33178 2.16846 1.26146 3.82359 2.21648 1.14594 2.22335 0.459318 1.54923 0 3.13337 1.47651 1.36003 0.844773 1.72563 2.2895 1.67729 2.26563 1.52328 2.65636 1.87393 1.25188 0.762637 1.79155 3.86587 3.07638 0.264772 0.834238 1.43592 1.4907 1.48965 2.3126 3.34035 0.68373 1.54075 0.45652 1.88798 2.85432 2.26269 1.12463 1.02763 1.74987 3.27836 0.142986 1.63345 2.02903 2.83273 1.18 1.38955 1.02943 2.77514 1.73089 1.46901 0.471575 0.820873 2.46647 1.52429 3.25737 1.84427 1.93084 1.51829 4.08459 0.762109 2.45315 2.30367 1.84966 0.519627 2.51898 0.876037 1.76082 1.6324 1.15489 4.30056 0.592597 1.03142 2.65051 2.895 2.34493 1.37999 1.17097 4.43149 0.995566 1.20921 1.4151 0.349274 1.18397 1.72319 1.7273 0.89739 2.60873 2.31915 1.65444 1.29961 0.86885 1.80643 1.77815 1.66701 0.537509 2.5842 2.15732 1.03806 2.30854 2.85708 1.55543 2.23306 1.09267 2.41058 1.92715 1.71003 2.12666 0.597229 0 1.71556 0.942172 1.93879 1.60516 2.04368 1.05919 2.26548 2.66596 2.21776 1.4716 1.24519 2.76282 3.64464 2.24703 0.61331 3.11696 1.62326 1.99577 1.81791 1.81979 3.72745 3.38016 1.23596 2.8579 2.26121 0.835502 3.21907 2.12117 2.88056 1.02598 2.49172 3.39496 1.2256 5.11321 1.92569 3.14613 2.6678 2.02596 2.5436 0.784968 2.12467 0.816091 1.13073 0.176309 1.84711 0.475982 1.47108 1.46299 2.18272 1.0903 1.67489 0.0997608 1.42954 1.34568 1.22807 0.663144 2.35651 0.620998 3.81786 3.40513 1.6501 1.5924 0.438461 2.8076 1.25025 1.99954 1.17513 1.39531 0.947636 4.06522 0.56435 1.56139 0.349191 0.423977 6.47181 1.03198 1.18435 1.96703 0.626518 0.481529 3.12502 3.76226 1.63205 2.63816 3.16439 5.33143 1.9149 0.483438 3.19733 2.42275 0.319311 0.600694 3.30245 2.25015 2.86561 3.63699 1.86778 1.6403 1.73574 0.503711 3.31736 3.23469 1.15464 1.19549 1.96287 5.63493 3.45957 1.84824 4.46387 0.690496 3.70525 2.58381 2.74763 2.35547 1.73579 0.808268 1.64272 1.33625 2.52181 2.61844 0.487855 1.2002 1.63418 1.58991 2.06371 1.36049 0.659054 1.75127 0.878603 1.21138 2.70249 1.75016 1.74926 1.7893 0.492608 2.15304 2.86126 1.65015 0.47832 0.922331 2.28567 2.45543 1.80769 1.54064 2.82933 2.6109 0.748226 3.15697 2.04812 1.70181 0.881684 4.57862 1.15987 1.32365 2.32838 2.43069 2.75944 2.07621 1.22807 1.28924 0.918746 0.616939 0.505989 0.349556 2.54841 1.8325 1.86195 1.57365 0.277151 1.08265 1.04571 1.66601 2.05662 2.83546 1.50601 4.79762 1.38896 1.46367 2.80334 2.0555 1.48062 1.14379 2.41815 3.06867 1.77237 2.4548 2.08467 0.811479 1.48632 0.381147 2.36907 0.692871 1.89678 2.02305 1.71402 3.79764 4.89508 2.14331 0.946503 1.46139 2.60203 0.794116 0.659594 0.378445 2.94755 1.54687 1.62687 0.405917 0.346712 0.477742 5.08114 1.68504 0.940549 0.7857 2.93666 0.625224 1.49211 2.26156 2.19557 2.17506 2.23704 1.20433 1.74833 1.5549 2.09165 1.28334 1.446 1.15416 2.31539 2.52157 5.35186 0.906432 4.79147 1.51568 2.29696 2.82603 2.19795 1.69813 1.90321 1.85074 0 0 0.602603 0.90702 0.417829 2.38078 0.383664 1.62785 2.6189 1.05755 2.2175 1.54051 1.74427 0.427725 1.0955 2.89529 0.574919 1.458 1.82169 0.856521 2.4855 0.867549 0.258028 3.18698 4.04586 1.22225 3.07617 1.27933 1.28126 3.62026 3.50068 0 2.22091 2.19912 0.815891 1.20413 1.12715 2.3472 1.27427 6.54555 0.997418 2.17814 1.34389 3.9939 2.4686 0.603964 1.36497 0.395347 2.16256 4.89913 0.79452 1.09371 2.56865 0.704519 5.43859 0.845995 5.76503 1.8099 1.51472 1.68493 2.46401 2.00129 2.49323 2.16257 1.69406 1.50499 3.97379 1.2778 2.19771 1.5891 2.63774 1.45536 1.96414 2.70396 2.98054 1.73843 4.39116 1.83909 3.47405 4.00904 1.1169 1.68084 1.27197 1.16559 2.12726 2.04292 1.32144 5.3067 2.16025 1.14495 2.29886 0.804371 0.973515 1.63896 1.09544 1.54944 2.42422 2.93221 1.3177 0.41998 3.37034 1.09239 0.990317 2.10079 1.91917 3.07456 1.96676 0 2.19531 1.59754 1.50502 2.21542 0.657868 1.6527 1.17437 6.34917 1.78246 2.11209 1.90115 0.77999 3.72283 5.78788 3.5825 0.948337 1.02785 4.48942 3.34616 1.72354 1.20179 3.81808 2.18658 1.31373 6.17496 3.39871 0.369507 1.22334 0.324765 2.01528 0.762378 1.18174 2.83077 2.54181 1.05778 2.36982 3.81562 1.34358 1.95068 4.44129 4.45505 3.0072 1.09009 2.37437 1.89039 1.11462 0.471764 1.55086 1.94847 1.05698 4.24676 0.142388 1.4677 1.88802 4.90605 1.30457 2.64436 4.10184 1.10221 0.440822 1.44298 0.966575 4.87693 0.630667 1.06295 3.52998 0.756872 1.4337 6.43922 3.79348 1.47902 2.80452 0.660624 1.4873 1.29232 1.43907 1.95223 3.91704 1.78362 2.62109 0.671028 1.25264 1.32777 0 2.33974 1.63073 5.28025 1.75301 3.19107 1.9661 1.04042 0.50554 1.04916 3.32334 1.43793 7.7464 3.6182 2.17148 2.19553 1.676 2.76293 2.98366 1.09808 2.25973 5.18583 1.46974 2.18669 1.16186 5.73772 1.26884 1.7694 1.47851 6.94168 6.46269 4.43086 3.14358 2.61931 5.65245 2.40475 2.13591 1.1969 3.92908 4.92982 3.66199 0 1.71699 2.68055 2.94765 1.81114 2.42893 5.35246 0.754722 2.7684 1.33993 0.411414 4.55734 1.28495 2.17131 0.540775 1.98412 1.50554 2.09867 2.62003 2.89623 2.7133 1.85501 0.960724 2.64949 5.74008 1.38602 0.673904 1.72426 0.90688 0.767959 2.15572 3.37427 0.771778 4.15165 2.75048 5.67366 1.46253 1.61956 2.77508 3.555 2.15985 4.92224 1.43245 1.03645 1.6805 1.00274 1.85602 4.54887 1.26167 1.4185 3.38214 1.89939 2.25846 1.91273 2.21274 2.15531 0.67667 2.29857 1.08876 2.14963 0.786528 1.43226 1.48269 1.70152 1.40857 1.01405 2.1312 1.98482 1.86995 1.27972 6.4971 2.46575 1.21869 1.47281 1.25113 1.4388 1.91219 0.390174 1.81577 2.73683 1.68674 4.61142 2.34079 2.17738 1.36556 1.5339 3.64693 2.43585 1.34911 1.10187 2.96548 3.63589 2.92486 2.16793 1.65882 1.5193 1.32429 1.71129 2.98809 1.61488 1.82808 1.36448 1.45721 1.26253 1.82081 4.67868 0.650835 3.91808 2.96071 1.48557 2.23529 1.90946 2.06619 0.441115 6.75564 4.06515 3.5813 2.1169 0.733604 2.2847 1.40945 1.78006 1.05839 3.25381 1.24934 3.82778 0 2.88747 1.92072 1.72392 1.97433 3.21754 2.54821 2.01388 1.59243 1.33823 1.09805 1.99538 6.74638 2.71197 5.94793 1.72284 1.26189 1.65275 1.25419 2.49658 2.90531 6.57621 2.20755 0.994594 3.10181 3.9931 0.389017 1.11587 2.77841 1.49656 1.0324 0.843394 5.10578 2.35496 5.83978 1.39989 3.25678 2.00032 2.6928 2.98794 3.06407 2.68874 0.876085 1.37821 1.15389 0.861361 1.47678 1.79821 2.47926 1.99592 1.6614 1.86902 3.55476 1.65032 1.77216 2.28932 3.67281 1.11189 2.08356 1.57801 1.56911 1.91384 1.85924 3.21981 2.25206 1.08166 1.15789 1.68342 0.987083 0.864838 0.590083 4.36856 2.29877 2.95709 3.14841 1.49077 0.903125 2.13001 1.71426 0 2.57188 0.795219 2.17582 1.40838 1.47546 1.34505 1.86314 1.32641 1.78837 2.00067 1.90726 1.37222 2.49027 1.61202 0.230424 0.968981 0.783779 1.51124 5.53713 1.25316 1.79665 3.59401 1.06676 3.10652 1.13383 1.79613 3.19656 1.61023 1.55739 2.66546 2.26165 2.17286 3.31516 0.978505 1.06654 3.02733 2.23912 2.01248 0.717319 3.51145 2.24498 1.77412 0.350102 3.89333 3.48052 1.40665 2.45068 1.4885 1.91009 1.83877 2.44634 2.17905 2.215 2.334 1.7159 1.73986 2.10159 2.90963 1.20073 1.19344 6.1211 1.01677 2.91795 2.36452 2.38377 2.99073 1.36122 1.75972 2.87168 2.00948 3.20551 2.97582 2.54272 0.510794 4.22105 2.15629 0.398834 1.6035 2.54597 1.18312 1.59598 4.34974 1.16882 3.08467 4.87921 2.65733 3.61986 1.35313 0.925298 1.88768 3.10206 0.754989 2.17344 1.44963 1.30962 3.91155 1.62345 1.38172 1.80726 1.48903 1.47953 1.35085 4.05249 3.43745 2.25773 2.60768 2.0045 1.74296 1.73218 1.42338 1.74191 1.26095 3.06347 0.569689 1.96403 5.35004 1.02848 4.04811 2.29476 1.70932 1.79868 1.49371 0.469733 1.60193 2.42111 2.51598 5.51559 1.83729 2.85509 0.708873 1.4902 1.76874 1.70256 1.56012 4.20568 1.93385 1.53127 3.73441 4.30801 0.713209 3.05935 4.46637 1.9916 1.45158 4.46525 1.86666 0.997356 2.56842 2.84703 0.850455 1.66602 4.76052 0.586024 1.84804 3.67204 2.44953 1.31666 5.59782 1.31723 5.43115 3.78621 1.69235 2.8876 1.65346 4.06279 1.09718 2.10361 1.09858 1.46411 2.49763 1.60687 1.50576 3.19814 2.8621 1.76748 1.26877 1.86355 0.832423 2.01443 0.644818 1.99725 1.74415 0.657072 2.74736 2.32517 2.10457 2.0116 1.4885 3.10069 3.32652 2.55311 2.95299 2.33512 0.781278 0.605351 2.06379 3.55746 0.801779 2.04023 1.91552 1.38523 2.40344 1.2351 2.58395 2.55511 0.713256 2.69876 2.66405 1.98017 2.49925 4.71879 2.9056 1.29285 2.72858 1.02037 2.72092 1.02832 0.762733 1.6041 3.88845 0 2.27219 2.46529 2.39494 1.60197 0.387913 3.62158 1.55442 3.13464 3.34281 2.40797 2.82833 0.897959 5.88216 1.73626 1.76829 4.2632 1.15068 2.91291 2.00836 2.61479 1.78929 6.19172 0.920916 2.36488 5.15069 5.59291 2.88471 1.703 1.20668 2.51931 1.11433 1.24713 3.26978 2.95969 1.55304 1.04045 4.17509 1.66059 3.30239 0.622006 1.33235 1.48804 3.94547 2.5595 1.3808 2.84016 5.16266 2.32851 3.40792 3.58283 3.25608 2.33722 1.42559 1.00143 1.17273 2.51042 1.46942 2.88061 0.604447 1.45843 1.04641 2.36377 4.3073 2.67276 3.30799 0.582556 2.88657 0.982692 1.93241 6.69258 0.886863 0.645642 1.12936 2.51804 1.90013 2.62103 2.86589 1.72344 1.8976 2.86325 0.919443 3.04787 1.077 1.76895 3.30706 1.40447 4.86341 3.0702 0.99649 2.49106 1.68637 1.89835 3.39202 2.57969 1.74459 2.79253 2.3479 3.74075 1.99605 3.01675 0.850198 2.62384 2.70764 3.07956 1.47429 1.08781 3.47225 1.63662 1.89703 2.88891 1.38651 4.97156 1.59737 0 3.25792 1.23003 4.21497 2.83276 2.09438 3.06378 1.70761 2.5432 1.73865 1.86967 1.4095 2.7694 1.34509 1.97782 5.82019 2.11931 2.53811 1.11498 1.65261 2.61948 2.32634 3.99856 0.457023 2.45521 1.96816 1.98125 1.84788 0.540799 3.91634 0.722942 1.19064 2.6483 3.784 0.658933 2.13552 0.945262 4.95325 0.8258 1.37776 1.24942 1.74933 1.55155 1.14468 1.37836 1.70719 1.86167 3.23659 2.84733 1.81099 0.980932 2.59954 2.29097 0.906666 ]
@@@ Frame-accuracy per-class: [ 79.9162 55.7377 47.0588 71.6981 61.5385 30.7692 61.2245 91.4286 41.1348 83.9002 31.1111 38.3562 86.9565 40.5063 66.6667 41.1215 52.8736 34.8548 22.2222 82.9268 57.971 64.6617 28.5714 42.2535 73.1429 72.1088 62.8571 8.4507 64.3678 57.1429 13.8728 5.97015 46.1538 9.23077 96.9697 90.0662 39.4366 47.619 0 64.3715 45.7568 13.3333 39.3701 53.8745 35.5556 22.9508 33.5404 8.12183 53.9326 94.2529 42.1053 20.7407 45.1128 32.7273 79.5181 38.835 57.6744 82.5688 76.3573 66.6667 14.433 44.898 74.2301 64.7191 27.907 53.5211 48.4848 47.7612 69.6177 58.0645 47.7064 46.4646 26.5233 55.2147 68.9655 88.8889 24.1758 87.3239 23.1579 83.0189 51.4286 54.1353 22.2222 75.8621 17.9775 62.7451 21.6216 24.2424 52.9148 34.7826 29.6296 91.4729 57.931 28.9855 42.1053 14.5455 73.8462 56.7164 47.1338 98.8235 12.9032 49.2754 31.5789 30.9963 53.3333 27.0531 60.3175 63.5659 37.3333 83.7209 57.7778 0 22.4599 70.5036 51.0638 40 43.4783 0 24.6154 40 66.6667 31.8841 16.2162 51.2821 73.6842 45.977 0 29.6296 88.8889 60.3774 76.8362 35.2941 53.3333 36.7816 11.9403 88 64 88.3249 49.2308 35.2941 35.5556 72.6115 43.4783 44.8 16.3265 96.2963 59.887 45.3333 17.5824 62.2951 51.0638 61.9718 39.6694 41.5584 53.3333 84.1699 60.6061 17.0213 55.814 30.3308 30.5085 53.0612 47.619 4.87805 79.3651 14.9254 34.7826 54.5455 83.8095 24.7191 72.8682 62.7451 57.7778 78.4314 0 73.6842 75.4098 34.7826 32.9412 45.0704 44.4444 64 26.087 58.8235 73.8462 61.5385 94.7368 43.9024 3.07692 55.6701 80 28.2353 44.2478 54.7945 61.9718 67.4699 54.5455 42.4242 18.1818 84.7458 30.9859 44.4444 80 26.6667 21.0526 48.3516 21.8182 76.0736 51.1628 30.303 30.7692 45.5172 81.3559 0 38.7097 69.5035 37.9562 64 26.4151 65.6716 33.6634 34.6667 52.3077 42.1053 72.7273 30.1887 21.2766 0 68.9655 20.2335 60.9524 38.835 45.3782 48.7805 29.2683 11.7647 62.585 0 11.4286 70.7692 0 36.3636 0 53.1646 29.7521 17.3913 66.6667 13.3333 51.2821 19.5122 11.7647 13.3333 19.0476 71.1111 45.3333 64.5161 73.8462 96 43.038 94.1799 64.8276 37.037 42.0168 68.5714 53.2374 98.5915 54.902 60.8696 66.6667 86.4865 37.6068 86.7925 28.5714 28.4519 54.5455 34.4828 94.7368 0 74.0741 25.3521 59.4595 66.6667 57.1429 19.1617 95.2381 43.9024 92.6829 89.2308 0 61.5385 53.7313 45.933 78.3505 87.6033 22.6415 12.6316 43.9024 34.4828 47.2727 8.08081 37.1681 80 14.6341 34.7826 87.7193 82.6667 0 0 41.8605 4.44444 45.1613 32.7273 50.6329 92.3077 9.52381 26.087 80 62.7451 48.4211 0 0 55.1724 0 80.9917 0 16.092 30.1887 55.1724 23.5294 47.619 48.227 22.2222 25.8065 20.339 86.9565 59.176 60.8696 47.8873 48.8889 44.6602 93.0233 41.7391 96.5517 70.3196 12.9032 54.7945 30.5085 54.5455 88.3721 48.7805 43.2432 66.6667 82.0961 66.6667 27.3381 42.2535 51.1628 55.814 31.0078 44.9438 78.481 11.4943 38.7097 41.8605 72 9.52381 70.9677 66.6667 46.1538 31.3725 31.6832 49.4845 66.9951 66.6667 67.6923 83.2 86.9565 89.7196 25.4743 52.1008 56.7164 40.8163 90.5263 54.5455 56.338 63.1579 55.3459 20.6897 63.7363 0 58.5366 48.4848 27.3973 40.8602 23.5294 72.381 31.5789 7.40741 48 47.619 17.1429 77.9661 61.7284 95.3271 22.2222 63.4146 42.1053 43.5374 43.2749 10.1167 5.52995 55.814 66.6667 53.9326 36.0656 73.8462 80 90.3226 0 49.2308 57.4586 98.4127 82.9268 86.1538 5.71429 60.4651 68.2105 76.9231 0 83.7209 51.9722 30.303 31.3725 39.3443 44.4444 64.4068 51.7483 59.2593 41.7391 65.9794 49.7462 68.8172 32.7273 31.3253 0 74.3802 0 0 49.3506 16.4948 47.5248 36.3636 42.5197 62.9371 0 0 90.1961 82.1053 95.6522 42.4242 91.7647 51.6854 34.3434 74.1259 29.0909 58.0645 62.6263 88.8889 62.2222 26.087 88.5246 60.7407 31.3253 77.8761 19.3548 59.6491 93.8776 33.8028 26.6667 55.1724 12.9032 60.8696 53.3333 0 10.3896 0 42.1053 31.5789 76.5217 63.3663 56 29.6296 65.8228 3.50877 64.1975 51.9759 59.1549 0 15.3846 92.3077 64.8649 90.9091 32 0 78.0952 59.5041 60.4651 78.7879 0 74.6269 0 46.9274 46.1538 49.3506 42.3529 43.2432 36.8932 43.038 49.0566 60.8696 15.3846 66.6667 38.2979 35.0877 32.4324 25.2427 57.868 25.3521 29.3333 47.619 17.3913 34.7826 17.6991 19.7441 66.6667 55.6962 57.1429 54.0541 37.2093 38.7097 48.9796 0 32.2581 58.8235 15.3846 74.4186 78.5047 58.8235 65.3061 47.0588 32.2581 22.2222 68.6869 87.7193 20.6897 63.1579 58.8235 28.5714 35.2941 27.1186 26.087 0 47.0588 64.8649 60.1942 32.381 88.6598 51.0638 28.5714 0 36.3636 44.898 28.169 75.2688 18.1818 9.52381 37.3333 66.2069 57.1429 15.5844 26.8908 58.0645 54.5455 14.8148 55.1724 45.8716 13.3333 24.6154 89.3204 66.6667 89.6552 27.907 77.8667 66.6667 35.8209 30.303 77.4194 43.2432 10.2564 60.9524 15.3846 7.01754 13.3333 23.7288 65.0602 45.7143 57.1429 61.5385 86.911 27.027 51.6129 74.7664 5.71429 94.1176 36.3636 43.2432 15.873 51.6129 34.4828 3.63636 53.3333 82.3529 57.1429 40 38.5027 77.193 68.9655 13.9535 80 58.0645 0 16.4948 74.0741 27.1186 80 58.0645 76.1905 41.5094 46.7532 19.4595 51.8519 29.3578 82.8866 0 56.8047 0 40 57.6577 23.8411 52.3636 0 42.7984 41.0256 88.6957 73.1707 18.1818 64.8649 10.5263 0 28.5714 23.8806 52.3364 42.3529 35.5828 62.2754 15.0943 6.55738 60.274 18.018 66.6667 0 63.1579 54.4379 44.2105 0 14.8148 19.6721 9.1954 30.7692 0 34.0426 18.6667 68.5714 0 0 20.4082 0 52.1008 30.5085 30.1887 45.1613 63.4146 2.69058 72.7273 31.6832 66.6667 86.1538 4.44444 36.3636 22.8571 77.4194 56.1404 59.7403 48.8889 47.2727 31.5789 47.4227 49.5238 72.2892 10.5263 0 58.5153 78.673 53.3333 86.2222 78.1609 22.2222 10.0719 80 16 20.9424 0 59.3407 51.6129 35.5556 24.4898 53.0612 12.2137 28.5714 58.1818 54.2373 63.9456 53.5433 25.2632 40 52.1739 16.2162 44.8598 57.1429 32 45.7143 34.1463 76.9231 55.1724 49.3151 48.4076 73.1707 35.2941 62.069 59.5745 61.0526 76.9231 46.1538 53.3333 0 69.5652 8.93855 0 54.5455 37.8378 53.6585 68.2594 31.3725 82.3529 27.451 31.4607 64.4068 28.0702 48.8889 33.9623 52.1739 65.3061 10.8844 28.9855 38.5965 65.8824 18.6667 5.6338 28.9308 37.037 45.5696 52.1739 61.5385 46.8085 32.4324 55.5024 42.7481 63.1579 57.1429 71.7949 46.5116 5.71429 82.4742 2.35294 13.9535 41.9753 45.614 23.5294 43.6364 92.517 4.25532 0 0 56.8365 79.4979 22.7848 51.9337 47.2727 72.3404 21.2766 59.7403 2.06186 0 10.5263 48.1203 47.5524 37.3626 14.8148 32.6531 13.3333 58.8235 66.6667 60.6061 38.2979 16.3265 27.027 13.8614 31.8841 69.7674 48.9209 55.3846 34.4828 11.3208 6.06061 20.6897 69.1824 9.52381 6.45161 90.1408 61.0778 16.3265 46.5116 71.5447 80.7018 14.786 32.9412 21.0526 51.2821 0 35.2941 17.5439 22.6415 11.5702 22.6415 65.7534 68.8525 57.5875 76.0331 70.5882 19.3548 36.7347 47.0588 46.3158 0 5.12821 66.1417 51.7647 31.1111 0 57.6271 32.7869 54.5455 41.2214 32 35.2941 19.5122 43.0769 68.5315 76.2887 52.3077 68.5714 77.3109 80.7018 16.3265 18.6047 34.1463 17.7778 56.6038 85.7143 38.7097 50.7042 0 22.8571 81.6901 22.2222 54.2373 48.2759 58.9474 48.4848 40 44.4444 30.7692 34.2857 60.3175 51.2821 51.9481 96.7742 72.8972 85.7143 66.6667 4.37956 61.3333 51.0638 30.7692 66.6667 40.678 53.3333 40.8602 28.1081 55.7377 47.4227 38.0952 28.5714 29.5082 24.2424 74.0741 69.3878 29.2135 33.0935 45.977 82.0513 16.7939 61.5836 30.5085 90.9091 0 26.7658 58.5034 24.3902 52.1739 50.9091 42.7746 40.678 51.8519 46.5116 40 38.0952 48.9796 40 28.2209 62.9969 40 5.16796 46.1538 28.0702 22.7848 46.1538 25.3968 68.0851 46.5753 25.9542 43.4783 30.3797 31.7181 32.7869 95.6522 0 22.3108 90.1099 57.1429 31.8841 58.2857 43.9024 13.5593 48.4848 25.641 4.70588 25.641 30.1887 35.1648 65.8228 32.2581 28.3105 78.5047 32.4324 53.7815 57.1429 26.8657 50.8475 60.9524 44.4444 54.9451 62.7451 52.7473 11.7216 0 36.7347 26.8657 52.0325 56.6038 63.1579 61.0169 52.5822 55.7377 17.9104 86.4865 22.6804 3.53982 71.6763 24.8649 38.2979 42.6667 42.5532 34.4828 80 34.4828 32.8767 39.5382 7.22022 54.5455 29.3706 70.5882 46.332 42.7184 53.1646 54.2373 17.3913 57.9439 38.7097 13.7931 7.40741 74.1573 23.3766 14.4578 53.012 58.8235 10.8108 36.9231 72.7273 34.1463 18.6047 74.2857 47.0588 1.03627 78.7879 46.8085 44.7284 28.5714 65.5738 0 46.1538 0 37.1134 50.9804 19.7802 57.6271 0 75.5556 42.5532 76.1905 63.5294 0 59.4458 47.619 27.5862 23.7288 54.902 81.0127 54.2636 70.8861 34.7826 79.2453 38.5965 43.038 77.7202 22.2222 22.4299 0 23.1884 42.1053 0 23.2558 28.1481 17.5439 20.4082 73.2673 76.9231 28.5714 17.3913 75.5556 42.5532 38.7097 74.0741 39.3443 68.8525 47.0255 21.3333 79.2899 36.0248 25.7028 32.5581 28.1879 12.3077 47.8528 66.6667 34.0426 66.6667 30.6569 72.381 81.4371 54.5455 29.2683 0 41.791 26.5487 34.4828 42.5532 95.082 0 56.6038 40.5286 0 53.1469 21.6216 58.0645 12.1212 0 49.3506 3.25203 58.4615 22.695 42.2535 31.4607 57.5342 2.42424 80 25.641 0 23.0088 8 44.898 66.2614 44.2577 65.3061 51.7647 28.5714 20.5128 43.4783 72.2892 23.7037 59.5556 17.3913 80 51.2 58.2278 0 33.9623 50.9091 11.9658 0 40.5063 43.1373 32.2581 18.1818 48.7805 70.2041 66.055 47.8873 40.8163 54.3689 42.1053 77.551 37.037 67.036 42.623 7.59494 26.8908 20.6897 89.4118 36.4372 68.0851 59.3431 0 70.4225 76.7123 62.7497 22.2222 38.806 14.8148 12.0482 64.6154 28.866 19.2771 66.8731 18.1818 75.6757 60.6335 15.9292 57.5342 18.2857 30.0752 74.6269 46.3158 51.1628 38.7097 13.5593 22.4852 46.1538 18.5567 23.1579 0 31.1111 20.6897 80 19.7183 33.195 25.3807 58.5366 63.8655 23.3151 45.7143 38.7097 31.1111 51.6129 10.084 36.3636 0 14.5455 63.4921 22.2222 31.5789 44.0945 15.8273 36.3636 16.0584 43.0769 40.5063 66.1196 47.3118 46.8468 37.2881 3.50877 40 27.5862 70.5882 56.4706 34.4828 34.2857 3.50877 86.8778 21.2766 48.4848 36.3636 16 82.1918 0 77.9221 60.4651 12.1739 0 82.6667 49.3827 58.8235 0 74.4186 53.7634 69.7248 47.4576 58.427 68.6869 54.5455 34.4086 29.6296 12.3711 0 53.1401 69.3333 36.6197 16 75.3623 ]

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:347) Done 295 files, 0 with no tgt_mats, 0 with other errors. [CROSS-VALIDATION, 0.0214604 min, fps61518]
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:353) AvgLoss: 1.93309 (Xent), [AvgXent: 1.93309, AvgTargetEnt: 0]
progress: []
FRAME_ACCURACY >> 47.9814% <<

WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Close():kaldi-io.cc:515) Pipe ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- | had nonzero return status 36096
