speech-HP-Z2-Tower-G9
nnet-train-multistream-perutt --cross-validate=false --randomize=true --verbose=0 --num-streams=10 --max-frames=15000 --learn-rate=0.00004 --momentum=0.9 --l1-penalty=0 --l2-penalty=0 --feature-transform=exp_FG/blstm4i/final.feature_transform 'ark:copy-feats scp:exp_FG/blstm4i/train.scp ark:- | apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_tr90/utt2spk scp:data-fbank/train_tr90/cmvn.scp ark:- ark:- | add-deltas --delta-order=2 ark:- ark:- |' 'ark:ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- |' exp_FG/blstm4i/nnet/nnet_iter08_learnrate0.00004_tr0.6686_cv1.8011 exp_FG/blstm4i/nnet/nnet_iter09 
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:243) Not in compute-exclusive mode.  Suggestion: use 'nvidia-smi -c 3' to set compute exclusive mode
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:438) Selecting from 1 GPUs
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:453) cudaSetDevice(0): NVIDIA RTX A2000 12GB	free:11620M, used:410M, total:12031M, free/total:0.96589
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:501) Device: 0, mem_ratio: 0.96589
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:382) Trying to select device: 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:511) Success selecting device 0 free mem ratio: 0.96589
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:FinalizeActiveGpu():cu-device.cc:338) The active GPU is [0]: NVIDIA RTX A2000 12GB	free:11106M, used:924M, total:12031M, free/total:0.923168 version 8.6
copy-feats scp:exp_FG/blstm4i/train.scp ark:- 
apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_tr90/utt2spk scp:data-fbank/train_tr90/cmvn.scp ark:- ark:- 
add-deltas --delta-order=2 ark:- ark:- 
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:144) TRAINING STARTED
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Read():nnet/nnet-matrix-buffer.h:191) Read() started... Buffer size in MB: 0, max 3072, having 0 utterances.
ali-to-post ark:- ark:- 
ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl 'ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |' ark:- 
LOG (copy-feats[5.5.1074~1-71f3]:main():copy-feats.cc:143) Copied 2624 feature matrices.
LOG (apply-cmvn[5.5.1074~1-71f3]:main():apply-cmvn.cc:159) Applied cepstral mean and variance normalization to 2624 utterances, errors on 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:303) ### After 0 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:304) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.753944, max 0.905358, mean 0.0039434, stddev 0.0902407, skewness 0.0116066, kurtosis 1.15656 ) 
  f_w_gifo_r_   ( min -0.421141, max 0.414786, mean -0.000590853, stddev 0.079271, skewness -0.00321256, kurtosis 0.0371974 ) 
  f_bias_   ( min -0.349337, max 1.43705, mean 0.211128, stddev 0.465932, skewness 1.05942, kurtosis -0.647259 ) 
  f_peephole_i_c_   ( min -0.688079, max 0.491297, mean -0.00526978, stddev 0.13598, skewness -0.10749, kurtosis 2.5106 ) 
  f_peephole_f_c_   ( min -0.713258, max 0.972772, mean 0.00594818, stddev 0.182423, skewness 0.39713, kurtosis 4.62634 ) 
  f_peephole_o_c_   ( min -0.607303, max 0.507108, mean -0.0126706, stddev 0.196722, skewness 0.0643158, kurtosis -0.142301 ) 
  f_w_r_m_   ( min -0.507863, max 0.509798, mean 0.00057308, stddev 0.105836, skewness 0.00225457, kurtosis 0.030318 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -1.70265, max 1.18607, mean 0.00602384, stddev 0.0981805, skewness -0.179018, kurtosis 5.59165 ) 
  b_w_gifo_r_   ( min -0.35044, max 0.375522, mean -0.000185572, stddev 0.0731489, skewness 0.000352159, kurtosis -0.2049 ) 
  b_bias_   ( min -0.388887, max 1.23289, mean 0.203758, stddev 0.455279, skewness 1.02702, kurtosis -0.695394 ) 
  b_peephole_i_c_   ( min -0.449122, max 0.31841, mean 0.00372623, stddev 0.105022, skewness -0.180284, kurtosis 1.72656 ) 
  b_peephole_f_c_   ( min -0.646815, max 0.759802, mean 0.0141945, stddev 0.185483, skewness 0.577037, kurtosis 3.27379 ) 
  b_peephole_o_c_   ( min -0.598974, max 0.618825, mean -0.0155592, stddev 0.20352, skewness -0.0623741, kurtosis 0.525134 ) 
  b_w_r_m_   ( min -0.423329, max 0.40162, mean -0.000398497, stddev 0.0958193, skewness 0.00119238, kurtosis -0.0204401 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.992911, max 0.778985, mean -0.000155913, stddev 0.109231, skewness 0.00547655, kurtosis 0.0577176 ) , lr-coef 1, max-norm 0
  bias ( min -0.094005, max 2.55423, mean -5.30854e-09, stddev 0.084927, skewness 22.5544, kurtosis 647.201 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:305) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -8.67671, max 9.02116, mean 0.0321104, stddev 1.01793, skewness 0.450612, kurtosis 3.20408 ) 
[1] output of <BlstmProjected> ( min -4.41773, max 4.62047, mean 0.00195482, stddev 0.783244, skewness -0.0170014, kurtosis 1.33984 ) 
[2] output of <Tanh> ( min -0.999709, max 0.999806, mean 0.00207943, stddev 0.513364, skewness -0.00870367, kurtosis -0.753898 ) 
[3] output of <AffineTransform> ( min -18.0794, max 21.9897, mean 0.00429088, stddev 2.4785, skewness 0.869051, kurtosis 3.4276 ) 
[4] output of <Softmax> ( min 4.00506e-17, max 0.999986, mean 0.000780259, stddev 0.0220397, skewness 36.9854, kurtosis 1460.64 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:307) 
### BACKWARD PROPAGATION BUFFER CONTENT :
[0] diff of <Input>  ( min -5.48785, max 6.83323, mean -0.00925056, stddev 0.284602, skewness -1.62686, kurtosis 37.5579 ) 
[1] diff-output of <BlstmProjected> ( min -0.59304, max 0.6813, mean -0.000280442, stddev 0.0469981, skewness -0.010624, kurtosis 10.4853 ) 
[2] diff-output of <Tanh> ( min -0.894184, max 0.834449, mean -0.000317607, stddev 0.0607958, skewness 0.00128845, kurtosis 6.45668 ) 
[3] diff-output of <AffineTransform> ( min -0.999997, max 0.960047, mean -1.02069e-07, stddev 0.0164695, skewness -19.7301, kurtosis 2017.74 ) 
[4] diff-output of <Softmax> ( min -0.999997, max 0.960047, mean -1.02069e-07, stddev 0.0164695, skewness -19.7301, kurtosis 2017.74 ) 
### END BACKWARD


LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:308) 
### GRADIENT STATS :
Component 1 : <BlstmProjected>, ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  ### Gradients 
  f_w_gifo_x_corr_   ( min -50.4407, max 41.649, mean 0.0818875, stddev 3.50361, skewness -0.0800133, kurtosis 12.645 ) 
  f_w_gifo_r_corr_   ( min -52.7843, max 42.785, mean -0.00831769, stddev 2.60216, skewness -0.0990614, kurtosis 10.7079 ) 
  f_bias_corr_   ( min -46.1556, max 34.9864, mean 0.0754122, stddev 4.69113, skewness -0.484474, kurtosis 15.5016 ) 
  f_peephole_i_c_corr_   ( min -20.7536, max 22.573, mean 0.0423229, stddev 4.10325, skewness 0.277142, kurtosis 6.03694 ) 
  f_peephole_f_c_corr_   ( min -250, max 75.2341, mean -0.71332, stddev 18.3216, skewness -7.97457, kurtosis 107.094 ) 
  f_peephole_o_c_corr_   ( min -157.607, max 250, mean -0.593155, stddev 20.6774, skewness 4.07028, kurtosis 75.8621 ) 
  f_w_r_m_corr_   ( min -38.3433, max 46.9838, mean 0.0129205, stddev 4.08345, skewness 0.0950935, kurtosis 6.22966 ) 
  ---
  b_w_gifo_x_corr_   ( min -84.5733, max 66.218, mean 0.203428, stddev 4.53889, skewness 0.120222, kurtosis 52.1516 ) 
  b_w_gifo_r_corr_   ( min -33.389, max 30.8186, mean -0.00746928, stddev 2.60618, skewness -0.0187834, kurtosis 6.26482 ) 
  b_bias_corr_   ( min -67.7547, max 74.7867, mean -0.757278, stddev 7.16469, skewness 0.452947, kurtosis 29.8636 ) 
  b_peephole_i_c_corr_   ( min -46.3502, max 19.1859, mean 0.162986, stddev 4.8638, skewness -3.05389, kurtosis 28.6139 ) 
  b_peephole_f_c_corr_   ( min -31.8649, max 79.5654, mean 0.337293, stddev 10.3605, skewness 1.58453, kurtosis 11.7134 ) 
  b_peephole_o_c_corr_   ( min -75.1752, max 54.3313, mean -0.360555, stddev 14.4227, skewness -0.971929, kurtosis 6.36283 ) 
  b_w_r_m_corr_   ( min -41.0657, max 35.1292, mean -0.00682775, stddev 4.04913, skewness -0.0156403, kurtosis 3.06491 ) 

  ### Activations (mostly after non-linearities)
  YI_FW(0..1)^   ( min 0, max 1, mean 0.423148, stddev 0.349488, skewness 0.384349, kurtosis -1.30397 ) 
  YF_FW(0..1)^   ( min 0, max 1, mean 0.644566, stddev 0.322596, skewness -0.554444, kurtosis -0.990597 ) 
  YO_FW(0..1)^   ( min 0, max 1, mean 0.369649, stddev 0.358505, skewness 0.624846, kurtosis -1.16955 ) 
  YG_FW(-1..1)   ( min -1, max 1, mean 0.0183943, stddev 0.867991, skewness -0.0380497, kurtosis -1.79986 ) 
  YC_FW(-R..R)*  ( min -50, max 50, mean 0.344858, stddev 13.2619, skewness 0.14915, kurtosis 9.23255 ) 
  YH_FW(-1..1)   ( min -1, max 1, mean 0.0336881, stddev 0.691065, skewness -0.0588501, kurtosis -1.25773 ) 
  YM_FW(-1..1)   ( min -1, max 1, mean 0.00223968, stddev 0.333068, skewness -0.0794835, kurtosis 2.78882 ) 
  YR_FW(-R..R)   ( min -4.2393, max 4.34023, mean 0.0176497, stddev 0.772515, skewness 0.0119588, kurtosis 1.30862 ) 
  ---
  YI_BW(0..1)^   ( min 0, max 1, mean 0.456435, stddev 0.344144, skewness 0.243279, kurtosis -1.39943 ) 
  YF_BW(0..1)^   ( min 0, max 1, mean 0.641932, stddev 0.294874, skewness -0.547385, kurtosis -0.782285 ) 
  YO_BW(0..1)^   ( min 0, max 1, mean 0.378132, stddev 0.363511, skewness 0.568778, kurtosis -1.25572 ) 
  YG_BW(-1..1)   ( min -1, max 1, mean 0.0120191, stddev 0.865247, skewness -0.0228779, kurtosis -1.7975 ) 
  YC_BW(-R..R)*  ( min -50, max 50, mean 1.29384, stddev 12.1238, skewness 1.05259, kurtosis 10.4693 ) 
  YH_BW(-1..1)   ( min -1, max 1, mean 0.044883, stddev 0.695356, skewness -0.0548952, kurtosis -1.29444 ) 
  YM_BW(-1..1)   ( min -0.999998, max 1, mean 0.00418138, stddev 0.334434, skewness 0.0256751, kurtosis 2.46495 ) 
  YR_BW(-R..R)   ( min -4.41773, max 4.62047, mean -0.0137602, stddev 0.789802, skewness -0.0415239, kurtosis 1.40214 ) 

  ### Derivatives (w.r.t. inputs of non-linearities)
  DI_FW^  ( min -1, max 1, mean 1.14047e-05, stddev 0.0252073, skewness -1.32576, kurtosis 347.641 ) 
  DF_FW^  ( min -1, max 1, mean 7.75691e-06, stddev 0.021445, skewness 0.358199, kurtosis 574.971 ) 
  DO_FW^  ( min -1, max 1, mean 9.61504e-05, stddev 0.0280539, skewness 0.354744, kurtosis 99.8696 ) 
  DG_FW   ( min -1, max 1, mean -3.83617e-05, stddev 0.0321684, skewness -0.820866, kurtosis 390.237 ) 
  DC_FW*  ( min -76.5635, max 53.405, mean -0.00994765, stddev 0.840448, skewness -37.7825, kurtosis 3868.51 ) 
  DH_FW   ( min -5.65935, max 5.87012, mean -0.000197068, stddev 0.128613, skewness 0.0343666, kurtosis 74.287 ) 
  DM_FW   ( min -6.00024, max 6.8406, mean -0.000143173, stddev 0.333769, skewness 0.0297457, kurtosis 19.8338 ) 
  DR_FW   ( min -1.69779, max 1.44935, mean -0.000214752, stddev 0.0912102, skewness -0.00853655, kurtosis 11.749 ) 
  ---
  DI_BW^  ( min -1, max 1, mean -0.000161807, stddev 0.0264022, skewness -0.188596, kurtosis 410.918 ) 
  DF_BW^  ( min -1, max 1, mean -8.51841e-05, stddev 0.0177957, skewness 0.280505, kurtosis 433.212 ) 
  DO_BW^  ( min -1, max 0.850268, mean -0.000458683, stddev 0.022769, skewness -1.06924, kurtosis 110.45 ) 
  DG_BW   ( min -1, max 1, mean -6.70561e-05, stddev 0.0368332, skewness -1.27988, kurtosis 272.037 ) 
  DC_BW*  ( min -60.4915, max 29.0797, mean -0.00938918, stddev 0.625428, skewness -52.9079, kurtosis 4402.72 ) 
  DH_BW   ( min -4.29795, max 3.86778, mean -0.000351851, stddev 0.110936, skewness -1.13628, kurtosis 78.3395 ) 
  DM_BW   ( min -5.83619, max 5.53243, mean -3.49668e-05, stddev 0.290678, skewness -0.239145, kurtosis 22.2484 ) 
  DR_BW   ( min -1.68808, max 1.66138, mean -0.000354163, stddev 0.0951658, skewness -0.104483, kurtosis 13.9031 ) 
Component 2 : <Tanh>, 
Component 3 : <AffineTransform>, 
  linearity_grad ( min -93.6335, max 78.2429, mean -4.41587e-08, stddev 1.44175, skewness -0.338911, kurtosis 428.175 ) , lr-coef 1, max-norm 0
  bias_grad ( min -184.48, max 191.655, mean -5.96046e-09, stddev 7.86466, skewness 0.98214, kurtosis 511.477 ) , lr-coef 1
Component 4 : <Softmax>, 
### END GRADIENT

LOG (ali-to-pdf[5.5.1074~1-71f3]:main():ali-to-pdf.cc:68) Converted 2919 alignments to pdf sequences.
LOG (ali-to-post[5.5.1074~1-71f3]:main():ali-to-post.cc:73) Converted 2919 alignments.
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:168) MC05_98, missing targets
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:334) ### After 755062 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:335) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.813127, max 1.02813, mean 0.00383499, stddev 0.0924241, skewness 0.0196208, kurtosis 1.38729 ) 
  f_w_gifo_r_   ( min -0.417953, max 0.422586, mean -0.000591881, stddev 0.0797691, skewness -0.00326622, kurtosis 0.0449762 ) 
  f_bias_   ( min -0.358971, max 1.41457, mean 0.211323, stddev 0.467063, skewness 1.0564, kurtosis -0.647661 ) 
  f_peephole_i_c_   ( min -0.795716, max 0.479302, mean -0.00430439, stddev 0.14244, skewness -0.275116, kurtosis 3.52003 ) 
  f_peephole_f_c_   ( min -0.794434, max 0.917004, mean 0.00669835, stddev 0.191026, skewness 0.428515, kurtosis 4.22783 ) 
  f_peephole_o_c_   ( min -0.643463, max 0.53649, mean -0.0129355, stddev 0.197541, skewness 0.08604, kurtosis -0.0212903 ) 
  f_w_r_m_   ( min -0.507547, max 0.511603, mean 0.000567238, stddev 0.107093, skewness 0.000758426, kurtosis 0.0414588 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -1.78521, max 1.24475, mean 0.00614812, stddev 0.100926, skewness -0.196093, kurtosis 6.32091 ) 
  b_w_gifo_r_   ( min -0.37169, max 0.407157, mean -0.000166071, stddev 0.0740136, skewness 0.000132907, kurtosis -0.177203 ) 
  b_bias_   ( min -0.395846, max 1.23564, mean 0.203118, stddev 0.456732, skewness 1.02093, kurtosis -0.696836 ) 
  b_peephole_i_c_   ( min -0.492824, max 0.345471, mean 0.00178231, stddev 0.107968, skewness -0.249483, kurtosis 1.90944 ) 
  b_peephole_f_c_   ( min -0.669227, max 0.764493, mean 0.0167788, stddev 0.191197, skewness 0.585526, kurtosis 2.88105 ) 
  b_peephole_o_c_   ( min -0.601926, max 0.65114, mean -0.0161579, stddev 0.20846, skewness -0.0378945, kurtosis 0.609526 ) 
  b_w_r_m_   ( min -0.417634, max 0.413365, mean -0.000465865, stddev 0.0974354, skewness 0.00260483, kurtosis -0.00509405 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -1.00345, max 0.795666, mean -0.000155911, stddev 0.110019, skewness 0.00528258, kurtosis 0.0582955 ) , lr-coef 1, max-norm 0
  bias ( min -0.0954861, max 2.61578, mean -6.37956e-09, stddev 0.0871669, skewness 22.4336, kurtosis 641.638 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:336) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -14.8312, max 13.8231, mean 0.00769689, stddev 0.940803, skewness 0.731578, kurtosis 12.6034 ) 
[1] output of <BlstmProjected> ( min -4.10221, max 4.18164, mean -0.000305645, stddev 0.655614, skewness -0.0328687, kurtosis 3.09469 ) 
[2] output of <Tanh> ( min -0.999453, max 0.999534, mean 0.000261322, stddev 0.433199, skewness -0.000960404, kurtosis 0.105973 ) 
[3] output of <AffineTransform> ( min -11.9495, max 22.5021, mean 0.00506053, stddev 2.11076, skewness 1.0747, kurtosis 6.2059 ) 
[4] output of <Softmax> ( min 1.00614e-13, max 0.999984, mean 0.000781082, stddev 0.0197407, skewness 42.416, kurtosis 1891.76 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:338) 
### BACKWARD PROPAGATION BUFFER CONTENT :
[0] diff of <Input>  ( min -3.34266, max 3.77449, mean -0.00465511, stddev 0.237158, skewness -1.17674, kurtosis 33.4121 ) 
[1] diff-output of <BlstmProjected> ( min -0.689996, max 0.862018, mean -0.000194355, stddev 0.0367963, skewness -0.0841456, kurtosis 22.6183 ) 
[2] diff-output of <Tanh> ( min -0.803898, max 0.862301, mean -0.00023291, stddev 0.0485971, skewness -0.0460017, kurtosis 13.3993 ) 
[3] diff-output of <AffineTransform> ( min -0.996878, max 0.969473, mean -8.88939e-09, stddev 0.0129006, skewness -24.305, kurtosis 3414.24 ) 
[4] diff-output of <Softmax> ( min -0.996878, max 0.969473, mean -8.88939e-09, stddev 0.0129006, skewness -24.305, kurtosis 3414.24 ) 
### END BACKWARD


LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:339) 
### GRADIENT STATS :
Component 1 : <BlstmProjected>, ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  ### Gradients 
  f_w_gifo_x_corr_   ( min -56.6537, max 34.8322, mean -0.0991513, stddev 3.48725, skewness -0.338402, kurtosis 9.19072 ) 
  f_w_gifo_r_corr_   ( min -35.7924, max 32.3392, mean 0.00106333, stddev 2.93465, skewness -0.0208806, kurtosis 5.76133 ) 
  f_bias_corr_   ( min -22.8379, max 24.8305, mean 0.109455, stddev 4.1495, skewness 0.130934, kurtosis 3.08088 ) 
  f_peephole_i_c_corr_   ( min -32.9344, max 37.9823, mean 0.354377, stddev 6.43477, skewness 1.49559, kurtosis 11.1737 ) 
  f_peephole_f_c_corr_   ( min -61.5556, max 119.555, mean 0.782649, stddev 14.9941, skewness 1.89405, kurtosis 14.6023 ) 
  f_peephole_o_c_corr_   ( min -79.2712, max 129.323, mean 0.280329, stddev 19.4035, skewness 2.03054, kurtosis 14.5609 ) 
  f_w_r_m_corr_   ( min -36.0806, max 41.729, mean -0.0249857, stddev 4.33979, skewness 0.12964, kurtosis 3.78223 ) 
  ---
  b_w_gifo_x_corr_   ( min -81.3371, max 81.0778, mean 0.0505487, stddev 4.68539, skewness -0.892948, kurtosis 29.3293 ) 
  b_w_gifo_r_corr_   ( min -37.9649, max 43.9985, mean 0.00334335, stddev 3.07443, skewness 0.0249976, kurtosis 4.90546 ) 
  b_bias_corr_   ( min -57.6072, max 83.108, mean -0.370019, stddev 7.26325, skewness 1.65855, kurtosis 24.7705 ) 
  b_peephole_i_c_corr_   ( min -50.2487, max 57.8265, mean -0.347803, stddev 7.44873, skewness -0.785116, kurtosis 23.2857 ) 
  b_peephole_f_c_corr_   ( min -77.7113, max 71.1961, mean 0.319548, stddev 12.7526, skewness 0.196096, kurtosis 10.6316 ) 
  b_peephole_o_c_corr_   ( min -66.4353, max 56.0955, mean -0.489901, stddev 15.2289, skewness -0.662849, kurtosis 3.81886 ) 
  b_w_r_m_corr_   ( min -45.2165, max 55.3314, mean 0.0152328, stddev 4.66271, skewness 0.0228455, kurtosis 3.12626 ) 

  ### Activations (mostly after non-linearities)
  YI_FW(0..1)^   ( min 0, max 1, mean 0.316675, stddev 0.354647, skewness 0.77285, kurtosis -0.914354 ) 
  YF_FW(0..1)^   ( min 0, max 1, mean 0.472055, stddev 0.393048, skewness 0.0298578, kurtosis -1.60837 ) 
  YO_FW(0..1)^   ( min 0, max 1, mean 0.262589, stddev 0.340185, skewness 1.10123, kurtosis -0.312947 ) 
  YG_FW(-1..1)   ( min -1, max 1, mean 0.0144059, stddev 0.752662, skewness -0.0240038, kurtosis -1.40311 ) 
  YC_FW(-R..R)*  ( min -50, max 50, mean 0.269615, stddev 10.5142, skewness 0.349725, kurtosis 15.8375 ) 
  YH_FW(-1..1)   ( min -1, max 1, mean 0.0243786, stddev 0.587407, skewness -0.0277358, kurtosis -0.617289 ) 
  YM_FW(-1..1)   ( min -1, max 0.999997, mean 0.00211701, stddev 0.273899, skewness -0.0820493, kurtosis 5.3582 ) 
  YR_FW(-R..R)   ( min -3.764, max 4.18164, mean 0.00341607, stddev 0.629118, skewness 0.039622, kurtosis 3.09167 ) 
  ---
  YI_BW(0..1)^   ( min 0, max 1, mean 0.335921, stddev 0.357375, skewness 0.661421, kurtosis -1.08786 ) 
  YF_BW(0..1)^   ( min 0, max 1, mean 0.475219, stddev 0.379061, skewness -0.0309573, kurtosis -1.53724 ) 
  YO_BW(0..1)^   ( min 0, max 1, mean 0.278852, stddev 0.353277, skewness 1.00184, kurtosis -0.573162 ) 
  YG_BW(-1..1)   ( min -1, max 1, mean 0.0106558, stddev 0.75494, skewness -0.0162744, kurtosis -1.41185 ) 
  YC_BW(-R..R)*  ( min -50, max 50, mean 0.844553, stddev 9.8757, skewness 1.42697, kurtosis 16.0674 ) 
  YH_BW(-1..1)   ( min -1, max 1, mean 0.0319068, stddev 0.597586, skewness -0.013483, kurtosis -0.698749 ) 
  YM_BW(-1..1)   ( min -1, max 0.999999, mean 0.00206819, stddev 0.287776, skewness -0.0405362, kurtosis 4.41707 ) 
  YR_BW(-R..R)   ( min -4.10221, max 3.78734, mean -0.00402373, stddev 0.677295, skewness -0.0887784, kurtosis 3.10081 ) 

  ### Derivatives (w.r.t. inputs of non-linearities)
  DI_FW^  ( min -1, max 1, mean -0.000205892, stddev 0.0273838, skewness -1.82472, kurtosis 446.883 ) 
  DF_FW^  ( min -1, max 1, mean -1.66935e-05, stddev 0.0205073, skewness 4.85423, kurtosis 660.216 ) 
  DO_FW^  ( min -1, max 0.882175, mean -0.000108777, stddev 0.0260074, skewness -0.718893, kurtosis 141.099 ) 
  DG_FW   ( min -1, max 1, mean -0.00015983, stddev 0.0304165, skewness -0.865816, kurtosis 496.471 ) 
  DC_FW*  ( min -23.1572, max 16.5141, mean 0.000772008, stddev 0.393625, skewness 0.117237, kurtosis 722.597 ) 
  DH_FW   ( min -4.43696, max 4.33631, mean -3.04764e-05, stddev 0.124356, skewness 0.425307, kurtosis 116.646 ) 
  DM_FW   ( min -6.17943, max 5.53847, mean -0.000441655, stddev 0.307126, skewness 0.107682, kurtosis 28.8648 ) 
  DR_FW   ( min -1.23991, max 1.54327, mean -0.000367209, stddev 0.0856282, skewness 0.114962, kurtosis 20.7343 ) 
  ---
  DI_BW^  ( min -0.60543, max 1, mean -0.000160654, stddev 0.0159723, skewness 3.75634, kurtosis 326.344 ) 
  DF_BW^  ( min -0.728287, max 0.876935, mean -3.26744e-05, stddev 0.0115808, skewness -0.0854147, kurtosis 446.884 ) 
  DO_BW^  ( min -0.507843, max 0.524082, mean -0.000361171, stddev 0.0153022, skewness -0.671828, kurtosis 91.5841 ) 
  DG_BW   ( min -1, max 1, mean 2.13324e-05, stddev 0.0251914, skewness -1.64096, kurtosis 401.04 ) 
  DC_BW*  ( min -23.1214, max 15.0715, mean -0.000361931, stddev 0.340689, skewness -17.3824, kurtosis 1702.3 ) 
  DH_BW   ( min -2.80276, max 2.69413, mean -0.000115278, stddev 0.0823661, skewness -1.71665, kurtosis 105.536 ) 
  DM_BW   ( min -4.83708, max 3.34705, mean -5.09949e-05, stddev 0.205647, skewness -0.265723, kurtosis 20.707 ) 
  DR_BW   ( min -0.878164, max 0.9895, mean -0.000228545, stddev 0.0679373, skewness -0.0520268, kurtosis 11.0528 ) 
Component 2 : <Tanh>, 
Component 3 : <AffineTransform>, 
  linearity_grad ( min -82.8144, max 72.7374, mean -5.54548e-08, stddev 1.6172, skewness -0.190771, kurtosis 197.812 ) , lr-coef 1, max-norm 0
  bias_grad ( min -183.472, max 136.016, mean 8.34465e-08, stddev 7.273, skewness -8.44943, kurtosis 423.104 ) , lr-coef 1
Component 4 : <Softmax>, 
### END GRADIENT

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:346) PER-CLASS PERFORMANCE:
@@@ Frames per-class: [ 181212 144 127 606 532 43 173 190 515 703 138 211 211 608 45 289 271 572 372 153 368 892 307 454 467 612 374 177 314 165 203 169 193 120 162 350 506 100 142 40889 89709 154 463 1777 286 231 687 352 545 459 79 394 481 237 200 663 1037 475 2210 428 257 167 4192 1910 93 230 151 223 4363 199 339 612 931 689 168 53 430 425 404 147 72 468 398 184 543 242 103 448 1376 60 779 717 668 146 541 163 358 830 561 408 249 268 103 1105 608 477 208 629 257 136 769 185 359 2082 165 69 303 74 247 336 172 147 117 128 443 612 117 125 52 316 956 165 305 574 202 297 222 1167 208 209 172 556 182 606 267 121 534 323 470 239 776 321 812 179 268 1139 537 191 554 5133 217 193 202 128 264 242 282 141 453 190 390 248 274 208 168 117 250 72 227 832 107 208 74 120 577 200 74 173 93 435 313 308 638 462 201 239 312 172 59 151 330 221 210 68 77 424 248 789 348 126 86 566 208 142 331 485 286 372 234 328 637 273 293 132 41 150 57 102 173 655 328 426 861 178 245 87 533 72 131 265 150 790 182 357 233 69 187 112 206 273 85 62 302 205 411 179 278 260 193 525 723 141 288 139 513 788 215 97 150 236 273 455 166 615 208 63 196 43 120 342 911 206 87 1156 192 146 131 166 31 221 252 417 373 499 401 233 179 409 182 439 579 96 189 251 1812 272 55 134 245 227 120 170 454 69 211 126 232 303 701 89 59 87 90 418 122 334 282 91 119 125 379 59 189 157 163 620 120 203 224 199 71 381 86 544 80 247 220 94 114 76 256 175 1186 420 378 336 135 140 331 666 306 356 196 203 291 152 221 543 214 149 360 365 670 264 264 692 216 616 923 524 408 234 395 301 375 105 525 80 359 98 456 204 309 258 288 286 181 90 101 263 53 253 167 354 150 174 194 393 579 494 437 273 142 236 159 226 115 606 198 255 508 136 282 304 327 234 1467 199 113 213 747 267 237 238 267 223 423 127 334 351 773 400 668 269 762 507 73 172 259 944 420 60 318 817 125 150 155 430 99 155 360 316 425 603 146 124 301 210 170 102 140 531 288 716 282 161 233 181 349 120 133 242 135 51 141 111 147 205 551 427 131 99 109 133 445 7274 204 271 135 128 125 377 120 372 449 808 108 557 45 280 47 596 57 276 400 121 347 315 193 77 146 96 105 216 193 341 1125 236 219 105 417 109 338 639 130 396 177 211 138 121 275 91 104 194 51 542 461 346 197 315 121 109 444 210 64 135 84 52 96 283 268 315 144 187 498 569 470 194 79 151 71 285 409 404 165 215 321 466 69 209 585 117 83 141 150 364 478 225 439 81 192 156 773 344 306 243 112 146 203 501 258 143 117 231 316 156 83 53 468 163 539 465 176 116 79 157 350 139 86 253 155 101 132 342 440 324 189 390 153 111 192 365 208 291 263 103 139 134 307 402 452 451 5943 88 406 111 189 488 786 548 86 632 99 382 216 141 365 388 65 90 187 430 300 463 604 247 79 314 305 463 45 473 718 330 223 226 1312 201 160 62 170 227 141 90 90 193 68 539 610 241 156 182 257 224 114 99 209 178 32 193 174 235 238 150 123 308 261 195 275 142 111 371 821 518 831 306 124 432 87 357 932 174 327 225 300 36 190 272 120 358 461 397 462 306 105 348 166 390 216 520 219 260 71 179 258 429 201 53 142 256 406 60 128 173 26 88 191 83 60 155 179 1000 125 44 191 230 383 687 216 257 321 233 417 202 1061 342 327 163 248 165 412 229 303 517 124 815 539 79 306 155 255 89 321 245 73 308 228 196 192 269 176 228 101 8591 469 337 966 237 430 177 258 334 85 304 414 487 401 57 178 99 260 178 151 248 261 926 126 248 230 264 241 147 220 155 139 646 219 209 393 1097 85 160 936 605 267 175 213 208 155 166 207 194 260 180 542 294 745 650 41 235 409 387 268 281 172 798 467 256 138 171 316 378 542 197 77 199 253 693 506 326 151 543 272 142 171 242 241 191 161 274 304 47 226 320 127 248 43 388 236 233 244 332 111 261 196 246 68 466 707 315 146 315 282 30 427 201 153 510 1026 278 186 316 45 464 106 316 404 415 543 346 257 452 1347 138 153 366 558 585 171 109 855 530 221 155 160 161 111 196 131 341 863 116 777 103 428 210 393 95 260 221 295 129 320 516 185 97 126 343 492 405 270 645 150 141 159 382 1153 426 252 535 332 205 417 575 597 292 372 180 236 213 343 301 154 349 528 290 243 378 616 213 266 230 676 309 338 165 324 314 639 310 148 365 184 119 116 199 264 1367 367 467 1442 213 598 641 335 156 122 351 114 129 98 489 213 162 386 116 74 274 197 265 173 129 167 753 211 214 1711 286 746 207 83 123 191 204 353 783 57 223 151 323 285 99 1197 111 116 189 130 401 488 428 96 280 248 468 506 117 284 315 328 129 334 124 557 265 243 383 268 66 97 474 153 221 139 391 194 634 156 322 748 309 195 326 651 462 370 172 385 490 392 610 304 249 70 501 324 335 228 370 206 587 560 108 850 256 209 153 168 206 131 204 232 340 200 284 174 301 124 107 220 67 370 1390 1253 311 434 127 136 104 493 416 778 103 335 338 242 66 208 459 231 96 291 160 220 368 145 728 458 351 255 350 348 137 125 1039 1466 285 221 232 361 605 549 4192 543 376 361 3105 243 319 117 225 232 542 309 859 113 244 913 235 271 853 881 252 243 274 129 90 371 274 176 453 107 172 140 172 411 267 778 147 558 2509 132 95 258 162 203 87 21 467 192 231 265 665 552 160 225 162 279 3235 297 303 235 317 297 155 692 494 82 179 192 734 178 103 204 87 225 63 351 225 260 282 352 367 219 77 182 384 386 147 340 273 260 234 61 251 141 663 194 320 94 144 ]
@@@ Loss per-class: [ 0.210583 0.571742 0.851377 0.482822 0.628809 0.850642 0.851383 0.587697 0.732146 1.02602 0.508345 1.07646 0.918676 0.616881 1.27215 0.655216 0.748882 0.947397 0.968611 0.747324 0.579191 0.865533 0.935144 0.534519 0.406546 0.71831 0.466827 1.75996 0.741003 1.21933 0.817884 0.539138 0.739615 1.06042 0.466004 0.580994 1.25981 1.28485 1.0401 0.699117 0.319285 0.886366 0.92759 0.693206 0.941217 0.748917 0.42211 1.06666 0.564524 0.487226 0.80564 1.13883 0.866705 1.16083 0.824223 0.78186 0.560962 0.45361 0.35615 1.0656 0.885863 0.74283 0.246556 0.333761 1.28214 1.23451 1.35812 0.898711 0.780399 0.432885 0.584206 0.409604 0.410744 0.844768 0.810565 0.875028 0.305848 0.216698 0.477639 0.527608 0.780982 0.914105 0.305497 0.565608 0.691279 0.944245 1.24412 1.08137 0.718885 0.482985 0.476585 0.48065 0.660425 1.59215 1.1216 1.6089 0.590906 0.423321 0.614307 0.373465 1.45614 0.682783 1.17165 0.442406 0.407981 0.628297 1.22334 0.820868 0.662807 0.614003 0.92807 0.988568 1.50731 0.409726 0.98816 2.13099 1.07982 0.650055 1.48127 0.887512 0.957421 1.33571 1.11764 0.991572 0.525239 0.466211 0.727011 1.37208 0.76911 0.610671 0.273546 1.00835 0.512581 0.69553 1.46231 0.455611 0.533828 0.318687 0.976288 1.13688 1.16838 0.755452 0.656624 0.642771 0.464471 0.43719 0.988168 0.64403 0.619322 0.489733 0.213204 0.700435 0.704499 1.20213 0.989368 0.529279 0.327796 0.498397 0.596886 0.339564 0.577989 0.944872 1.10915 2.311 0.830088 1.11182 0.723271 0.996953 0.515488 1.23342 0.437338 0.681939 0.690562 0.853421 0.950533 0.473513 0.650427 0.556228 1.43484 0.264098 0.884493 1.03691 0.822849 0.78126 0.221967 0.99093 1.58448 1.12102 1.25801 0.61232 0.61679 0.844636 0.341302 0.540477 0.77679 0.919903 1.29592 1.24526 1.06934 0.504807 1.5391 1.26273 1.04628 1.55751 1.44421 0.78599 1.56283 0.769964 0.78024 1.03844 0.965666 1.06232 0.62229 12.2303 1.22827 0.615419 1.1019 0.542981 0.560821 1.19837 0.747121 0.555381 1.45482 0.625298 0.885809 1.23098 1.70112 0.419489 0.725415 0.715084 0.940351 0.654678 0.690201 1.93224 0.64122 1.51838 0.439889 1.95269 0.906004 0.870254 0.533087 0.425627 0.927619 0.676257 1.02764 2.01868 0.568032 3.24349 1.39767 1.06298 1.76355 0.892484 1.05326 0.992317 0.996796 0.649026 1.72629 0.662639 0.624687 0.43818 0.71322 1.11664 0.9894 1.99473 0.734251 0.586888 1.37677 0.899994 1.08579 1.00769 0.901886 0.963006 1.69111 1.43548 1.96502 1.05002 0.655376 1.92002 0.772563 1.46619 0.917621 1.12639 0.941619 0.530546 0.597669 1.34766 1.51198 0.845546 4.12371 1.77087 1.32238 1.64493 0.536547 0.33229 1.5691 0.881575 0.789695 0.662804 0.834471 1.177 0.513256 0.676596 0.969564 0.722034 0.242519 0.562779 1.21229 0.847997 1.09548 1.62552 0.993209 0.920762 0.763849 0.787045 1.22908 1.61543 0.362739 0.985757 0.462224 0.97224 1.51903 0.5676 1.6939 0.473958 1.26742 2.02983 1.27053 2.09223 1.01246 0.789941 0.710765 1.02649 1.38617 0.860981 1.02965 0.503738 1.49916 1.228 1.04527 0.649461 0.742573 0.819987 0.400159 0.788229 0.882404 0.709725 0.891974 0.944831 0.693682 0.888127 0.904212 1.09874 0.271605 0.248709 1.00518 0.823619 0.834183 1.06774 1.30642 0.330282 0.64705 1.59292 0.851894 2.09227 0.576995 1.41297 0.419816 0.95178 0.59437 0.737795 1.30687 1.25053 0.631691 1.00628 0.871321 0.55168 0.562983 0.651862 0.579799 0.920671 0.962157 1.01972 0.521388 0.677706 0.824168 0.817908 0.766938 0.993881 0.797987 1.11781 0.872395 1.04171 0.980447 0.568152 0.394438 0.825579 1.10219 0.585746 1.13744 0.613821 1.8107 0.491798 0.818219 0.552216 1.50429 0.960095 0.829212 1.07838 0.913609 1.07717 1.98703 1.6217 0.830346 0.833153 1.11383 0.553108 0.992041 0.394958 1.99241 0.69708 0.421615 0.495024 0.535466 0.571357 0.688363 0.999039 0.227805 0.447787 1.3928 0.567343 0.691955 1.43325 1.22492 1.16399 0.769064 0.527839 0.625942 0.640267 0.765907 0.843567 0.430826 1.27468 0.382667 1.10315 0.933874 0.624215 1.46568 0.86043 0.846281 0.276502 0.855541 1.01156 0.94875 0.467716 0.653893 3.00455 0.363818 0.824951 0.794342 1.06566 0.559028 0.852267 1.336 0.837754 0.881743 1.77946 1.19017 0.828784 0.988983 0.943165 0.412259 0.364382 0.845309 0.573199 0.811638 0.445759 0.511806 0.647441 0.916909 0.91476 2.74321 1.24695 0.576686 0.928181 0.799178 0.528611 1.13964 0.832773 0.342364 1.07396 0.81704 1.01013 1.8316 1.37102 0.571833 0.469404 0.588583 0.985882 1.77112 0.460232 1.24858 0.796188 1.04334 1.36762 0.370676 0.880007 1.8642 0.482932 1.49556 0.815354 1.83711 0.430731 1.26095 0.705375 0.755956 1.01408 1.28179 0.638996 1.80503 1.06005 1.78787 1.08397 0.943333 2.18339 1.40156 0.639346 0.356512 1.26818 1.03715 2.05047 1.57512 1.77477 0.992315 0.883164 1.11212 1.39301 0.895355 0.848777 0.811312 1.57267 1.11958 0.940664 1.06689 1.22823 1.25984 0.167695 0.64318 1.01601 0.657988 0.818133 1.21889 2.21926 0.88572 0.70262 1.29809 0.681941 1.14385 1.36446 1.56375 1.47646 1.92641 1.16987 1.24262 0.884664 0.476835 0.555537 0.423353 0.687268 1.38409 1.08999 0.930815 0.857879 0.650115 0.604742 1.03083 1.60095 0.779629 0.710661 1.1929 0.860056 0.924121 1.56531 0.999314 2.13566 0.844761 0.919906 0.648337 1.29146 1.05012 1.01182 0.666981 0.782637 0.56786 0.463685 1.41294 1.61238 1.82307 1.18251 1.64193 0.764295 1.3289 1.64185 1.69751 1.66006 0.996878 0.562888 0.693314 1.36896 0.352154 1.1865 0.761326 0.700839 0.993566 0.84935 0.771995 1.21175 1.80207 0.768303 0.733638 1.66045 1.25793 1.01754 1.17162 0.584891 0.808009 0.622087 0.959533 1.36822 0.813693 1.13732 1.8804 2.16791 0.930589 1.16544 0.518431 0.659309 0.496853 1.18641 1.034 1.04229 0.591102 0.665264 0.52507 0.640194 0.675249 0.221687 1.47394 0.822834 0.564591 0.698956 2.7027 0.430102 1.16836 0.627894 1.24234 1.22112 0.777589 0.840712 3.32959 1.00445 0.753107 1.01462 0.477161 0.56901 0.607914 1.13529 1.13439 0.644097 1.34867 0.495692 3.52494 0.680445 0.48495 0.911729 1.91041 1.92367 0.722622 0.825339 1.49922 1.72098 1.42365 0.681954 0.891542 0.932294 0.691706 1.11422 1.38451 0.624639 0.622645 1.41251 1.49116 0.770636 1.18384 0.560679 1.1319 1.11791 1.49371 1.90634 1.91296 0.930027 0.611579 0.473007 0.733208 0.928 1.07943 1.00029 1.02221 0.952793 0.559627 0.920768 2.34363 0.585876 0.325599 0.610836 0.321921 0.669615 0.814027 0.949784 0.72784 2.41593 1.51078 0.763247 0.909585 0.701831 0.957944 1.81975 1.31624 1.45799 0.972432 0.539161 0.713887 0.562319 0.999227 1.54539 1.22162 0.900422 1.35308 1.03217 1.28174 0.456512 1.02911 0.921046 0.751907 0.766458 0.887327 0.723866 0.805847 2.40064 0.881273 1.27797 0.855213 1.13414 1.32603 1.04132 1.16272 1.87523 1.15469 2.29597 1.43465 1.03808 1.18814 0.39233 1.53123 1.58817 1.83258 1.21266 1.10352 0.401184 1.24709 1.07311 0.78292 0.851868 0.790518 0.657285 1.09733 1.21182 1.76572 2.0606 0.717619 0.649093 0.686078 0.606414 1.16876 0.720557 1.73672 0.317475 0.486545 1.25899 0.997372 0.543374 0.842398 2.2886 0.622961 0.842457 2.28596 0.393103 1.17251 1.58568 0.976096 0.835978 1.5765 3.57039 1.12631 0.166551 0.531866 1.16651 0.482378 1.5664 0.902772 1.70274 0.82992 0.569136 3.33902 0.703509 0.928939 0.883864 1.12671 2.31007 1.65848 1.77291 0.536593 0.693984 1.29617 1.32008 0.784309 0.644331 2.12288 1.20189 1.09014 0.988167 0.851015 1.98918 1.11346 0.690172 1.35943 0.780159 0.969188 1.39521 1.01214 0.550852 2.06323 1.0762 0.541837 0.714966 0.809388 0.763372 1.36657 1.28452 1.25956 1.04757 1.51648 1.32462 1.73835 1.36658 0.770072 0.953204 0.443277 0.794286 1.46965 1.02377 1.27106 0.687979 0.883662 0.733123 1.57944 0.725649 0.821719 1.13298 1.2887 0.358889 1.00794 1.15122 0.69316 1.20521 1.54977 1.59092 0.933909 0.583143 0.772631 1.1319 0.675589 0.574926 0.580486 1.96485 1.45749 1.11476 0.89637 1.49134 0.706867 0.852173 0.654076 0.556042 0.978577 0.733881 1.26507 0.552672 1.02248 0.71684 1.0977 1.20229 1.23021 0.771302 1.46171 0.777179 1.44378 0.671915 0.936704 0.552142 0.448017 0.885549 2.13086 0.697721 0.698999 2.48904 0.788267 1.59939 1.11682 0.756241 0.958527 1.0271 0.59701 1.3332 2.58209 1.356 1.11795 0.93484 0.71428 1.18914 1.53674 1.59586 0.591072 0.70554 0.850344 1.38577 1.28188 0.695372 1.2924 1.02188 1.26584 1.70718 0.811962 0.68839 1.1394 1.52759 1.04011 1.17888 1.30458 0.7626 1.70617 0.796218 0.722154 0.528498 0.672043 1.05215 0.632021 0.570986 0.913418 0.85581 1.40125 0.95978 1.55707 1.08943 1.84637 0.939553 1.2837 0.886733 2.23028 0.96501 0.690634 0.881519 0.881871 0.712491 1.15596 1.34945 1.20103 1.56636 0.707608 0.916806 1.49309 0.840372 0.487024 0.922379 0.486811 0.540833 1.12325 0.816228 0.644082 0.915989 1.02909 0.935539 0.817309 0.7454 1.22638 0.952445 0.985758 0.634171 0.838114 0.768235 0.871549 0.945645 1.46037 0.966485 0.580645 0.753501 0.816186 1.04053 0.710198 1.04938 0.796003 1.70389 1.59018 1.1205 0.75119 0.655991 1.13498 1.22097 1.2626 0.850831 1.46415 1.00224 0.442128 0.60326 0.944423 0.387602 0.845992 0.559799 1.89874 1.27003 0.743272 1.32168 1.65954 0.832852 1.15737 0.986508 1.29853 1.76185 1.60919 1.24995 1.40072 1.22063 1.43018 1.13321 1.1828 1.43824 0.396213 1.77783 0.559107 1.88458 0.614938 1.68605 1.02417 1.85197 1.45416 1.46526 1.21436 0.282573 1.49713 0.602914 1.17904 1.1679 1.2148 2.23092 0.594678 1.60665 1.2128 0.779654 1.05943 0.627678 0.896714 0.497027 1.77415 0.899901 1.13599 1.10756 0.689681 1.35394 0.957274 0.981697 1.04132 1.61141 1.04835 1.53347 1.76629 1.578 0.922167 0.494676 0.641316 1.00437 1.66986 0.616948 1.31477 1.61394 1.53379 0.89095 1.08705 0.804079 1.41121 0.547148 0.771425 0.773835 0.797578 1.01322 0.892936 1.03319 0.635278 1.43696 0.960283 1.2842 0.509085 0.545563 0.90132 1.64481 1.96868 1.36137 1.18925 1.35569 1.04601 0.565285 1.39516 0.650881 0.789702 0.717765 1.01707 1.54891 0.874137 1.006 0.431029 1.09381 1.66115 0.592584 1.01367 1.00607 1.30833 0.875332 1.28101 0.860548 1.47896 2.45404 1.179 1.04969 1.22798 0.271942 0.265909 0.605205 1.23417 1.34504 1.53663 0.980904 1.02507 1.25471 0.593297 0.971771 1.07784 0.853852 0.682909 2.47583 1.1101 0.620659 0.666957 1.12249 1.3201 1.29432 0.791207 1.78996 1.28657 0.667529 0.633365 0.393669 1.05828 0.9165 0.489495 0.75445 0.825388 0.695606 0.455865 1.31777 0.714132 1.10097 0.539954 0.683868 0.81069 0.378863 1.32957 1.05275 0.781779 0.444949 1.36313 0.916771 0.97655 1.14334 0.900864 1.31419 1.46289 0.36395 1.98614 0.324177 0.641503 1.07371 0.93244 1.19803 0.469749 0.806258 1.38534 1.42537 0.873505 2.17903 1.38525 0.66454 1.01688 1.26167 1.77182 1.1348 1.84478 0.650393 1.54627 1.70963 1.04706 0.991047 0.941809 0.539405 0.961885 1.11743 1.44161 0.565304 1.13853 1.22858 4.19956 0.968741 0.749569 1.38326 1.52187 1.07752 1.77595 1.70398 0.750636 0.531751 1.25198 0.524778 0.833111 1.44974 0.898504 1.24315 2.11064 1.43311 0.816687 1.25495 1.59774 1.81953 1.35912 0.525235 1.08263 1.16866 1.18414 1.96185 0.86295 2.56707 1.16762 1.24194 0.862313 1.14986 0.559896 0.941141 0.600885 1.91864 0.657065 0.594478 0.918826 1.08776 1.08253 0.831335 1.58892 0.688007 0.782998 0.748533 1.28611 0.62116 0.806738 1.3658 0.962688 1.08934 ]
@@@ Frame-accuracy per-class: [ 92.46 85.1211 68.2353 88.211 81.5023 68.9655 75.5043 83.4646 75.2667 64.8188 86.6426 70.4492 73.2861 83.977 54.9451 81.5199 75.5064 71.0917 64.9664 76.873 83.5821 70.8123 66.9919 84.7085 91.123 77.551 88.6515 35.493 80.1272 61.0272 74.2015 80.826 77.5194 62.2407 84.9231 82.7389 58.8351 53.7313 67.3684 74.3272 88.237 71.1974 67.7454 76.962 65.9686 73.8661 84.8 66.0993 81.0266 85.963 80.5031 62.3574 72.0665 63.1579 78.803 72.1929 78.5542 87.2766 90.6582 62.077 69.1262 75.8209 91.8545 89.1913 60.9626 48.59 49.505 72.4832 77.3003 86.2155 81.296 86.8571 88.6742 71.9362 76.5579 72.8972 89.4309 96.5922 85.5377 92.8814 81.3793 71.9317 90.3388 85.6369 83.1647 68.866 65.7005 71.5719 76.3531 94.2149 83.3868 87.5261 78.6836 51.1945 67.9594 50.1529 83.682 87.5376 79.4301 88.1273 52.1042 81.5642 57.0048 86.929 88.5785 79.1623 66.6667 77.8396 77.6699 80.5861 69.7856 76.0108 52.2949 87.587 65.861 38.8489 63.9209 88.5906 47.2727 73.4027 72.4638 67.7966 68.0851 66.9261 83.8782 81.4694 89.3617 63.745 78.0952 82.1485 90.4339 70.0906 80.8511 78.8512 53.3333 86.0504 87.191 89.1649 75.2998 63.9618 59.1304 73.3154 82.1918 78.4831 88.972 88.0658 67.5398 81.6074 76.3018 81.8372 93.754 79.9378 77.4154 62.9526 68.1564 82.1413 91.7209 84.5953 81.5149 88.2049 84.1379 76.4858 66.1728 35.7977 74.1021 67.6289 77.5221 72.0848 83.7927 55.643 85.7875 82.0926 75.7741 75.2998 72.4036 85.1064 79.4411 84.1379 58.9011 91.0511 74.4186 63.789 79.1946 72.1992 93.5065 61.8454 44.2953 60.5187 55.615 81.5155 82.6156 71.9611 89.7416 81.9459 71.464 73.0689 52.16 64.9275 73.9496 87.1287 49.9244 56.8849 69.3587 52.5547 54.1935 75.1472 46.6801 76.8841 77.1879 69.5652 72.8324 60.3707 79.6163 0 61.5385 82.1833 63.8743 85.6376 78.8913 65.449 76.2353 80.8044 62.6917 83.0189 72.2892 55.1495 36.5217 97.561 74.3516 75.2098 73.6682 77.8429 80.4411 43.1373 81.8737 51.4286 87.3477 30.3448 68.4411 71.9397 89.701 83.9975 70.137 78.8811 69.379 23.0216 86.4 25.7778 56.1743 65.4479 50.2924 73.6 68.0992 72.5061 71.2029 79.1086 60.6822 76.3916 81.137 88.6775 78.369 65.0177 73.1369 55.1971 75.7546 82.3082 58.4687 81.0256 52.4917 74.8414 72.7605 65.6422 49.2492 49.5532 42.6859 72.4409 87.0229 41.3793 75.5187 55.4745 73.1761 64.891 61.7143 83.7008 80 62.116 55.5133 73.8739 3.1746 53.7246 58.2178 50.7784 83.8019 91.8919 49.5641 75.3747 78.5515 78.6325 80 64.3914 85.9362 84.9741 65.4354 73.161 92.2483 86.2385 66.6667 72.8625 67.2098 51.8681 71.3693 72.7273 75.9076 79.1367 64.3026 52.1739 92.4731 73.1466 87.5267 70.3911 48.7395 81.1429 51.9337 87.2162 57.1429 34.6786 56.2832 52.459 69.4561 78.8845 79.0514 57.1429 59.6306 76.1905 66.6667 81.5471 55.6017 67.8133 70.3786 83.208 79.7203 72.8702 95.9538 76.9513 70.8075 80 72.1088 69.8413 78.6026 77.1242 72.1248 62.6781 92.6254 93.4602 70.0132 75.1857 74.5387 71.8861 62.4434 89.2723 82.5449 55.8205 71.2468 34.398 85.0772 49.8361 88.9391 73.4131 79.7203 80.2676 59.0846 64.8427 79.7912 70.6994 69.9433 87.9422 84.5266 77.8589 81.8625 71.306 71.2362 65.6716 84.7029 77.2803 72.7031 75.8294 76.3083 60.8696 77.886 64.9746 74.4797 68.9487 75.9289 81.2379 90.1213 76.0908 68.3196 88.3978 62.069 80.0759 33.6449 89.9408 80 84.3441 56.4784 75.0716 73.0077 64.803 70.9232 65.5207 43.2 56.3071 76.4912 75.2643 66.4577 87.4172 71.8615 89.2003 37.7834 79.8434 87.3156 84.9817 84.6018 81.7734 80.916 70.7889 94.3782 86.7168 59.9119 82.4356 77.4582 57.5701 62.7368 64.9895 75.1402 82.774 83.353 80.7843 77.728 75.1067 85.9729 64.6692 86.3126 70.1299 71.3443 83.1527 42.1769 74.2029 77.4566 91.0535 72.2949 61.157 73.1554 83.4251 86.0558 0 85.5305 78.2811 68.3417 68.1672 81.8308 70.4581 54.2891 71.4167 77.1331 49.7992 60.6965 74.1093 67.4487 75.122 88.2562 90.6867 74.87 83.6008 74.6903 86.6873 82.6552 74.3802 72.9614 73.029 34.4569 63.5052 87.8229 75.7282 79.1519 88.7892 63.0508 77.8589 90.6618 66.4327 74.5247 70.3518 39.2694 58.427 81.0325 81.4077 84.1076 73.6648 47.9705 89.4942 58.9641 71.2583 56.4315 61.4765 90.7675 72.4799 38.7097 86.6368 54.9451 74.5098 65.2632 88.5163 55.6522 74.141 74.6567 74.0741 61.295 77.0206 41.8605 65.8065 47.7816 68.3938 68.2464 49.8845 55.2972 77.5988 88.494 63.4249 73.8041 62.5592 54.1317 34.7032 69.4239 68.8038 58.2375 65.0694 69.2958 74.2317 75.0903 40.3292 58.8022 74.3169 74.6411 64.2674 66.0194 96.0369 80.6067 69.2641 83.5443 80.1902 59.2593 38.3562 74.2407 76.0095 57.3643 77.4908 65.0888 64.7619 46.6321 58.2011 41.7132 62.7575 61.5917 74.6667 84.654 83.4065 89.9044 80.2057 50.3145 74.5875 79.7203 72.1541 79.3651 81.0878 68.8822 58.4687 75.5832 80.6002 60.4317 75.895 69.1716 62.1277 67.0659 31.8021 73.0897 71.8793 80.4598 57.6497 68.2594 74.8466 85.1948 79.2332 80.2844 86.2119 56.1175 44.7639 56.8889 64.1638 52.5799 76.3709 64.2166 57.1429 50.2128 50.108 66.3507 88.1789 79.0419 48.5981 87.5133 66.055 77.1084 85.0698 68.5552 72.103 69.1824 65.3968 42.5107 83.1541 78.6127 48.1262 54.6624 76.8473 70.9434 82.0438 75.3689 81.9723 70.7124 59.6671 66.4495 66.3677 32.7273 43.5021 75.2998 64.1509 83.4915 81.1594 86.0215 57.2491 67.9675 72.5466 85.7459 79.2913 80.9119 84.7458 77.2448 98.6547 55.409 78.4033 84.0432 77.6664 5.78035 86.0079 60.3015 81.3072 60.5081 53.7102 80.1642 71.8147 13.7405 68.5083 69.3333 66.6667 86.8552 78.5329 81.2242 62.6263 65.4088 82.3529 58.5925 86.2999 19.7802 78.9863 85.7342 68.0787 43.4004 42.3841 77.3333 75.9305 46.729 46.4 51.6129 80.8791 74.2049 77.3481 78.453 59.9483 51.0949 83.5959 80.2621 47.205 54.3131 75.0685 60.5825 86.8597 63.7555 63.3166 50.5967 34.7339 33.8462 68.2171 83.0946 86.6242 79.6646 75.0831 62.3482 66.7747 71.8929 73.6573 79.4918 75.7895 34.0807 80.7537 91.4181 83.5101 91.7619 80.5873 77.9116 72.3699 82.2857 38.8811 53.2976 80.8023 71.145 81.153 61.8968 8.21918 61.9423 63.1193 64.7303 82.2873 80.39 84.7799 69.6216 55.1387 53.0806 70.3013 58.2583 68.1178 59.1224 89.9135 69.2483 72.1689 74.1259 81.337 70.793 75.4366 74.4417 24.2991 72.9825 67.0565 77.9828 67.7686 52.1401 69.1643 45.283 56.4972 57.9634 41.9162 56.1983 66.881 69.0808 87.8561 46.2151 47.191 31.8538 55.5315 66.7536 88.4364 59.5843 68.3495 78.3826 76.2313 76.4072 78.0247 57.9369 68.6131 42.7481 34.2508 78.8732 82.1752 79.2727 83.2244 68.5338 80.5797 52.2088 91.6002 86.5616 59.1195 74.062 87.4598 72.7984 46.9274 78.6936 75.3564 42.1769 91.41 70.8972 53.944 70.6494 75.3247 47.0255 33.698 67.9803 94.093 81.7891 57.1852 88.3601 54.3158 79.9071 52.9577 73.1141 81.9133 18.7135 79.1461 65.6212 73.4359 64.259 38.2609 50.4202 38.191 80.6142 83.4734 61.3861 59.1549 81.4532 79.007 39.5257 61.167 70.282 71.4556 77.4327 47.4576 70.7483 77.8135 55.914 72.8538 69.2483 58.7112 64.5489 84.3736 37.4269 74.1433 82.4346 81.5855 72.8972 79.7721 57.1429 61.3909 53.3762 63.0631 58.7952 59.6401 39.1555 48.7535 77.6037 71.6469 85.1777 73.6357 50.6024 63.2696 61.7827 76.3871 69.2737 77.7975 47.5362 74.8904 75.9358 68.616 65.704 91.5452 71.09 66.3144 75.9447 54.1772 59.3548 58.1454 66.6667 82.4802 75.0247 65.8499 80.5281 87.2125 86.2385 51.9298 49.5627 68.866 69.9793 50.6527 78.6378 76.1384 80.4598 90.5263 72.4062 82.6833 63.5294 81.6901 66.6667 77.9923 65.9619 63.3833 64.6217 78.4962 51.1211 76.8642 55.4707 79.1075 77.3723 84.2444 89.6113 70.9984 26.6212 77.9715 76.4602 36.0656 79.0643 54.5906 67.101 74.4368 70.9206 69.6589 87.3995 66.6667 19.7802 58.127 72.3005 72.3539 80.3461 56.5584 53.5419 54.8341 86.2136 75.1381 72.8757 58.4838 57.9805 79.3997 62.3098 65.4142 67.0554 45.6621 72.8229 82.5636 68.623 50.8039 69.7819 62.5387 62.7803 79.3893 44.8669 78.1845 78.8651 90.1288 78.7138 64.7343 80.9802 81.2352 74.9682 80.6283 55.6622 70.4289 48.3926 69.4981 48.6739 74.3466 63.6119 71.7949 41.8972 68.4134 79.1878 71.27 73.1978 79.4733 59.1362 57.2438 58.9342 47.5817 79.0637 71.9812 52.6733 70.5882 87.218 72.9927 80 80.9731 66.7782 75.2137 78.3893 77.0083 69.3446 69.3208 76.2737 77.9436 63.4304 73.2475 67.7389 80.5508 71.0472 77.9392 71.8573 74.4731 54.7842 65.9436 82.779 78.8368 74.1507 66.4653 80.7396 67.0906 76.6224 50.8857 55.2189 66.7579 72.0867 81.1715 64.3777 59.1479 60.1134 72.0293 60.6803 70.3743 83.8821 81.0304 70.8438 89.7896 74.2176 86.262 49.7959 67.4253 78.6026 58.6873 41.6244 72.9316 66.9789 73.8462 63.9069 42.0601 51.0067 62.6594 60.2532 60.6403 51.8732 69.4981 65.6716 58.5269 89.8345 51.7483 79.7546 47.4695 81.7147 51.5663 67.0659 39.6761 53.2637 62.5917 59.4059 93.1717 36.5217 83.6689 64.0264 61.2056 63.0473 25.1256 82.6722 49.3274 60.9442 80.7388 68.1992 81.6936 78.1986 83.5473 41.4508 76.2923 65.996 58.4845 77.3939 57.8723 66.4323 67.5119 70.3196 53.2819 66.6667 52.2088 44.1256 57.6271 70.2259 87.3533 82.6816 61.6541 55.3846 81.5595 58.6319 44.6953 47.3118 71.5198 71.9794 81.6391 55.5911 86.2016 75.4843 75.2827 81.3299 62.4809 72.2947 70.9189 82.861 56.2319 68.7419 60.3466 88.6624 84.0295 77.8325 58.517 32.6241 56.0319 69.0293 62.5931 69.5842 84.2105 40.678 80.1702 72.6137 83.871 71.017 48.7329 67.3031 63.8436 83.6795 61.5012 47.1483 84.1076 72.2581 66.6667 62.8429 71.3533 61.8911 69.6517 56.2249 32.5581 66.2132 71.1111 63.6977 92.9881 93.0195 80.5778 58.2278 63.5294 52.7473 68.8995 64.843 58.5834 82.2094 69.5652 63.4873 71.7873 81.2371 27.0677 66.1871 80.7399 85.5292 67.3575 62.0926 74.1433 75.2834 50.2035 59.1065 77.8312 79.1712 91.6074 64.5793 78.7447 85.7963 72 70.9163 81.0005 85.987 67.2504 77.6524 71.3978 86.5837 80.4294 76.2511 88.1336 53.7259 68.7915 74.4122 84.6563 59.9589 73.5524 73.1915 65.1885 70.5376 55.8525 50.4039 90.7504 38.7665 91.6155 80.8976 66.242 70.7182 67.3697 86.7839 74.8515 64.0657 55.7377 74.1313 34.2541 60.8345 84.153 70.255 57.3319 46.5116 64.9275 50.5338 81.1594 42.5273 50.8411 71.034 71.1864 70.188 82.9647 72.4528 71.2042 59.5745 84.3077 62.8993 61.7143 0 74.6524 82.0779 59.6112 52.7307 65.2141 49.2308 41.1215 75.8315 88.6154 57.6029 83.0165 73.6134 52.0593 78.9809 62.6772 46.3866 57.2347 76.1011 56.0162 55.7576 54.5961 53.5065 82.7774 63.8655 65.7005 65.0367 36.5714 74.0576 17.3228 68.8478 68.7361 75.2399 65.4867 81.7021 74.8299 86.1048 41.2903 79.4521 82.4447 69.599 64.4068 66.9604 69.4698 47.2169 82.3028 76.4228 79.9205 53.7102 81.5373 71.9794 59.5944 73.0159 75.4325 ]

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:347) Done 2622 files, 1 with no tgt_mats, 0 with other errors. [TRAINING, 0.344136 min, fps36568]
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:353) AvgLoss: 0.607819 (Xent), [AvgXent: 0.607819, AvgTargetEnt: 0]
progress: []
FRAME_ACCURACY >> 80.7936% <<

