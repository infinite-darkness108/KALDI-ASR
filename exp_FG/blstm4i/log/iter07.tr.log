speech-HP-Z2-Tower-G9
nnet-train-multistream-perutt --cross-validate=false --randomize=true --verbose=0 --num-streams=10 --max-frames=15000 --learn-rate=0.00004 --momentum=0.9 --l1-penalty=0 --l2-penalty=0 --feature-transform=exp_FG/blstm4i/final.feature_transform 'ark:copy-feats scp:exp_FG/blstm4i/train.scp ark:- | apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_tr90/utt2spk scp:data-fbank/train_tr90/cmvn.scp ark:- ark:- | add-deltas --delta-order=2 ark:- ark:- |' 'ark:ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- |' exp_FG/blstm4i/nnet/nnet_iter06_learnrate0.00004_tr0.8452_cv2.0237 exp_FG/blstm4i/nnet/nnet_iter07 
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:243) Not in compute-exclusive mode.  Suggestion: use 'nvidia-smi -c 3' to set compute exclusive mode
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:438) Selecting from 1 GPUs
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:453) cudaSetDevice(0): NVIDIA RTX A2000 12GB	free:11620M, used:410M, total:12031M, free/total:0.96589
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:501) Device: 0, mem_ratio: 0.96589
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:382) Trying to select device: 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:511) Success selecting device 0 free mem ratio: 0.96589
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:FinalizeActiveGpu():cu-device.cc:338) The active GPU is [0]: NVIDIA RTX A2000 12GB	free:11106M, used:924M, total:12031M, free/total:0.923168 version 8.6
copy-feats scp:exp_FG/blstm4i/train.scp ark:- 
apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_tr90/utt2spk scp:data-fbank/train_tr90/cmvn.scp ark:- ark:- 
add-deltas --delta-order=2 ark:- ark:- 
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:144) TRAINING STARTED
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Read():nnet/nnet-matrix-buffer.h:191) Read() started... Buffer size in MB: 0, max 3072, having 0 utterances.
ali-to-post ark:- ark:- 
ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl 'ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |' ark:- 
LOG (copy-feats[5.5.1074~1-71f3]:main():copy-feats.cc:143) Copied 2624 feature matrices.
LOG (apply-cmvn[5.5.1074~1-71f3]:main():apply-cmvn.cc:159) Applied cepstral mean and variance normalization to 2624 utterances, errors on 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:303) ### After 0 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:304) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.6171, max 0.659678, mean 0.00393339, stddev 0.0854315, skewness 0.0130791, kurtosis 0.689589 ) 
  f_w_gifo_r_   ( min -0.423808, max 0.409622, mean -0.000584781, stddev 0.0781965, skewness -0.00163646, kurtosis 0.0165963 ) 
  f_bias_   ( min -0.347321, max 1.37597, mean 0.212593, stddev 0.462291, skewness 1.06385, kurtosis -0.652082 ) 
  f_peephole_i_c_   ( min -0.508982, max 0.501393, mean -0.00290128, stddev 0.129134, skewness 0.0852708, kurtosis 1.54232 ) 
  f_peephole_f_c_   ( min -0.677315, max 0.896185, mean 0.00406839, stddev 0.174893, skewness 0.245727, kurtosis 4.60294 ) 
  f_peephole_o_c_   ( min -0.506909, max 0.47153, mean -0.0109051, stddev 0.188157, skewness 0.195957, kurtosis -0.248091 ) 
  f_w_r_m_   ( min -0.515848, max 0.49277, mean 0.000606301, stddev 0.103023, skewness 0.00135127, kurtosis 0.0109339 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -1.52951, max 1.00319, mean 0.00620816, stddev 0.0921298, skewness -0.148053, kurtosis 3.98113 ) 
  b_w_gifo_r_   ( min -0.365698, max 0.34341, mean -0.000185749, stddev 0.0712488, skewness 0.000154862, kurtosis -0.273771 ) 
  b_bias_   ( min -0.362448, max 1.21022, mean 0.206188, stddev 0.45288, skewness 1.04248, kurtosis -0.689325 ) 
  b_peephole_i_c_   ( min -0.379041, max 0.296354, mean 0.00611141, stddev 0.0979208, skewness -0.0595562, kurtosis 1.0251 ) 
  b_peephole_f_c_   ( min -0.632409, max 0.725262, mean 0.0133015, stddev 0.171914, skewness 0.624714, kurtosis 3.69144 ) 
  b_peephole_o_c_   ( min -0.561734, max 0.501894, mean -0.01613, stddev 0.193637, skewness -0.163593, kurtosis 0.299808 ) 
  b_w_r_m_   ( min -0.39749, max 0.378303, mean -0.000270939, stddev 0.0920627, skewness 0.000582951, kurtosis -0.0551958 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.953385, max 0.755871, mean -0.000155917, stddev 0.107488, skewness 0.0058496, kurtosis 0.0632606 ) , lr-coef 1, max-norm 0
  bias ( min -0.0882759, max 2.45332, mean -2.32831e-09, stddev 0.0803106, skewness 23.33, kurtosis 685.641 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:305) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -8.67671, max 9.02116, mean 0.0321104, stddev 1.01793, skewness 0.450612, kurtosis 3.20408 ) 
[1] output of <BlstmProjected> ( min -4.85567, max 4.54219, mean 0.000397321, stddev 0.778075, skewness -0.00407166, kurtosis 1.27415 ) 
[2] output of <Tanh> ( min -0.999879, max 0.999773, mean 0.000506139, stddev 0.513468, skewness -0.00254007, kurtosis -0.772313 ) 
[3] output of <AffineTransform> ( min -16.2901, max 22.0175, mean 0.006295, stddev 2.41728, skewness 0.83674, kurtosis 3.18668 ) 
[4] output of <Softmax> ( min 1.13167e-15, max 0.999973, mean 0.000780069, stddev 0.0209531, skewness 37.3593, kurtosis 1512.87 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:307) 
### BACKWARD PROPAGATION BUFFER CONTENT :
[0] diff of <Input>  ( min -4.30632, max 5.16242, mean -0.00556275, stddev 0.317554, skewness 0.0551026, kurtosis 25.214 ) 
[1] diff-output of <BlstmProjected> ( min -0.725433, max 0.738701, mean -0.000289243, stddev 0.0495977, skewness -0.0242113, kurtosis 7.55066 ) 
[2] diff-output of <Tanh> ( min -0.964123, max 0.816953, mean -0.000325833, stddev 0.0644607, skewness -0.00181793, kurtosis 4.69586 ) 
[3] diff-output of <AffineTransform> ( min -0.999999, max 0.969917, mean -2.56101e-07, stddev 0.0179856, skewness -22.1314, kurtosis 1756.11 ) 
[4] diff-output of <Softmax> ( min -0.999999, max 0.969917, mean -2.56101e-07, stddev 0.0179856, skewness -22.1314, kurtosis 1756.11 ) 
### END BACKWARD


LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:308) 
### GRADIENT STATS :
Component 1 : <BlstmProjected>, ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  ### Gradients 
  f_w_gifo_x_corr_   ( min -64.7298, max 50.8791, mean 0.0876789, stddev 3.65714, skewness -0.909779, kurtosis 26.061 ) 
  f_w_gifo_r_corr_   ( min -70.2554, max 63.2312, mean -0.000277572, stddev 2.56001, skewness -0.0228259, kurtosis 17.9959 ) 
  f_bias_corr_   ( min -49.1513, max 44.2048, mean 0.0330955, stddev 5.12351, skewness 0.8169, kurtosis 21.7026 ) 
  f_peephole_i_c_corr_   ( min -78.7521, max 116.663, mean 0.139885, stddev 10.503, skewness 4.51555, kurtosis 68.672 ) 
  f_peephole_f_c_corr_   ( min -88.7337, max 250, mean 0.909875, stddev 20.2876, skewness 6.09337, kurtosis 72.7251 ) 
  f_peephole_o_c_corr_   ( min -90.0505, max 250, mean -0.783716, stddev 20.2622, skewness 5.14713, kurtosis 74.1873 ) 
  f_w_r_m_corr_   ( min -54.155, max 41.7077, mean -0.00523562, stddev 4.06854, skewness 0.00887775, kurtosis 6.31195 ) 
  ---
  b_w_gifo_x_corr_   ( min -186.208, max 230.985, mean -0.00367276, stddev 8.25188, skewness 3.53038, kurtosis 212.021 ) 
  b_w_gifo_r_corr_   ( min -108.596, max 71.3856, mean -0.00386346, stddev 3.235, skewness -0.129714, kurtosis 33.6808 ) 
  b_bias_corr_   ( min -212.53, max 170.974, mean 0.332165, stddev 14.7853, skewness -1.44874, kurtosis 66.3856 ) 
  b_peephole_i_c_corr_   ( min -45.0816, max 79.2842, mean 0.556918, stddev 7.87548, skewness 2.58523, kurtosis 37.1622 ) 
  b_peephole_f_c_corr_   ( min -77.2995, max 180.4, mean 1.17758, stddev 15.9924, skewness 4.09502, kurtosis 50.4324 ) 
  b_peephole_o_c_corr_   ( min -80.0044, max 153.199, mean -0.322188, stddev 19.6931, skewness 1.15582, kurtosis 15.081 ) 
  b_w_r_m_corr_   ( min -51.3104, max 59.8115, mean -0.0141418, stddev 5.05536, skewness 0.0417166, kurtosis 3.3028 ) 

  ### Activations (mostly after non-linearities)
  YI_FW(0..1)^   ( min 0, max 1, mean 0.427506, stddev 0.348962, skewness 0.369876, kurtosis -1.31765 ) 
  YF_FW(0..1)^   ( min 0, max 1, mean 0.63771, stddev 0.322404, skewness -0.516828, kurtosis -1.04004 ) 
  YO_FW(0..1)^   ( min 0, max 1, mean 0.365846, stddev 0.355133, skewness 0.636158, kurtosis -1.14115 ) 
  YG_FW(-1..1)   ( min -1, max 1, mean 0.0232232, stddev 0.871006, skewness -0.0461005, kurtosis -1.80648 ) 
  YC_FW(-R..R)*  ( min -50, max 50, mean 0.283587, stddev 13.369, skewness 0.074485, kurtosis 9.21674 ) 
  YH_FW(-1..1)   ( min -1, max 1, mean 0.0362714, stddev 0.684329, skewness -0.0626509, kurtosis -1.23251 ) 
  YM_FW(-1..1)   ( min -1, max 1, mean 0.00399224, stddev 0.330156, skewness -0.0582362, kurtosis 2.8331 ) 
  YR_FW(-R..R)   ( min -4.43153, max 4.54219, mean 0.0158322, stddev 0.757954, skewness 0.0238835, kurtosis 1.17931 ) 
  ---
  YI_BW(0..1)^   ( min 0, max 1, mean 0.463145, stddev 0.33655, skewness 0.22486, kurtosis -1.37393 ) 
  YF_BW(0..1)^   ( min 0, max 1, mean 0.647019, stddev 0.287659, skewness -0.571193, kurtosis -0.707218 ) 
  YO_BW(0..1)^   ( min 0, max 1, mean 0.389675, stddev 0.357473, skewness 0.525675, kurtosis -1.27081 ) 
  YG_BW(-1..1)   ( min -1, max 1, mean 0.0102923, stddev 0.856528, skewness -0.0186873, kurtosis -1.78356 ) 
  YC_BW(-R..R)*  ( min -50, max 50, mean 1.10164, stddev 11.356, skewness 1.07887, kurtosis 12.0304 ) 
  YH_BW(-1..1)   ( min -1, max 1, mean 0.0405411, stddev 0.69665, skewness -0.0533075, kurtosis -1.31225 ) 
  YM_BW(-1..1)   ( min -0.999988, max 1, mean 0.00410607, stddev 0.341242, skewness 0.0188665, kurtosis 2.16697 ) 
  YR_BW(-R..R)   ( min -4.85567, max 4.26258, mean -0.0150418, stddev 0.793738, skewness -0.0232686, kurtosis 1.37213 ) 

  ### Derivatives (w.r.t. inputs of non-linearities)
  DI_FW^  ( min -1, max 1, mean -3.92789e-07, stddev 0.0241537, skewness -0.620903, kurtosis 350.461 ) 
  DF_FW^  ( min -1, max 1, mean 1.20803e-05, stddev 0.0204643, skewness -0.2231, kurtosis 541.3 ) 
  DO_FW^  ( min -0.856473, max 1, mean 3.24144e-05, stddev 0.0265958, skewness 0.262087, kurtosis 75.7552 ) 
  DG_FW   ( min -1, max 1, mean -1.03299e-05, stddev 0.0320192, skewness 1.02901, kurtosis 385.358 ) 
  DC_FW*  ( min -26.5063, max 104.842, mean 0.00473103, stddev 0.75127, skewness 65.3545, kurtosis 7576.36 ) 
  DH_FW   ( min -3.53813, max 3.72831, mean -0.000237047, stddev 0.123922, skewness 0.232831, kurtosis 59.6416 ) 
  DM_FW   ( min -5.39871, max 6.00094, mean -8.51985e-05, stddev 0.318313, skewness 0.0335594, kurtosis 13.7746 ) 
  DR_FW   ( min -1.14767, max 1.33018, mean -0.000386115, stddev 0.0883373, skewness 0.000548665, kurtosis 7.72377 ) 
  ---
  DI_BW^  ( min -1, max 1, mean -7.97719e-05, stddev 0.0276615, skewness 0.236411, kurtosis 275.86 ) 
  DF_BW^  ( min -1, max 1, mean -1.96414e-05, stddev 0.018897, skewness -0.1779, kurtosis 282.455 ) 
  DO_BW^  ( min -0.785256, max 0.76629, mean 0.000107226, stddev 0.0234142, skewness 0.812343, kurtosis 52.6417 ) 
  DG_BW   ( min -1, max 1, mean 0.000331129, stddev 0.0454921, skewness -0.583077, kurtosis 189.966 ) 
  DC_BW*  ( min -22.7269, max 19.5602, mean -0.00342603, stddev 0.412174, skewness -14.6902, kurtosis 780.014 ) 
  DH_BW   ( min -2.84119, max 3.84164, mean 0.000682478, stddev 0.111908, skewness -0.203194, kurtosis 45.9411 ) 
  DM_BW   ( min -4.2168, max 4.73754, mean 0.00141029, stddev 0.296313, skewness -0.031473, kurtosis 9.61204 ) 
  DR_BW   ( min -1.09987, max 1.10883, mean -0.000810713, stddev 0.101232, skewness -0.08302, kurtosis 6.89863 ) 
Component 2 : <Tanh>, 
Component 3 : <AffineTransform>, 
  linearity_grad ( min -122.347, max 102.349, mean -2.51656e-08, stddev 1.67709, skewness -1.29255, kurtosis 560.384 ) , lr-coef 1, max-norm 0
  bias_grad ( min -272.029, max 172.341, mean 1.07288e-07, stddev 9.53734, skewness -13.0165, kurtosis 602.031 ) , lr-coef 1
Component 4 : <Softmax>, 
### END GRADIENT

LOG (ali-to-pdf[5.5.1074~1-71f3]:main():ali-to-pdf.cc:68) Converted 2919 alignments to pdf sequences.
LOG (ali-to-post[5.5.1074~1-71f3]:main():ali-to-post.cc:73) Converted 2919 alignments.
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:168) MC05_98, missing targets
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:334) ### After 755062 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:335) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.686081, max 0.746735, mean 0.00389883, stddev 0.088032, skewness 0.0112436, kurtosis 0.927634 ) 
  f_w_gifo_r_   ( min -0.424222, max 0.413743, mean -0.000587032, stddev 0.0787126, skewness -0.00333338, kurtosis 0.0235455 ) 
  f_bias_   ( min -0.351889, max 1.38379, mean 0.211202, stddev 0.464323, skewness 1.06073, kurtosis -0.650783 ) 
  f_peephole_i_c_   ( min -0.621049, max 0.504573, mean -0.00362917, stddev 0.132112, skewness -0.062059, kurtosis 2.11466 ) 
  f_peephole_f_c_   ( min -0.706738, max 0.759955, mean 0.00455142, stddev 0.173853, skewness 0.18798, kurtosis 3.47898 ) 
  f_peephole_o_c_   ( min -0.675701, max 0.496666, mean -0.0102628, stddev 0.19194, skewness 0.0631598, kurtosis -0.0257423 ) 
  f_w_r_m_   ( min -0.510858, max 0.502346, mean 0.000601689, stddev 0.104422, skewness 0.00182213, kurtosis 0.0251966 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -1.65715, max 1.10765, mean 0.00609001, stddev 0.0952547, skewness -0.171456, kurtosis 4.91685 ) 
  b_w_gifo_r_   ( min -0.348904, max 0.367284, mean -0.000200308, stddev 0.0722196, skewness 0.000304645, kurtosis -0.236067 ) 
  b_bias_   ( min -0.389512, max 1.22297, mean 0.205265, stddev 0.454035, skewness 1.0348, kurtosis -0.69304 ) 
  b_peephole_i_c_   ( min -0.423105, max 0.313897, mean 0.00699103, stddev 0.101187, skewness -0.136644, kurtosis 1.34592 ) 
  b_peephole_f_c_   ( min -0.563952, max 0.746451, mean 0.0167046, stddev 0.177378, skewness 0.578724, kurtosis 3.06632 ) 
  b_peephole_o_c_   ( min -0.57515, max 0.538246, mean -0.0158073, stddev 0.197649, skewness -0.164656, kurtosis 0.350606 ) 
  b_w_r_m_   ( min -0.409727, max 0.396105, mean -0.000354284, stddev 0.0940523, skewness -0.00200197, kurtosis -0.0343945 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.96999, max 0.756577, mean -0.000155915, stddev 0.108394, skewness 0.00561884, kurtosis 0.0595853 ) , lr-coef 1, max-norm 0
  bias ( min -0.0883743, max 2.50188, mean -1.81608e-09, stddev 0.0827467, skewness 22.8174, kurtosis 659.944 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:336) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -14.8312, max 13.8231, mean 0.00769689, stddev 0.940803, skewness 0.731578, kurtosis 12.6034 ) 
[1] output of <BlstmProjected> ( min -4.0133, max 3.93872, mean 0.00187279, stddev 0.654647, skewness -0.0358592, kurtosis 3.19155 ) 
[2] output of <Tanh> ( min -0.999347, max 0.999242, mean 0.00184133, stddev 0.430972, skewness -0.00252706, kurtosis 0.151953 ) 
[3] output of <AffineTransform> ( min -12.4819, max 22.1519, mean 0.00915442, stddev 2.0575, skewness 1.03605, kurtosis 5.88332 ) 
[4] output of <Softmax> ( min 4.07688e-13, max 0.999948, mean 0.000780992, stddev 0.0182499, skewness 42.3258, kurtosis 1942.52 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:338) 
### BACKWARD PROPAGATION BUFFER CONTENT :
[0] diff of <Input>  ( min -5.20523, max 2.59624, mean 0.00322857, stddev 0.264841, skewness -1.57276, kurtosis 33.0297 ) 
[1] diff-output of <BlstmProjected> ( min -0.590859, max 0.699929, mean 2.067e-05, stddev 0.0451508, skewness -0.0241783, kurtosis 11.2751 ) 
[2] diff-output of <Tanh> ( min -0.629816, max 0.710715, mean 2.98843e-05, stddev 0.0569287, skewness -0.0190292, kurtosis 7.47824 ) 
[3] diff-output of <AffineTransform> ( min -0.999585, max 0.944607, mean -1.54906e-08, stddev 0.0158852, skewness -22.1964, kurtosis 2230.91 ) 
[4] diff-output of <Softmax> ( min -0.999585, max 0.944607, mean -1.54906e-08, stddev 0.0158852, skewness -22.1964, kurtosis 2230.91 ) 
### END BACKWARD


LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:339) 
### GRADIENT STATS :
Component 1 : <BlstmProjected>, ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  ### Gradients 
  f_w_gifo_x_corr_   ( min -33.2039, max 36.1544, mean 0.0100037, stddev 3.37086, skewness 0.234853, kurtosis 6.82759 ) 
  f_w_gifo_r_corr_   ( min -34.1961, max 36.6728, mean -0.000985416, stddev 2.89354, skewness 0.0178833, kurtosis 5.69498 ) 
  f_bias_corr_   ( min -22.6736, max 23.3411, mean -0.221241, stddev 3.97825, skewness 0.30988, kurtosis 3.62283 ) 
  f_peephole_i_c_corr_   ( min -66.5376, max 59.8257, mean 0.39576, stddev 9.33564, skewness -0.0694339, kurtosis 22.1217 ) 
  f_peephole_f_c_corr_   ( min -124.56, max 137.891, mean 0.0147794, stddev 19.3646, skewness -0.262983, kurtosis 15.9839 ) 
  f_peephole_o_c_corr_   ( min -108.199, max 82.9216, mean -0.789772, stddev 17.0842, skewness -0.678802, kurtosis 12.6434 ) 
  f_w_r_m_corr_   ( min -33.5638, max 36.3405, mean -0.017851, stddev 4.25964, skewness 0.0603636, kurtosis 3.15182 ) 
  ---
  b_w_gifo_x_corr_   ( min -94.3224, max 80.7409, mean 0.177946, stddev 4.12324, skewness -0.0505933, kurtosis 22.722 ) 
  b_w_gifo_r_corr_   ( min -36.8513, max 37.7978, mean 0.00202994, stddev 3.15255, skewness 0.0111028, kurtosis 5.46461 ) 
  b_bias_corr_   ( min -27.1955, max 48.6949, mean -0.237243, stddev 5.49127, skewness 0.956029, kurtosis 10.6215 ) 
  b_peephole_i_c_corr_   ( min -34.2401, max 48.1116, mean 0.0331369, stddev 6.14126, skewness 1.81004, kurtosis 21.5191 ) 
  b_peephole_f_c_corr_   ( min -116.761, max 98.4232, mean 0.282667, stddev 12.8309, skewness -1.21896, kurtosis 31.3289 ) 
  b_peephole_o_c_corr_   ( min -49.0214, max 58.9836, mean 0.742387, stddev 12.599, skewness 0.128447, kurtosis 3.01648 ) 
  b_w_r_m_corr_   ( min -45.8582, max 48.099, mean 0.013971, stddev 4.55305, skewness 0.00770789, kurtosis 1.51257 ) 

  ### Activations (mostly after non-linearities)
  YI_FW(0..1)^   ( min 0, max 1, mean 0.315259, stddev 0.351919, skewness 0.777337, kurtosis -0.89457 ) 
  YF_FW(0..1)^   ( min 0, max 1, mean 0.470555, stddev 0.389633, skewness 0.0244286, kurtosis -1.59401 ) 
  YO_FW(0..1)^   ( min 0, max 1, mean 0.266144, stddev 0.340302, skewness 1.08319, kurtosis -0.347001 ) 
  YG_FW(-1..1)   ( min -1, max 1, mean 0.0188149, stddev 0.750165, skewness -0.034772, kurtosis -1.39287 ) 
  YC_FW(-R..R)*  ( min -50, max 50, mean 0.0892309, stddev 10.0486, skewness 0.0407765, kurtosis 17.6002 ) 
  YH_FW(-1..1)   ( min -1, max 1, mean 0.0237728, stddev 0.584724, skewness -0.0408723, kurtosis -0.599069 ) 
  YM_FW(-1..1)   ( min -1, max 1, mean 0.00477287, stddev 0.27428, skewness -0.0721744, kurtosis 5.30813 ) 
  YR_FW(-R..R)   ( min -3.92046, max 3.93872, mean 0.0096038, stddev 0.630689, skewness 0.0647368, kurtosis 3.08116 ) 
  ---
  YI_BW(0..1)^   ( min 0, max 1, mean 0.339589, stddev 0.355426, skewness 0.64605, kurtosis -1.09449 ) 
  YF_BW(0..1)^   ( min 0, max 1, mean 0.475541, stddev 0.375123, skewness -0.0454493, kurtosis -1.51543 ) 
  YO_BW(0..1)^   ( min 0, max 1, mean 0.281181, stddev 0.352013, skewness 0.992597, kurtosis -0.58331 ) 
  YG_BW(-1..1)   ( min -1, max 1, mean 0.0112013, stddev 0.749572, skewness -0.017736, kurtosis -1.39695 ) 
  YC_BW(-R..R)*  ( min -50, max 50, mean 0.839432, stddev 9.43152, skewness 1.62544, kurtosis 17.6445 ) 
  YH_BW(-1..1)   ( min -1, max 1, mean 0.0302976, stddev 0.595254, skewness -0.014585, kurtosis -0.687094 ) 
  YM_BW(-1..1)   ( min -0.999999, max 0.999998, mean 0.00146145, stddev 0.290061, skewness -0.04934, kurtosis 4.2792 ) 
  YR_BW(-R..R)   ( min -4.0133, max 3.71015, mean -0.00588059, stddev 0.67388, skewness -0.114061, kurtosis 3.2886 ) 

  ### Derivatives (w.r.t. inputs of non-linearities)
  DI_FW^  ( min -1, max 0.987155, mean -0.000124851, stddev 0.0188885, skewness -6.67254, kurtosis 482.887 ) 
  DF_FW^  ( min -1, max 1, mean -0.000133462, stddev 0.0161676, skewness -3.32222, kurtosis 888.983 ) 
  DO_FW^  ( min -0.552894, max 0.561913, mean -1.0501e-05, stddev 0.0212409, skewness 0.0985536, kurtosis 78.4572 ) 
  DG_FW   ( min -1, max 1, mean -0.000170661, stddev 0.0240668, skewness -1.55452, kurtosis 570.093 ) 
  DC_FW*  ( min -13.2211, max 17.0558, mean -0.0022326, stddev 0.294735, skewness 0.245213, kurtosis 842.444 ) 
  DH_FW   ( min -2.53099, max 3.01677, mean -0.000264933, stddev 0.102125, skewness -0.0905377, kurtosis 74.6738 ) 
  DM_FW   ( min -3.10515, max 3.22334, mean 0.000713257, stddev 0.248934, skewness 0.0468373, kurtosis 14.584 ) 
  DR_FW   ( min -0.836491, max 1.06478, mean -7.00317e-05, stddev 0.070899, skewness 0.0172141, kurtosis 11.0239 ) 
  ---
  DI_BW^  ( min -1, max 1, mean 0.000123974, stddev 0.0271119, skewness 2.406, kurtosis 491.207 ) 
  DF_BW^  ( min -1, max 1, mean 0.000119969, stddev 0.0178639, skewness 9.68004, kurtosis 851.704 ) 
  DO_BW^  ( min -0.6592, max 0.531583, mean 0.000122388, stddev 0.0214331, skewness 0.398519, kurtosis 79.6472 ) 
  DG_BW   ( min -1, max 1, mean 4.11832e-05, stddev 0.0357797, skewness 0.995376, kurtosis 287.071 ) 
  DC_BW*  ( min -13.1116, max 29.3111, mean 0.00612895, stddev 0.566479, skewness 26.2929, kurtosis 1182.1 ) 
  DH_BW   ( min -2.72114, max 2.57564, mean 0.000145671, stddev 0.108628, skewness -0.209097, kurtosis 67.7964 ) 
  DM_BW   ( min -3.85393, max 3.61944, mean 0.00156785, stddev 0.288323, skewness 0.0485608, kurtosis 11.4859 ) 
  DR_BW   ( min -1.02779, max 1.03284, mean 0.000298893, stddev 0.093774, skewness 0.0307028, kurtosis 8.56051 ) 
Component 2 : <Tanh>, 
Component 3 : <AffineTransform>, 
  linearity_grad ( min -62.2077, max 52.0292, mean -6.27458e-08, stddev 1.6057, skewness -0.227374, kurtosis 83.808 ) , lr-coef 1, max-norm 0
  bias_grad ( min -94.0212, max 57.6596, mean 4.02331e-08, stddev 4.6485, skewness -9.19042, kurtosis 229.186 ) , lr-coef 1
Component 4 : <Softmax>, 
### END GRADIENT

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:346) PER-CLASS PERFORMANCE:
@@@ Frames per-class: [ 181212 144 127 606 532 43 173 190 515 703 138 211 211 608 45 289 271 572 372 153 368 892 307 454 467 612 374 177 314 165 203 169 193 120 162 350 506 100 142 40889 89709 154 463 1777 286 231 687 352 545 459 79 394 481 237 200 663 1037 475 2210 428 257 167 4192 1910 93 230 151 223 4363 199 339 612 931 689 168 53 430 425 404 147 72 468 398 184 543 242 103 448 1376 60 779 717 668 146 541 163 358 830 561 408 249 268 103 1105 608 477 208 629 257 136 769 185 359 2082 165 69 303 74 247 336 172 147 117 128 443 612 117 125 52 316 956 165 305 574 202 297 222 1167 208 209 172 556 182 606 267 121 534 323 470 239 776 321 812 179 268 1139 537 191 554 5133 217 193 202 128 264 242 282 141 453 190 390 248 274 208 168 117 250 72 227 832 107 208 74 120 577 200 74 173 93 435 313 308 638 462 201 239 312 172 59 151 330 221 210 68 77 424 248 789 348 126 86 566 208 142 331 485 286 372 234 328 637 273 293 132 41 150 57 102 173 655 328 426 861 178 245 87 533 72 131 265 150 790 182 357 233 69 187 112 206 273 85 62 302 205 411 179 278 260 193 525 723 141 288 139 513 788 215 97 150 236 273 455 166 615 208 63 196 43 120 342 911 206 87 1156 192 146 131 166 31 221 252 417 373 499 401 233 179 409 182 439 579 96 189 251 1812 272 55 134 245 227 120 170 454 69 211 126 232 303 701 89 59 87 90 418 122 334 282 91 119 125 379 59 189 157 163 620 120 203 224 199 71 381 86 544 80 247 220 94 114 76 256 175 1186 420 378 336 135 140 331 666 306 356 196 203 291 152 221 543 214 149 360 365 670 264 264 692 216 616 923 524 408 234 395 301 375 105 525 80 359 98 456 204 309 258 288 286 181 90 101 263 53 253 167 354 150 174 194 393 579 494 437 273 142 236 159 226 115 606 198 255 508 136 282 304 327 234 1467 199 113 213 747 267 237 238 267 223 423 127 334 351 773 400 668 269 762 507 73 172 259 944 420 60 318 817 125 150 155 430 99 155 360 316 425 603 146 124 301 210 170 102 140 531 288 716 282 161 233 181 349 120 133 242 135 51 141 111 147 205 551 427 131 99 109 133 445 7274 204 271 135 128 125 377 120 372 449 808 108 557 45 280 47 596 57 276 400 121 347 315 193 77 146 96 105 216 193 341 1125 236 219 105 417 109 338 639 130 396 177 211 138 121 275 91 104 194 51 542 461 346 197 315 121 109 444 210 64 135 84 52 96 283 268 315 144 187 498 569 470 194 79 151 71 285 409 404 165 215 321 466 69 209 585 117 83 141 150 364 478 225 439 81 192 156 773 344 306 243 112 146 203 501 258 143 117 231 316 156 83 53 468 163 539 465 176 116 79 157 350 139 86 253 155 101 132 342 440 324 189 390 153 111 192 365 208 291 263 103 139 134 307 402 452 451 5943 88 406 111 189 488 786 548 86 632 99 382 216 141 365 388 65 90 187 430 300 463 604 247 79 314 305 463 45 473 718 330 223 226 1312 201 160 62 170 227 141 90 90 193 68 539 610 241 156 182 257 224 114 99 209 178 32 193 174 235 238 150 123 308 261 195 275 142 111 371 821 518 831 306 124 432 87 357 932 174 327 225 300 36 190 272 120 358 461 397 462 306 105 348 166 390 216 520 219 260 71 179 258 429 201 53 142 256 406 60 128 173 26 88 191 83 60 155 179 1000 125 44 191 230 383 687 216 257 321 233 417 202 1061 342 327 163 248 165 412 229 303 517 124 815 539 79 306 155 255 89 321 245 73 308 228 196 192 269 176 228 101 8591 469 337 966 237 430 177 258 334 85 304 414 487 401 57 178 99 260 178 151 248 261 926 126 248 230 264 241 147 220 155 139 646 219 209 393 1097 85 160 936 605 267 175 213 208 155 166 207 194 260 180 542 294 745 650 41 235 409 387 268 281 172 798 467 256 138 171 316 378 542 197 77 199 253 693 506 326 151 543 272 142 171 242 241 191 161 274 304 47 226 320 127 248 43 388 236 233 244 332 111 261 196 246 68 466 707 315 146 315 282 30 427 201 153 510 1026 278 186 316 45 464 106 316 404 415 543 346 257 452 1347 138 153 366 558 585 171 109 855 530 221 155 160 161 111 196 131 341 863 116 777 103 428 210 393 95 260 221 295 129 320 516 185 97 126 343 492 405 270 645 150 141 159 382 1153 426 252 535 332 205 417 575 597 292 372 180 236 213 343 301 154 349 528 290 243 378 616 213 266 230 676 309 338 165 324 314 639 310 148 365 184 119 116 199 264 1367 367 467 1442 213 598 641 335 156 122 351 114 129 98 489 213 162 386 116 74 274 197 265 173 129 167 753 211 214 1711 286 746 207 83 123 191 204 353 783 57 223 151 323 285 99 1197 111 116 189 130 401 488 428 96 280 248 468 506 117 284 315 328 129 334 124 557 265 243 383 268 66 97 474 153 221 139 391 194 634 156 322 748 309 195 326 651 462 370 172 385 490 392 610 304 249 70 501 324 335 228 370 206 587 560 108 850 256 209 153 168 206 131 204 232 340 200 284 174 301 124 107 220 67 370 1390 1253 311 434 127 136 104 493 416 778 103 335 338 242 66 208 459 231 96 291 160 220 368 145 728 458 351 255 350 348 137 125 1039 1466 285 221 232 361 605 549 4192 543 376 361 3105 243 319 117 225 232 542 309 859 113 244 913 235 271 853 881 252 243 274 129 90 371 274 176 453 107 172 140 172 411 267 778 147 558 2509 132 95 258 162 203 87 21 467 192 231 265 665 552 160 225 162 279 3235 297 303 235 317 297 155 692 494 82 179 192 734 178 103 204 87 225 63 351 225 260 282 352 367 219 77 182 384 386 147 340 273 260 234 61 251 141 663 194 320 94 144 ]
@@@ Loss per-class: [ 0.262152 0.723369 0.98187 0.642077 0.7275 1.09773 1.14579 0.724534 0.95493 1.29023 0.595095 1.35138 1.24994 0.858842 1.57841 0.81936 0.922996 1.18818 1.1617 0.830155 0.712453 0.996894 1.0173 0.653614 0.439109 0.91505 0.579715 1.9857 0.821411 1.45571 0.970074 0.84074 0.993712 1.27413 0.496637 0.81125 1.45163 1.5095 1.01444 0.941545 0.391626 1.02692 1.01609 0.76772 1.1793 0.917419 0.467967 1.27253 0.837906 0.64116 1.09813 1.34531 1.01829 1.38736 0.957445 1.00817 0.653731 0.581342 0.481782 1.13459 1.25226 0.859947 0.296754 0.329742 1.39889 1.35093 1.46614 1.17192 0.895637 0.591505 0.633623 0.517936 0.519256 0.992546 0.886563 1.22778 0.369552 0.264811 0.613307 0.679026 0.993288 1.02508 0.430361 0.637329 0.810395 1.05119 1.42838 1.12116 0.887919 0.791292 0.660942 0.691139 0.748735 1.94878 1.27318 1.79007 0.731604 0.425661 0.673382 0.623666 1.64256 0.848654 1.57599 0.567108 0.467972 0.679238 1.34967 0.949262 0.877482 0.77513 1.16532 1.40237 1.61969 0.455316 1.30553 2.89897 1.29318 0.891318 1.82335 1.0893 1.2757 1.86959 1.29386 1.23628 0.663461 0.574105 1.06065 1.61202 0.944056 0.632824 0.310138 1.33158 0.622135 0.787821 1.78182 0.564062 0.780883 0.397798 1.11503 1.23248 1.26738 0.830269 0.740072 0.704004 0.660313 0.826207 1.10106 0.885503 0.767973 0.557683 0.263099 1.09797 1.05586 1.47723 1.15466 0.584853 0.347011 0.54934 0.692598 0.407936 0.730882 1.27289 1.43575 3.16359 0.912461 1.25712 0.845816 1.13986 0.611326 1.37236 0.468884 1.02811 0.802074 1.00218 1.31524 0.564923 0.861906 0.712075 1.77566 0.309677 1.18139 1.32936 0.924904 0.965927 0.273637 1.16016 1.87782 1.29114 1.5491 0.79764 0.771008 1.08553 0.415697 0.670171 0.980745 1.12576 1.52061 1.39956 1.39924 0.743292 1.84246 1.60036 1.22813 1.96606 1.623 0.894053 1.86835 0.886424 1.01933 1.42798 1.29438 1.20904 0.734769 12.567 1.41169 0.659412 1.34626 0.664352 0.600977 1.4464 0.859754 0.657035 1.58014 0.923949 1.25059 1.50742 1.89371 0.602832 0.84014 0.780781 1.09297 0.757983 0.811323 2.30862 0.834743 1.79824 0.629055 2.13181 1.21592 0.930099 1.15487 0.476918 0.953006 0.846292 1.26448 2.48583 0.822824 3.55513 1.67289 1.27088 2.34242 1.12296 1.28584 1.24284 1.25487 0.759648 2.02085 0.680605 0.68613 0.583833 0.888725 1.29098 1.17172 2.23525 0.896635 0.71513 1.67164 1.19634 1.25441 1.32349 0.887851 1.00113 1.92667 1.64378 2.2635 1.51189 0.905915 2.3566 0.907192 1.65764 1.0862 1.35782 1.10043 0.629089 0.812465 1.54837 1.78887 0.967534 6.27183 2.07681 1.4063 1.93756 0.587335 0.42457 1.8384 1.11108 0.971155 0.788905 1.29122 1.43109 0.598589 0.885937 1.01251 0.816352 0.288753 0.696353 1.77769 1.13134 1.48457 1.84711 1.11323 1.14598 0.966276 0.906656 1.42855 1.7414 0.422798 1.22174 0.466 1.3228 1.86707 0.750449 2.06117 0.522211 1.51342 2.2315 1.57146 2.73132 1.45823 0.906007 0.899082 1.11999 1.68567 1.02213 1.32102 0.568695 1.77843 1.45234 1.25601 0.994969 1.18893 0.928876 0.630572 0.909403 1.0271 0.929158 1.08727 1.58476 1.08044 1.07851 1.1608 1.19594 0.399857 0.253704 1.12341 0.960273 1.04117 1.18337 1.57045 0.366916 0.75954 1.95708 1.06115 2.45561 0.734591 1.60425 0.534566 1.12811 0.720251 0.964065 1.46067 1.45433 0.702195 1.21847 0.999549 0.713727 0.809293 0.741076 0.688789 1.09464 1.09767 1.13886 0.595942 0.876375 1.04233 1.16294 0.935631 1.27315 0.973423 1.30394 1.11156 1.2521 1.20049 0.731579 0.573715 0.961613 1.4304 0.853784 1.36153 0.72928 3.03335 0.525184 1.14088 0.866617 1.74575 1.13426 0.949401 1.2667 1.02731 1.36295 2.35106 2.05183 1.07233 1.04739 1.16264 0.750587 1.17979 0.544281 2.25913 0.901338 0.427838 0.582119 0.605979 0.696978 0.789612 1.20285 0.296473 0.471468 1.64687 0.673915 0.91034 1.63611 1.39233 1.39408 0.950763 0.571753 0.769513 0.963916 0.88829 0.974428 0.512328 1.42946 0.436154 1.45907 1.17717 0.737597 1.87565 0.917759 1.07187 0.329773 0.939055 1.30447 1.17404 0.517972 0.751385 4.39948 0.667851 1.04689 0.892685 1.42147 0.617087 0.950493 1.58443 0.966836 1.24595 2.23543 1.3408 0.98607 1.17106 1.35546 0.470599 0.454112 1.07377 0.712159 1.00361 0.485515 0.567976 0.745458 0.977849 0.992965 3.04705 1.61117 0.875696 1.3594 1.00232 0.626931 1.39269 1.17893 0.463302 1.24233 1.19929 1.17213 2.51281 1.65704 0.64755 0.589659 0.651188 1.48451 2.21637 0.826406 1.51051 0.809682 1.09302 1.70941 0.48664 1.06957 2.12423 0.582057 2.18866 0.909239 2.3712 0.697974 1.50529 0.889596 0.926287 1.18995 1.42269 0.782352 2.26211 1.30204 2.28472 1.23837 1.03034 2.97163 1.64119 0.859224 0.508461 1.49107 1.25144 2.41322 1.79455 2.18044 1.13279 0.947078 1.51106 1.7813 0.960541 1.0988 1.0012 1.76626 1.18313 1.36926 1.42825 1.53168 2.04919 0.181387 0.694908 1.08857 0.764223 1.08329 1.64479 2.71527 1.03237 0.953221 1.62951 0.903101 1.40697 1.82384 1.70198 1.7353 2.5635 1.63121 1.54965 1.14102 0.566429 0.725041 0.56042 0.749438 1.51268 1.41166 1.25906 0.90533 0.838786 0.730024 1.43102 1.7526 0.912134 0.807051 1.66865 1.07004 0.882056 1.90097 1.12837 2.42185 1.09427 1.02798 0.579617 1.56239 1.3171 1.25758 0.674957 1.13701 0.856902 0.566412 1.54916 1.78747 2.35926 1.48541 2.00335 0.91515 1.73647 2.0542 1.98543 2.04115 1.17487 0.97501 0.875027 1.62957 0.407587 1.34551 0.891777 0.867739 1.05865 1.22759 0.863274 1.59409 2.19566 1.1665 0.936197 1.98307 1.58837 1.11934 1.34479 0.748285 1.00833 0.836926 1.1665 1.55678 0.862071 1.34722 2.3046 2.51535 1.13765 1.42133 0.623698 0.948975 0.804294 1.33034 1.15353 1.18201 0.698001 0.805265 0.601551 0.785716 0.834033 0.522179 1.68406 0.944639 0.71711 0.808238 3.72746 0.453888 1.93552 0.823606 1.59611 1.5036 0.907737 1.14958 4.43837 1.44991 1.02875 1.16966 0.577785 0.718859 0.716821 1.57756 1.47865 0.833678 1.45447 0.704966 5.22238 0.87227 0.616352 1.00812 2.02207 2.23812 0.846651 1.05802 1.88338 2.60377 1.68436 0.960358 1.15496 1.3335 0.718815 1.39398 1.60489 0.790836 0.714674 1.86969 1.84263 0.927795 1.59282 0.708373 1.72315 1.21359 1.73864 2.35514 2.32144 1.10342 0.778585 0.584924 0.974996 1.14635 1.25608 1.20087 1.00354 1.42653 0.610616 1.29502 3.10396 0.645153 0.425201 0.795327 0.442965 0.904294 1.11377 1.13539 1.06157 2.61835 1.81955 1.16309 1.16361 1.28055 1.10903 3.33333 1.41215 1.70984 1.26198 0.617682 1.00168 0.812782 1.18246 1.82523 1.42609 1.06584 1.57846 1.17393 1.44927 0.638115 1.19529 1.00786 0.802667 0.841566 0.94605 0.907702 0.927748 2.94456 1.11704 1.39509 1.05912 1.45273 1.81041 1.2514 1.72956 2.26015 1.27603 2.90867 1.86249 1.10308 1.55551 0.539907 1.94481 1.8486 2.07358 1.53547 1.27242 0.445138 1.43516 1.21338 0.941101 1.07744 1.09654 0.79263 1.27363 1.42708 1.92712 2.57087 1.064 0.881545 0.771004 0.729295 1.3774 0.925777 2.31012 0.367708 0.744696 1.10224 1.27666 0.693716 1.10834 2.77283 0.74865 0.90239 2.82317 0.6029 1.33642 1.78239 1.21053 1.19833 1.8927 4.07351 1.38561 0.201994 0.577462 1.60825 0.593024 1.87799 1.235 2.04078 0.979896 0.784432 3.9409 0.937137 1.13497 1.04701 1.44434 2.74833 1.80528 2.34517 0.62909 0.900853 1.51492 1.7069 1.14769 0.749515 2.65393 1.54749 1.32089 1.23985 0.943488 2.54553 1.36814 1.1757 1.66886 0.897335 1.12141 1.7594 1.28666 0.626 2.50533 1.32063 0.616667 0.847838 0.986063 0.923013 1.61148 1.54056 1.38863 1.2848 1.82218 1.50752 2.03129 1.44014 0.932398 1.13266 0.53633 1.15868 1.71988 1.31473 1.47798 0.85837 1.10231 1.09514 1.94206 0.853394 1.00321 1.50739 1.77861 0.502474 1.31121 1.33478 0.826258 1.3147 1.79513 1.86326 1.07925 0.703202 0.894848 1.33392 0.889271 0.66176 0.749216 2.50959 1.73004 1.47304 1.28047 1.73613 0.671577 0.993872 0.779269 0.766 1.26111 0.958622 1.59641 0.64968 1.68522 0.821918 1.40083 1.42424 1.70345 0.947019 1.85186 0.964883 1.57398 0.857015 1.4532 0.624873 0.597802 1.09168 2.60968 0.849699 0.803492 4.03383 1.0136 2.01697 1.2244 0.864704 1.19674 1.13685 0.955438 1.58273 3.96551 1.62196 1.33716 1.08393 0.934408 1.34802 1.74563 1.74164 0.747902 0.736895 1.08409 1.72984 1.84223 1.10012 1.51072 1.21592 1.65828 1.93219 0.994557 0.905293 1.3985 1.74597 1.17311 1.36993 1.85817 0.930198 2.02363 0.9568 0.820542 0.790465 0.887829 1.35449 0.750616 0.683739 0.993061 1.16489 1.65595 1.23802 2.16242 1.47186 2.33889 1.17461 1.62891 0.961061 3.1745 1.30451 0.765764 0.976154 1.21197 0.911291 1.32785 1.72976 1.46284 1.87027 1.14783 1.15247 1.76892 1.13185 0.643595 1.27683 0.541734 0.632129 1.50376 1.02016 0.789189 1.133 1.30279 1.04374 1.09402 1.02904 1.49377 1.15151 1.14344 0.632273 1.08629 0.887348 1.04561 1.05517 1.66148 1.12599 0.711818 0.957736 0.902102 1.17 0.938242 1.16725 1.20107 1.92529 1.882 1.32942 1.02714 0.739179 1.55343 1.43463 1.52941 0.920349 1.7162 1.14287 0.456414 0.646274 1.26937 0.464978 1.12235 0.725524 2.27883 1.54113 1.09385 1.5039 1.93892 0.988888 1.3388 1.16659 1.39075 2.08438 1.91425 1.5001 1.79241 1.49595 1.66173 1.57482 1.46052 1.84569 0.466122 2.1527 0.646646 2.24542 0.754188 2.16828 1.22706 2.18944 1.64194 1.58099 1.45326 0.407628 1.95553 0.800355 1.85531 1.53113 1.44961 2.6898 0.751466 1.82324 1.45942 0.952851 1.31936 0.788647 1.03103 0.604475 2.28902 1.17043 1.31286 1.4535 0.797595 1.47675 0.999371 1.28336 1.13452 1.97092 1.36397 1.81065 1.92272 1.79347 1.12542 0.60492 0.705661 1.33641 2.15425 0.798597 1.70153 2.00814 1.74142 1.10877 1.27706 1.03912 1.69395 0.685713 0.871699 0.790641 1.35613 1.12108 1.0111 1.3462 0.882657 1.63935 0.980262 1.55787 0.615713 0.599671 1.0655 2.10741 2.86617 1.64354 1.4557 1.86981 1.26181 0.771095 1.93725 0.804963 0.975488 0.996743 1.13006 1.86135 1.08215 1.0555 0.700383 1.36652 1.74989 0.784092 1.30869 1.19545 1.74775 1.02331 1.62905 0.949307 1.77719 2.75655 1.46983 1.38723 1.42806 0.328429 0.304241 0.708897 1.37965 1.70421 1.73241 1.36102 1.24413 1.24419 0.693444 1.22843 1.35356 0.940709 0.853286 3.99724 1.30608 0.764801 0.839147 1.54143 1.53103 1.45649 0.879496 2.07064 1.57565 0.848597 0.76443 0.604194 1.33694 1.47807 0.609467 0.81731 1.00215 0.699652 0.629906 1.39659 0.901206 1.2809 0.698634 0.840467 0.906086 0.49991 1.52204 1.28229 0.832226 0.514254 1.6081 1.25377 1.24551 1.52644 1.24992 1.51363 1.71093 0.47959 2.30416 0.445852 0.867753 1.37901 1.02522 1.40193 0.563612 0.899421 1.89889 1.65348 1.14995 2.58417 1.59285 1.10946 1.41416 1.43756 2.49303 1.4087 2.14694 0.792965 1.71149 2.2939 1.32999 1.2316 1.05155 0.675629 1.24525 1.54117 1.96398 0.777112 1.37883 1.94478 9.14773 1.21289 0.879132 1.73668 1.73959 1.45448 2.16192 1.95344 1.01493 0.749716 1.43084 0.65467 0.968421 1.57701 1.12483 1.50069 2.34305 1.53435 1.12709 1.44626 2.31046 2.29372 1.8573 0.644993 1.18628 1.03689 1.45437 2.10335 1.01921 3.38825 1.33807 1.58394 1.26667 1.50443 0.753804 1.27094 0.74249 2.21099 0.715574 0.819124 1.13174 1.48755 1.25952 0.933531 1.79515 1.16804 1.28645 1.02159 1.78967 0.859915 1.02082 1.76386 1.205 1.4063 ]
@@@ Frame-accuracy per-class: [ 90.3895 78.2007 67.451 84.089 80.939 71.2644 69.7406 79.2651 67.7013 58.1379 86.6426 64.3026 61.9385 73.788 39.5604 79.4473 70.7182 60.6114 60.6711 72.9642 81.6825 64.8739 68.2927 80.5281 88.984 71.0204 85.4473 29.8592 75.6757 55.5891 70.7617 67.2566 72.3514 52.2822 83.6923 80.4565 54.2942 46.7662 68.7719 64.5422 85.8906 67.9612 64.0777 74.5429 57.5916 69.1145 86.2545 58.4397 72.594 81.1752 67.9245 54.7529 66.0436 56.4211 76.808 63.6021 75.6627 81.8086 85.2748 61.3769 62.9126 74.6269 90.0894 89.139 58.8235 45.5531 52.8053 64.877 74.8482 80.2005 83.3579 85.3878 86.8492 67.8753 77.1513 67.2897 88.5017 93.067 81.335 93.5593 71.7241 68.5165 84.8181 81.8428 81.5087 63.5052 62.8019 70.2341 72.2121 80.9917 82.7453 81.3937 78.2349 38.9078 64.8199 50.1529 79.219 87.658 80.8549 81.7625 47.2946 77.095 47.343 82.768 84.3057 77.6963 64.7482 72.4384 71.8447 73.9927 58.3496 59.2992 51.4604 85.7623 56.7976 18.705 54.6952 85.906 42.0202 67.162 60.8696 53.5593 65.5319 56.8093 80.4961 78.6939 74.8936 52.5896 81.9048 82.4645 90.5384 58.6103 78.8871 78.1549 46.9136 88.4034 80.8989 87.1949 72.9017 63.4845 58.5507 72.956 77.2603 77.8236 83.3645 80.6584 64.9205 77.5889 74.814 83.5073 92.3374 66.563 70.1538 50.6964 65.9218 81.176 90.4186 85.6397 78.4491 85.9453 74.9425 64.5995 59.7531 14.786 70.6994 67.6289 75.7522 74.2049 80.0441 57.2178 85.7875 70.8249 76.867 72.4221 67.0623 80.8511 73.0539 77.2414 50.989 90.3303 66.0465 57.0743 77.8523 70.5394 93.5065 60.8479 40.2685 58.7896 52.4064 77.8416 79.1069 63.5332 88.8019 78.9189 68.9826 65.1357 45.76 60.2899 57.1429 73.9274 40.8472 41.535 63.1829 40.8759 49.0323 72.3204 33.8028 75.1108 69.7274 58.498 70.5202 58.4289 80.5755 0 60.0302 80.7415 58.6387 80.5369 77.6119 59.9696 73.8824 80.4388 55.1959 64.1509 60.241 47.8405 31.3043 88.7805 70.317 70.1754 72.1461 74.5604 76.6106 34.7339 77.3931 44.5714 81.1621 30.3448 47.9087 74.1996 67.1096 82.3529 74.5205 75.5245 61.6702 10.0719 74.6667 16 47.9419 65.0823 39.7661 72 61.8182 67.1533 64.3985 81.8942 49.5512 80.2303 78.553 82.3977 74.2225 65.7244 66.2045 48.7455 71.0808 81.9277 51.0441 64.6154 48.505 71.8816 70.2011 64.764 39.039 47.2786 35.012 48.8189 76.3359 27.5862 75.5187 49.3431 69.0071 58.5956 58.2857 80.5015 74.2857 60.0683 47.9087 70.8709 0 46.9526 62.1782 45.7485 84.8728 90.4905 46.3263 68.5225 69.6379 75.7021 65.7534 61.2059 81.1044 76.6839 68.6016 72.3658 91.2 83.6697 52.2523 65.4275 56.2118 43.956 73.029 68.0352 70.407 76.259 58.156 47.4308 92.4731 66.5568 86.9565 68.1564 35.2941 84.5714 44.1989 84.8268 40.8163 29.2975 45.3097 29.5082 60.251 71.7131 71.4097 58.8235 58.0475 74.2857 64.2202 82.0306 47.3029 58.4767 65.4788 70.6767 64.3357 69.9869 91.3295 73.0946 62.1118 73.9394 67.5737 50.7937 75.1092 74.5098 63.1579 62.6781 87.8213 92.7467 70.2774 75.1857 61.9926 69.7509 56.4103 88.2221 77.6509 51.8934 68.7023 31.4496 79.5883 47.2131 88.0361 68.9972 78.7879 76.2542 55.7559 59.3707 77.5541 65.0284 64.2722 85.1986 78.5219 75.7502 80.1299 66.1582 66.0955 60.5544 84.1972 71.9735 65.5126 65.4028 73.0733 52.1739 72.3227 58.8832 60.6791 65.0367 70.4362 76.9826 80.7626 69.808 59.5041 77.3481 62.069 78.5579 9.34579 89.5464 61.4925 70.2398 51.8272 67.0487 74.5501 61.7535 70.9232 58.6451 37.7143 44.6069 70.8772 70.6131 65.8307 82.5607 60.6061 82.4402 31.2343 73.9726 88.1023 80.5861 82.8319 81.7734 77.8626 69.5096 91.3799 86.7168 51.9824 80.0937 72.5084 51.215 61.8947 63.3124 72.8972 82.774 78.1582 72.9412 73.2436 73.3997 84.9386 62.1723 84.5176 56.7718 66.0984 81.3793 32.6531 73.0435 72.447 91.3711 70.6302 54.5455 68.1319 83.6697 82.8685 0 66.2379 71.0801 68.3417 52.09 81.5534 72.6698 45.8284 68.1027 65.529 34.5382 55.7214 72.6841 67.4487 60.4878 91.1032 88.429 66.8977 80.9491 71.8584 83.5913 82.227 74.3802 65.5222 71.3693 20.2247 54.8454 76.0148 56.3107 70.6714 85.2018 52.8814 72.0195 87.2167 63.1579 57.7947 64.3216 19.1781 52.4345 81.9304 77.4624 80.1956 57.4586 38.3764 73.93 54.1833 73.9073 63.0705 53.9597 89.4327 63.4508 34.1014 84.1256 32.967 71.6578 42.1053 77.9547 52.1739 70.1627 68.6642 72.428 62.7338 74.8019 28.9406 64.5161 32.7645 66.3212 75.8294 26.7898 54.2636 73.7921 86.3616 57.0825 71.9818 58.7678 46.4671 23.7443 66.7651 67.7091 55.1724 62.2951 70.9859 69.0307 69.3141 28.8066 58.4392 68.8525 58.3732 54.4987 46.6019 95.4839 80.6067 66.6667 77.9747 69.4136 44.4444 30.137 72.216 73.1591 41.8605 72.3247 54.4379 55.2381 40.4145 47.2663 27.1881 51.664 55.3633 69.8667 81.8455 79.1923 84.8034 79.6915 52.8302 63.3663 67.1329 72.1541 73.7485 81.5822 58.006 52.9002 74.339 77.3848 46.0432 70.1671 70.3672 50.2128 68.2635 28.2686 66.4452 67.4897 79.2059 56.7627 61.2059 60.1227 87.2727 71.5655 72.6568 81.8578 55.4649 41.4784 50.6667 53.2423 43.2432 74.3769 45.2611 48.0836 36.5957 41.9006 60.9795 73.4824 75.4491 48.5981 88.3671 60.5505 75.2549 78.4103 69.6884 60.9442 70.4403 53.9683 31.0984 67.3835 68.2081 40.6312 46.9453 67.9803 68.6792 76.7883 69.0125 77.3498 67.5462 50.9603 69.0554 58.296 33.2468 30.9166 70.9832 58.319 81.9734 76.3285 79.5699 51.3011 63.7398 68.323 81.989 74.8616 78.6237 82.4859 71.0947 88.7892 52.2427 72.8762 77.9402 74.3847 1.15607 85.3755 42.2111 80.2614 54.5035 48.7633 80.7114 63.3205 6.10687 54.1436 58.1333 63.4146 84.193 74.6494 78.7428 49.2929 65.4088 73.4499 56.9558 79.3959 0 73.4952 81.2804 68.6838 37.1365 36.2031 73.8286 72.9529 39.8754 38.4 50.4399 73.8462 67.8445 65.1934 78.453 52.1964 49.635 76.5524 78.7879 31.8841 45.3674 71.7808 48.5437 81.9599 45.4148 58.2915 43.9141 26.3305 18.4615 65.1163 78.51 80.6794 69.6017 69.7674 63.9676 59.6434 75.717 59.8465 82.0327 65.9649 17.9372 81.5612 88.6184 76.7599 88.7553 77.3246 64.257 62.1965 61.7143 32.7273 43.7534 65.3295 65.9542 62.5277 57.2379 0 65.6168 58.3486 68.0498 80.0558 72.3727 73.4591 66.5946 49.2659 38.8626 68.0057 52.8529 64.0205 55.4273 83.9577 66.9704 71.785 71.3287 79.6657 70.4062 68.2189 75.9305 16.8224 68.0702 66.2768 72.3247 67.7686 43.5798 64.5533 33.9623 46.3277 54.8303 10.7784 42.9752 61.0932 56.2674 84.8576 31.0757 56.1798 25.5875 48.59 59.4524 87.2727 54.0416 63.301 73.4059 70.2355 67.5449 76.5432 56.7122 65.6934 36.6412 28.1346 69.6177 73.716 77.8182 81.0458 59.3081 74.9758 40.1606 90.0061 81.557 65.4088 64.2741 77.1704 68.1018 43.5754 75.2722 74.5418 28.5714 85.8995 62.5821 48.855 65.4545 63.4508 39.6601 35.4486 53.202 93.2666 81.1502 52.1481 85.3595 50.5263 68.525 43.3803 71.1799 75.6353 15.2047 75.8621 59.3486 67.0769 54.7945 40 52.6611 29.1457 79.0787 72.2689 56.1056 50.7042 69.5985 76.5246 39.5257 52.3139 63.7744 60.4915 72.0497 31.8644 65.7596 60.4502 50.1792 71.6164 62.4146 55.3699 55.1461 82.0957 16.3743 66.6667 80.5125 77.787 63.9252 74.6439 53.8642 55.1559 54.0193 54.0541 50.6024 50.8997 33.0134 50.4155 74.1014 66.5535 82.6291 67.9477 38.5542 58.1741 56.1661 73.5484 62.5698 62.8774 45.7971 72.511 73.7968 57.3099 56.3177 88.0466 64.139 58.1242 73.9171 52.1519 60.6452 50.6266 64.2998 79.7404 73.4452 60.6432 71.9472 86.1086 77.0642 32.9825 41.9825 47.8351 57.1429 44.9086 80.4954 71.4026 77.1757 84.2105 65.3422 74.571 52.549 83.7022 52.8736 75.6757 60.8879 57.3876 56.4417 75.4887 40.3587 73.0402 52.9262 73.8337 52.5547 81.8864 84.523 70.9984 21.843 74.4849 75.7522 9.83607 73.2164 47.1464 73.6156 70.1273 63.322 64.632 72.3861 57.5039 8.79121 52.7449 66.6667 68.8784 76.1434 55.1143 49.862 51.6595 83.1068 75.8011 65.6772 48.3755 48.8599 63.0286 58.7287 59.9488 55.3936 40.1826 67.4459 78.6051 60.9481 48.2315 64.7975 62.5387 47.5336 72.2646 30.4183 71.1567 75.5067 77.2532 70.7395 56.0386 77.4796 78.3848 73.9517 63.8743 55.6622 65.0113 34.5178 57.1429 39.9376 67.7638 54.9865 65.641 18.9723 57.6419 77.3604 69.7904 66.5434 75.7552 55.814 51.5901 54.5455 40 65.7997 64.4783 43.9604 64.239 80.6015 66.18 78.8024 76.1077 59.2469 64.9573 77.047 69.2521 60.0423 74.0047 69.5779 69.3201 51.7799 64.9499 62.6301 79.8623 66.9405 73.1836 67.4777 72.1311 52.1576 67.2451 81.7443 73.6672 73.8552 65.2568 71.8028 63.275 71.3057 46.3768 51.1785 58.5499 68.2927 76.1506 57.5107 51.6291 55.5766 69.543 55.2381 68.4492 82.9116 83.8407 57.9783 88.2307 67.6602 76.6773 46.5306 62.0199 67.2489 57.1429 27.4112 70.4801 65.5738 72.6154 60.5433 32.618 40.2685 52.0947 52.1519 53.1073 46.1095 56.3707 57.3134 43.7956 88.8889 37.7622 77.8265 38.7435 77.294 42.4096 68.2635 34.0081 49.6084 59.6577 54.5969 88.8322 26.087 79.6421 48.8449 49.459 56.3923 14.0704 78.0793 46.6368 54.0773 74.4063 60.5364 75.9651 76.7656 83.5473 34.1969 70.9447 63.1791 50.8004 74.6298 48.5106 70.2988 61.1727 70.3196 44.7876 57.6981 47.3896 43.0493 52.354 67.3511 83.9635 78.9572 58.6466 46.1538 74.6048 50.1629 39.7291 50.8961 62.8352 65.8098 73.9165 47.9233 83.7209 74.8163 71.4055 63.4271 60.0306 69.9923 62.4865 78.0027 55.0725 68.7419 55.0459 83.0573 84.8485 72.9064 44.489 7.0922 50.2493 60.4006 48.2861 59.5186 81.5115 23.2446 75.7447 66.1909 71.8894 65.0206 35.4776 61.0979 64.4951 80.1187 55.6901 41.8251 77.2616 62.7957 66.373 49.8753 67.1353 51.5759 67.6617 45.7831 24.186 56.2358 63.7037 58.0297 91.2621 93.0993 79.6148 53.6249 52.549 41.0256 56.4593 59.7771 60.7443 80.5395 63.7681 53.0551 67.356 73.4021 9.02256 59.952 78.346 79.4816 58.0311 58.319 61.6822 72.1088 43.4193 51.5464 75.6349 77.6445 80.7966 59.8826 61.9116 80.6313 70.5455 70.1195 81.4815 80.2591 66.1996 67.7201 67.5269 84.3707 75.8051 73.7034 85.4621 53.7259 61.8858 74.1355 82.8208 54.2094 70.7355 69.7872 52.3282 53.7634 50.3226 51.0501 85.8639 28.1938 93.2515 72.0307 61.1465 67.7716 63.1517 85.6495 74.0594 54.6201 51.0018 66.4093 35.3591 53.5666 68.8525 60.0567 50.2756 30.6977 56.2319 45.5516 79.4203 39.1252 31.7757 59.2164 66.4407 68.7556 78.2228 58.1132 55.4974 46.8085 75.6923 54.0541 38.8571 0 63.5294 76.3636 49.2441 52.7307 51.3899 35.1131 36.7601 67.8492 79.3846 55.0984 79.3077 70.5882 47.7759 71.7622 58.5827 36.6387 55.9486 69.0253 49.9494 36.3636 42.3398 38.4416 82.2328 64.4258 72.4638 60.1467 28.5714 70.51 4.72441 62.8734 64.745 63.3397 53.4513 72.3404 66.3946 80.6378 41.2903 78.3562 75.9428 67.5291 58.9831 62.8488 68.0073 40.3071 69.5096 69.9187 68.7873 48.0565 76.7144 66.3239 48.6739 69.8413 59.5156 ]

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:347) Done 2622 files, 1 with no tgt_mats, 0 with other errors. [TRAINING, 0.34124 min, fps36878.4]
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:353) AvgLoss: 0.745492 (Xent), [AvgXent: 0.745492, AvgTargetEnt: 0]
progress: []
FRAME_ACCURACY >> 76.8142% <<

