speech-HP-Z2-Tower-G9
nnet-train-multistream-perutt --cross-validate=true --randomize=false --verbose=0 --num-streams=10 --max-frames=15000 --feature-transform=exp_FG/blstm4i/final.feature_transform 'ark:copy-feats scp:exp_FG/blstm4i/cv.scp ark:- | apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_cv10/utt2spk scp:data-fbank/train_cv10/cmvn.scp ark:- ark:- | add-deltas --delta-order=2 ark:- ark:- |' 'ark:ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- |' exp_FG/blstm4i/nnet/nnet_iter04 
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:243) Not in compute-exclusive mode.  Suggestion: use 'nvidia-smi -c 3' to set compute exclusive mode
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:438) Selecting from 1 GPUs
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:453) cudaSetDevice(0): NVIDIA RTX A2000 12GB	free:11515M, used:515M, total:12031M, free/total:0.957153
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:501) Device: 0, mem_ratio: 0.957153
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:382) Trying to select device: 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:511) Success selecting device 0 free mem ratio: 0.957153
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:FinalizeActiveGpu():cu-device.cc:338) The active GPU is [0]: NVIDIA RTX A2000 12GB	free:11001M, used:1029M, total:12031M, free/total:0.91443 version 8.6
copy-feats scp:exp_FG/blstm4i/cv.scp ark:- 
apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_cv10/utt2spk scp:data-fbank/train_cv10/cmvn.scp ark:- ark:- 
add-deltas --delta-order=2 ark:- ark:- 
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:144) CROSS-VALIDATION STARTED
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Read():nnet/nnet-matrix-buffer.h:191) Read() started... Buffer size in MB: 0, max 3072, having 0 utterances.
ali-to-post ark:- ark:- 
ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl 'ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |' ark:- 
LOG (copy-feats[5.5.1074~1-71f3]:main():copy-feats.cc:143) Copied 296 feature matrices.
LOG (apply-cmvn[5.5.1074~1-71f3]:main():apply-cmvn.cc:159) Applied cepstral mean and variance normalization to 296 utterances, errors on 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:303) ### After 0 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:304) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.462523, max 0.503519, mean 0.00397666, stddev 0.0801092, skewness 0.0101798, kurtosis 0.256305 ) 
  f_w_gifo_r_   ( min -0.422243, max 0.399551, mean -0.000603555, stddev 0.0771586, skewness -0.00046548, kurtosis 0.000619411 ) 
  f_bias_   ( min -0.351716, max 1.31913, mean 0.213927, stddev 0.45893, skewness 1.06779, kurtosis -0.655864 ) 
  f_peephole_i_c_   ( min -0.465029, max 0.442255, mean -0.00218984, stddev 0.124593, skewness 0.124501, kurtosis 1.11836 ) 
  f_peephole_f_c_   ( min -0.705292, max 0.785748, mean 0.00318549, stddev 0.164768, skewness 0.259382, kurtosis 4.4216 ) 
  f_peephole_o_c_   ( min -0.515255, max 0.415366, mean -0.00973649, stddev 0.175047, skewness 0.218968, kurtosis -0.210873 ) 
  f_w_r_m_   ( min -0.558468, max 0.468663, mean 0.000596786, stddev 0.0996326, skewness 0.000344229, kurtosis -0.0201168 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -0.860912, max 0.762125, mean 0.00627521, stddev 0.0848214, skewness -0.0544364, kurtosis 1.65198 ) 
  b_w_gifo_r_   ( min -0.367031, max 0.293487, mean -0.00021847, stddev 0.0692153, skewness 0.00106212, kurtosis -0.351764 ) 
  b_bias_   ( min -0.318202, max 1.18163, mean 0.20853, stddev 0.449851, skewness 1.05414, kurtosis -0.686093 ) 
  b_peephole_i_c_   ( min -0.355501, max 0.27163, mean 0.00533876, stddev 0.0905548, skewness -0.102579, kurtosis 0.886673 ) 
  b_peephole_f_c_   ( min -0.600643, max 0.661436, mean 0.0116935, stddev 0.155359, skewness 0.547026, kurtosis 3.77405 ) 
  b_peephole_o_c_   ( min -0.541353, max 0.465101, mean -0.0167221, stddev 0.180103, skewness -0.11891, kurtosis 0.231351 ) 
  b_w_r_m_   ( min -0.383361, max 0.359649, mean -0.000205052, stddev 0.0875846, skewness 0.000784828, kurtosis -0.102961 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.914352, max 0.724912, mean -0.000155918, stddev 0.105338, skewness 0.00617894, kurtosis 0.0667777 ) , lr-coef 1, max-norm 0
  bias ( min -0.0775918, max 2.21985, mean 5.12227e-10, stddev 0.0723276, skewness 23.5702, kurtosis 697.995 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:305) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -11.004, max 12.2048, mean 0.00682255, stddev 0.969149, skewness 0.287893, kurtosis 3.83541 ) 
[1] output of <BlstmProjected> ( min -3.96536, max 4.17651, mean -0.000872654, stddev 0.735008, skewness 0.00485745, kurtosis 0.935422 ) 
[2] output of <Tanh> ( min -0.999281, max 0.999529, mean -0.000576908, stddev 0.506913, skewness -0.00171826, kurtosis -0.815601 ) 
[3] output of <AffineTransform> ( min -11.5689, max 19.1574, mean 0.0130321, stddev 2.27482, skewness 0.654812, kurtosis 2.09077 ) 
[4] output of <Softmax> ( min 7.3485e-12, max 0.998929, mean 0.000779932, stddev 0.0154122, skewness 39.4165, kurtosis 1830.38 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:334) ### After 79212 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:335) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.462523, max 0.503519, mean 0.00397666, stddev 0.0801092, skewness 0.0101798, kurtosis 0.256305 ) 
  f_w_gifo_r_   ( min -0.422243, max 0.399551, mean -0.000603555, stddev 0.0771586, skewness -0.00046548, kurtosis 0.000619411 ) 
  f_bias_   ( min -0.351716, max 1.31913, mean 0.213927, stddev 0.45893, skewness 1.06779, kurtosis -0.655864 ) 
  f_peephole_i_c_   ( min -0.465029, max 0.442255, mean -0.00218984, stddev 0.124593, skewness 0.124501, kurtosis 1.11836 ) 
  f_peephole_f_c_   ( min -0.705292, max 0.785748, mean 0.00318549, stddev 0.164768, skewness 0.259382, kurtosis 4.4216 ) 
  f_peephole_o_c_   ( min -0.515255, max 0.415366, mean -0.00973649, stddev 0.175047, skewness 0.218968, kurtosis -0.210873 ) 
  f_w_r_m_   ( min -0.558468, max 0.468663, mean 0.000596786, stddev 0.0996326, skewness 0.000344229, kurtosis -0.0201168 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -0.860912, max 0.762125, mean 0.00627521, stddev 0.0848214, skewness -0.0544364, kurtosis 1.65198 ) 
  b_w_gifo_r_   ( min -0.367031, max 0.293487, mean -0.00021847, stddev 0.0692153, skewness 0.00106212, kurtosis -0.351764 ) 
  b_bias_   ( min -0.318202, max 1.18163, mean 0.20853, stddev 0.449851, skewness 1.05414, kurtosis -0.686093 ) 
  b_peephole_i_c_   ( min -0.355501, max 0.27163, mean 0.00533876, stddev 0.0905548, skewness -0.102579, kurtosis 0.886673 ) 
  b_peephole_f_c_   ( min -0.600643, max 0.661436, mean 0.0116935, stddev 0.155359, skewness 0.547026, kurtosis 3.77405 ) 
  b_peephole_o_c_   ( min -0.541353, max 0.465101, mean -0.0167221, stddev 0.180103, skewness -0.11891, kurtosis 0.231351 ) 
  b_w_r_m_   ( min -0.383361, max 0.359649, mean -0.000205052, stddev 0.0875846, skewness 0.000784828, kurtosis -0.102961 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.914352, max 0.724912, mean -0.000155918, stddev 0.105338, skewness 0.00617894, kurtosis 0.0667777 ) , lr-coef 1, max-norm 0
  bias ( min -0.0775918, max 2.21985, mean 5.12227e-10, stddev 0.0723276, skewness 23.5702, kurtosis 697.995 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:336) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -7.66434, max 7.26584, mean -0.0147177, stddev 0.961893, skewness 0.577064, kurtosis 2.62679 ) 
[1] output of <BlstmProjected> ( min -4.58957, max 4.23417, mean -0.00222865, stddev 0.694229, skewness -0.0369806, kurtosis 1.68082 ) 
[2] output of <Tanh> ( min -0.999794, max 0.99958, mean -0.000292601, stddev 0.474597, skewness -0.0139979, kurtosis -0.523036 ) 
[3] output of <AffineTransform> ( min -11.688, max 19.7241, mean 0.019466, stddev 2.1599, skewness 0.82503, kurtosis 3.43125 ) 
[4] output of <Softmax> ( min 7.00046e-12, max 0.992707, mean 0.000780803, stddev 0.0179688, skewness 36.8261, kurtosis 1530.06 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:346) PER-CLASS PERFORMANCE:
@@@ Frames per-class: [ 1670 30 8 79 45 6 24 17 70 220 22 36 34 39 10 53 43 120 22 20 34 66 38 35 87 73 52 35 43 17 86 33 19 32 16 75 106 10 5 1873 21251 22 63 135 22 30 80 98 44 43 9 67 66 27 41 51 107 54 285 40 48 24 308 222 21 35 16 33 248 15 54 49 139 81 14 4 45 35 47 26 17 66 4 14 44 25 18 49 111 11 94 64 72 34 66 27 32 33 78 42 15 34 9 135 37 103 31 64 37 21 67 0 93 69 23 2 34 10 32 37 25 34 18 19 28 43 46 13 4 26 88 25 22 43 33 62 12 98 32 25 22 78 11 62 24 13 88 37 45 30 117 35 60 38 37 129 16 23 64 982 29 24 10 20 31 33 34 16 52 44 64 25 22 25 3 9 30 11 42 35 13 12 11 8 32 32 9 20 32 48 32 42 56 36 35 41 27 16 5 29 35 31 12 7 9 45 27 81 21 16 19 72 29 0 46 70 68 37 26 33 50 37 32 9 5 26 23 15 14 128 52 51 59 20 20 8 73 5 17 32 22 16 4 39 60 11 16 7 19 20 8 7 52 22 37 15 32 12 39 94 72 13 59 17 69 35 25 11 13 18 58 26 24 119 16 14 9 5 13 35 18 40 17 83 10 20 20 32 21 19 33 104 48 60 79 47 20 14 27 49 56 7 20 34 199 37 4 3 21 22 15 27 39 6 10 11 22 25 47 21 6 14 6 60 3 43 26 14 8 10 70 4 15 29 11 133 11 35 22 51 21 57 14 109 15 36 29 5 21 20 18 19 114 55 69 35 21 21 64 44 39 43 15 21 37 31 46 70 32 25 50 48 101 37 32 62 34 53 184 59 33 24 47 16 35 9 79 14 45 12 20 16 36 46 8 52 28 13 12 31 17 29 40 53 13 20 9 73 85 128 108 21 7 44 30 32 12 46 15 32 90 31 20 32 52 21 237 45 11 21 215 49 25 30 31 29 71 13 57 48 98 46 27 41 19 60 21 4 38 48 50 5 63 71 0 0 25 47 11 16 42 44 49 71 27 15 49 22 22 11 30 67 41 56 15 28 24 35 7 14 15 34 7 8 38 0 9 28 57 50 12 13 39 28 40 746 35 13 19 19 18 27 12 16 52 60 21 82 1 33 49 89 6 38 42 18 51 39 26 11 19 7 23 28 18 51 98 35 37 10 34 11 56 273 7 39 31 18 21 15 24 22 15 25 6 21 53 59 24 25 15 4 49 28 14 9 8 3 8 29 34 0 8 18 51 52 48 23 3 60 16 24 35 46 16 52 37 72 3 38 59 15 5 40 14 54 52 32 51 4 14 21 187 40 33 16 15 18 19 52 32 28 37 29 41 52 10 6 95 18 46 53 17 8 5 18 31 15 14 27 7 8 17 7 93 28 14 64 22 15 8 48 40 29 32 15 10 26 38 92 13 54 242 3 84 0 22 55 226 137 6 121 19 57 20 16 55 85 4 17 33 53 42 81 83 26 30 36 55 64 9 66 84 47 14 13 91 43 19 13 23 37 17 19 27 24 0 59 88 26 15 20 111 16 50 7 32 22 5 17 15 28 38 22 27 28 48 52 41 28 16 114 105 37 112 43 4 69 17 87 95 11 45 15 22 24 24 65 3 27 29 73 63 47 17 34 18 53 17 37 17 20 6 14 36 78 20 8 14 23 47 6 6 22 3 11 89 3 5 18 20 146 25 8 25 44 29 28 22 26 34 24 73 34 85 42 37 35 79 40 39 34 32 23 18 104 65 9 45 19 21 17 48 42 21 40 28 25 27 73 23 34 2 186 119 39 90 27 23 23 38 48 0 28 66 71 45 13 24 7 25 19 16 23 24 129 50 34 21 69 32 14 26 16 14 79 10 15 35 83 24 21 61 28 128 42 66 19 12 25 28 26 60 26 36 30 128 60 8 15 24 59 47 2 19 63 42 22 4 29 30 49 65 12 8 20 32 71 48 32 17 59 28 24 21 20 22 26 24 15 35 0 17 35 13 29 14 47 49 7 13 6 17 31 19 38 15 53 143 34 68 37 23 6 34 29 7 46 92 30 48 31 3 30 16 40 24 44 69 43 19 65 170 29 5 28 134 73 20 11 82 86 29 13 21 12 10 24 12 81 163 12 193 6 28 39 45 31 23 36 65 11 39 113 30 11 28 125 45 52 34 87 20 29 16 97 42 97 26 45 39 15 109 53 18 59 52 33 29 52 31 45 25 45 136 11 24 33 61 26 28 29 106 30 33 18 48 56 86 92 23 37 23 14 2 14 36 346 138 60 357 25 129 51 39 29 11 53 15 14 13 44 38 41 41 8 18 32 16 20 21 17 8 96 16 23 156 45 91 21 6 43 48 25 45 29 8 22 23 31 42 3 198 10 14 29 25 39 64 39 11 26 28 39 96 13 53 5 34 9 11 21 67 28 24 50 19 10 11 22 23 15 13 30 30 176 37 84 80 124 21 74 32 81 43 23 25 68 52 83 27 20 0 33 56 14 23 30 22 132 113 9 71 18 15 16 5 38 61 32 70 35 44 36 82 12 19 25 56 12 24 164 178 24 42 24 19 11 41 67 112 11 27 62 39 5 26 82 58 18 39 25 15 60 20 122 54 35 24 51 28 24 13 180 152 39 59 14 42 123 70 441 14 35 36 425 22 33 13 41 32 48 41 161 16 18 110 56 36 262 66 33 47 21 15 29 84 58 48 47 2 22 14 22 35 120 98 20 59 274 17 15 22 15 59 16 0 27 31 40 47 63 69 16 68 32 39 426 46 55 29 28 47 14 42 42 14 17 28 110 23 16 27 12 36 25 38 21 57 45 37 40 25 7 21 46 54 29 44 49 16 46 13 48 16 103 37 35 12 34 ]
@@@ Loss per-class: [ 0.840589 1.43525 1.58845 1.85169 1.37394 3.20267 2.39858 0.924239 2.42068 1.88476 2.63657 2.75644 1.0173 2.54282 2.93415 2.71372 2.22471 2.64145 3.85265 0.771362 1.573 2.32468 3.94883 1.95628 2.62456 2.0685 2.24088 5.34964 1.54853 1.52602 5.193 4.31728 2.3494 5.91876 0.342882 0.444024 2.33113 1.56882 4.01341 0.691201 1.01703 3.49277 4.50976 1.76869 1.79856 2.85352 1.97686 7.24778 2.31631 1.15918 3.51818 3.13321 3.17685 2.7351 1.42787 1.80224 2.0973 0.779963 2.29522 1.76312 3.82697 1.99876 0.672369 1.97701 4.45121 1.62932 1.99955 3.09817 0.691485 2.61388 1.81629 1.70426 1.78787 2.58564 1.45342 2.24146 2.78475 0.437079 1.78572 1.66038 2.2562 2.01898 3.76101 1.78674 2.56575 2.52236 3.91147 3.22181 1.21386 3.49808 2.88577 1.07418 1.97937 3.26201 4.08359 3.91266 1.09792 2.70653 1.92823 0.620578 5.10852 1.53025 3.67692 2.22641 0.807282 4.14706 2.37917 2.2336 2.27109 1.55823 2.35199 0 3.82937 1.7852 2.40643 2.75411 2.21044 2.50628 1.9032 2.5949 2.40557 3.26761 2.97634 2.22732 0.828686 2.13103 4.46997 2.38363 1.46245 2.08574 0.795261 1.88913 1.49555 2.70132 3.02711 1.41107 1.59967 0.543083 2.38282 2.90435 3.36904 1.54025 1.61475 1.78131 2.71346 1.08851 1.73049 3.14673 2.71853 1.66201 1.08031 1.78381 2.34156 2.00571 1.88172 0.412474 1.0071 2.49666 1.4657 2.78333 2.51621 2.32108 2.50921 5.90364 1.71337 2.52215 2.98442 2.02854 0.840997 3.19791 1.722 2.25152 2.97031 1.60581 3.76625 0.949295 1.6335 1.90671 2.92253 3.1391 2.32114 2.47877 6.24384 1.89866 1.89769 2.66105 2.7099 2.4124 1.8963 3.09493 1.2361 3.01141 1.95478 2.65813 1.57092 1.29667 1.85662 2.91347 1.88791 1.85203 3.26693 2.72569 1.50338 2.99574 2.82198 1.64576 2.45486 1.35047 2.20476 2.24515 2.36258 2.56563 0.708625 0 2.64409 1.22201 2.53299 1.48278 2.12378 2.10965 1.60385 2.57191 2.96203 1.0728 3.24363 4.3101 3.77974 2.96556 1.07829 3.05065 1.80633 2.38591 2.12472 2.70069 4.52778 3.32077 1.45904 2.50718 1.84465 1.18362 3.54478 2.0311 3.54013 1.20188 3.99931 5.2348 1.75164 5.71417 2.65837 2.85594 3.91214 2.55189 2.75448 0.992414 2.52606 1.10471 1.39439 0.873154 2.30667 0.482884 1.7193 2.24123 2.42642 1.7582 1.6773 0.343137 2.45854 2.19939 0.748372 1.01439 2.30525 0.855812 5.08343 3.8962 2.08911 3.94069 0.752341 4.71204 1.54615 1.97132 1.67875 2.35371 1.69864 3.55493 1.26045 2.35503 0.709415 1.48008 8.55841 1.80532 2.41822 2.65206 1.54055 0.525152 3.98987 3.09281 1.75433 2.34749 4.7018 5.12256 1.96428 2.39131 1.92099 2.8446 0.466145 1.07809 4.32004 1.95319 4.11111 3.72232 2.56376 2.54726 1.87372 1.51418 3.67212 3.18213 1.179 1.30022 1.45913 5.072 4.62102 2.02441 4.21063 1.05677 2.8761 2.48339 3.04828 4.93349 2.71729 2.62036 2.85683 2.22633 2.57397 2.6328 0.820915 2.45907 2.71307 2.29929 2.28981 3.27505 2.33896 1.6697 2.25066 2.16348 3.59461 2.4255 2.21951 2.88527 2.13311 3.02109 2.54143 2.6292 1.13 1.33895 2.20929 2.11531 2.25816 2.78624 3.37345 2.39467 1.50732 2.96154 1.88153 2.41189 0.957974 5.88592 1.22845 1.79428 2.78227 3.39974 3.50138 2.824 1.50725 2.37917 0.811862 0.773367 1.25318 1.21532 2.26215 2.47906 2.27696 2.32961 0.654486 0.706695 1.44546 1.99386 2.86596 3.22558 2.48391 3.91233 1.89699 2.75419 3.32074 2.09605 3.10733 1.0634 2.73186 5.24607 2.47064 2.29053 4.19611 1.74789 1.7695 0.507395 3.02889 0.480912 1.16012 2.8903 2.3259 3.61094 4.73336 2.76888 1.19619 3.50403 3.12193 1.53959 1.35135 0.671384 3.66192 1.61585 0.820169 1.44349 0.522557 0.645993 5.09382 1.67781 1.25682 1.18977 4.97925 1.1633 2.71126 2.13286 2.68433 2.83627 2.9848 1.06652 2.3597 2.6187 3.13641 2.02005 1.22754 2.10606 2.60352 3.05115 6.34314 1.05173 4.36135 2.1335 1.43533 2.50894 2.03495 2.39181 3.09716 2.89489 0 0 0.256639 1.99927 2.00173 2.0161 0.775913 2.0003 2.55403 2.23341 3.09767 3.40744 2.66012 0.878414 1.76936 2.56654 0.962082 1.43985 1.55414 0.814286 2.51751 0.940126 0.820567 2.68412 4.62326 3.27163 4.13954 1.88824 3.24102 3.0883 2.64265 0 2.13335 3.00318 1.34039 1.69302 2.18943 3.59328 3.9316 6.33786 1.11524 1.66607 1.85718 4.22586 3.60532 1.50833 1.69977 1.64116 2.09582 5.59348 0.721057 2.04461 3.08141 1.30649 4.81648 1.96807 5.65935 2.10689 3.45371 1.32205 2.74313 2.27975 2.79748 2.91615 3.62398 2.65947 4.421 2.4547 2.83438 4.2091 2.85219 2.15923 2.82104 3.74186 4.09106 2.02584 5.12992 3.07359 4.47047 4.48337 2.16557 2.11559 1.90162 1.45467 2.10131 2.31437 1.34838 5.79726 3.15092 1.02158 4.10877 1.45691 1.28423 3.30711 1.59362 2.52575 2.67398 3.96796 2.003 1.32463 3.99459 1.21947 1.77944 2.47914 1.65283 2.52077 4.47639 0 2.62486 2.89904 1.25848 2.37258 1.56886 1.61441 1.406 6.56846 2.79361 2.39365 1.85594 0.86251 3.567 6.18767 3.69335 1.29816 1.49432 4.9645 3.02326 3.30006 2.59012 4.67253 1.60983 1.32245 5.36989 3.84585 1.97828 1.76194 1.15096 2.84137 1.69522 1.7896 3.44753 2.40557 2.28724 2.45247 3.95404 1.26129 2.09563 4.36632 3.34072 2.80335 1.52793 2.45614 2.67661 2.85188 0.544882 1.86239 2.29721 1.71477 3.34359 0.462947 1.95145 2.28513 4.47241 2.63397 3.15326 3.91901 2.45127 0.951677 2.14548 1.34065 5.11886 1.07246 1.9493 2.9395 1.52385 2.16253 3.41725 5.35433 2.43406 3.41469 1.42388 3.0382 1.45811 1.62441 2.46594 4.06064 3.33226 2.70554 1.09906 1.43906 1.25524 0 2.78441 2.24487 6.12266 1.64121 4.2978 2.07215 2.72962 1.7052 2.00911 3.75463 2.24623 6.04445 5.06273 3.00919 2.59302 2.26807 3.48901 3.66298 1.91352 3.79304 5.18991 1.74534 2.88442 1.60746 7.35193 2.06002 0.940857 1.85511 4.12045 6.58177 4.49456 4.10784 3.93412 4.10656 2.98152 3.36622 2.16294 4.74142 7.44772 3.85408 0 2.06708 2.10782 3.23302 1.65224 3.31116 5.5201 1.1256 3.58984 2.2212 1.03207 4.9219 3.12017 2.17187 1.77808 3.22738 2.36548 2.69102 3.37004 2.99532 2.34648 2.93546 0.873652 3.34424 7.01932 1.91134 0.689162 1.04405 0.715946 1.46101 3.13229 3.85295 2.41219 4.28381 2.70918 5.99995 1.83644 3.08766 2.65195 5.36398 2.6654 4.08097 4.10543 1.48057 2.1562 1.08248 2.29466 4.7327 1.36455 0.58306 5.66015 2.11677 2.32616 1.53016 1.71363 2.85599 1.78678 3.50963 1.43133 2.46589 1.44184 4.13183 1.82749 2.39746 1.9807 2.18899 3.2765 2.05457 3.17663 2.25149 6.14635 4.4084 2.72007 2.43677 1.81356 1.08881 2.56497 1.76784 2.65311 4.07759 1.93092 4.68745 2.22922 2.15714 0.947634 2.3135 4.45493 4.73556 1.98432 1.49764 3.30458 3.47004 2.55967 2.3271 1.9901 2.63965 2.69642 2.50786 4.04005 1.44724 1.57206 2.97414 2.35697 1.78489 1.57869 5.27181 2.24017 4.23663 3.83781 3.26274 2.46871 2.38422 3.20213 0.79386 5.35906 4.4847 2.8099 1.32688 1.53337 4.67881 2.04035 2.28542 1.76162 2.93716 2.10311 3.66357 0 3.19511 1.99448 2.74055 3.39004 4.49477 3.53873 2.51352 1.76106 2.39405 1.48473 2.81959 5.08877 3.22657 5.53525 2.99461 2.34499 2.37897 2.16737 3.65791 3.49002 4.81764 2.8072 1.17728 3.25716 4.26949 1.65194 0.889088 3.0466 1.39108 1.03003 1.16075 5.13172 3.61686 5.71574 1.69182 3.18606 2.01666 4.0866 3.66355 3.86921 3.03105 1.92125 2.03699 1.84753 1.42442 3.86643 1.71207 2.91811 1.60677 2.81358 2.26227 3.59832 2.06461 1.69155 2.96225 4.135 2.49169 3.05979 1.73996 1.76435 2.55534 4.66367 4.45871 2.85819 1.36062 1.67521 2.09846 1.19113 1.08164 1.27377 4.25946 3.52169 3.83392 3.76369 1.97294 1.4504 2.35068 1.73533 0 2.41973 1.43488 3.10671 1.75895 2.8681 2.42998 2.81989 2.94079 3.61018 3.49035 2.62538 1.81267 3.24712 2.24071 2.0159 1.37211 1.61306 1.80797 6.32796 1.71926 3.11093 6.18315 1.68249 3.75052 1.86463 1.69994 3.28918 2.20401 2.8021 3.52051 4.46009 2.77428 2.7139 1.90684 1.69515 3.37159 3.04151 3.03161 1.17567 3.37786 2.29836 2.88783 1.6363 3.83693 3.31585 2.36194 2.46456 2.39381 3.27726 3.23619 2.47287 2.67182 3.80496 2.98141 2.58025 2.03359 2.33643 3.25352 1.70017 1.83015 5.30478 1.20456 2.27954 1.74328 2.86789 4.06989 1.4172 2.35348 4.04822 3.25382 4.3153 3.77277 3.42259 1.81867 6.38353 3.46561 0.898438 2.12151 3.41528 1.88561 2.10696 5.12864 1.77475 3.41935 3.74291 3.60639 5.02557 1.48678 1.07057 2.35569 3.36671 1.60252 3.52211 2.91531 2.33124 3.71871 1.91878 1.44023 2.73613 1.5379 2.7315 1.7516 4.46263 2.84063 2.1768 3.47307 2.1385 1.96101 2.5753 1.38845 2.10018 2.05717 3.20441 1.31219 2.4693 4.79441 1.30423 4.82912 3.76128 1.6359 2.4962 1.52444 2.46641 2.87795 3.35897 3.0363 4.19953 2.43826 3.11823 1.19428 3.11679 2.46988 1.96437 1.66903 3.72482 2.62406 2.62685 4.18216 4.95731 0.560609 3.03507 3.89449 2.1242 2.84368 4.02958 2.41526 2.86715 4.29949 3.66109 1.96905 1.90898 4.17306 1.07633 2.75814 3.09936 3.88161 2.53358 4.25187 1.79993 5.88039 3.30815 3.09639 3.45497 1.95909 3.87208 2.01954 2.49863 1.01001 1.85223 3.47029 2.0776 2.26903 3.21911 2.83297 2.73391 1.81683 2.08152 1.10436 3.11804 1.16035 2.70606 2.20694 1.37189 2.88498 3.23202 4.08418 1.79473 2.26966 2.81873 3.7167 3.11789 2.99363 3.77771 1.35473 0.79396 4.92145 3.46 2.06338 3.30307 2.41058 1.97357 2.19979 2.25675 2.97014 3.50573 0.930799 2.21497 2.40417 3.49064 2.95531 4.167 2.78451 1.59507 2.97739 1.59652 3.26173 1.69578 0.573809 2.56338 4.35926 0 3.41429 3.14901 2.02221 1.93178 2.08667 5.3559 2.25544 3.43795 4.23665 2.6412 4.53278 1.26399 4.62159 1.91849 2.99393 4.44693 2.71394 3.32737 2.10904 2.53435 2.49725 7.42863 1.98178 4.63103 5.70707 5.38805 3.22764 1.25312 1.89504 2.94691 0.903646 2.75324 3.75143 2.40408 2.80291 1.8334 4.28402 1.53479 4.32188 1.65106 2.25675 2.35441 6.66302 2.84648 1.80128 4.40933 5.19436 3.18678 3.50215 2.26143 3.99292 3.39724 1.75365 1.11036 1.27151 2.51229 2.36983 2.87247 1.28139 1.81351 1.76754 2.69036 3.85317 2.69864 3.71884 1.20258 3.81895 1.744 2.11396 4.82839 1.64278 0.764554 0.993248 2.60964 3.00279 1.4165 2.34624 3.11883 2.73847 4.05971 2.22024 3.79637 2.76945 1.83452 2.98704 2.51958 6.41642 2.08821 1.28802 4.45409 2.04704 1.71036 3.95503 2.62313 2.25099 3.43796 1.50021 3.49428 2.36101 3.64431 1.31104 2.07251 3.75574 3.37366 1.81389 1.04781 3.76365 2.5944 2.40603 2.77792 1.5661 5.16682 4.16723 0 3.33287 1.61877 4.57671 2.98789 2.20419 3.52248 2.29807 4.39486 2.94199 2.32087 2.05746 2.52181 2.09674 2.97522 5.36372 3.0607 4.56192 1.42038 1.92624 3.71994 3.39648 3.91364 1.7162 2.40911 4.43643 2.5288 3.2291 1.50499 5.48123 1.39419 1.8418 3.70154 4.93033 2.21601 1.93897 1.13977 4.39536 1.03235 1.3544 1.16947 2.59826 1.81077 2.28752 2.24357 2.59182 2.00164 3.97361 2.73941 1.44679 1.28278 2.73256 3.27865 2.37965 ]
@@@ Frame-accuracy per-class: [ 73.2116 52.459 47.0588 37.7358 68.1319 15.3846 24.4898 68.5714 39.7163 52.1542 31.1111 32.8767 72.4638 30.3797 0 26.1682 39.0805 24.8963 22.2222 82.9268 66.6667 46.6165 12.987 42.2535 19.4286 55.7823 51.4286 0 50.5747 51.4286 11.5607 23.8806 30.7692 0 96.9697 90.0662 40.3756 66.6667 0 78.8898 60.344 22.2222 40.9449 64.9446 62.2222 26.2295 34.7826 0 17.9775 73.5632 0 8.88889 24.0602 21.8182 55.4217 38.835 38.1395 80.7339 31.8739 56.7901 16.4948 20.4082 78.1199 59.7753 0 39.4366 48.4848 23.8806 80.8853 51.6129 49.5413 54.5455 60.9319 34.3558 62.069 22.2222 35.1648 90.1408 46.3158 67.9245 51.4286 45.1128 0 55.1724 15.7303 43.1373 0 30.303 71.7489 0 19.0476 75.969 37.2414 34.7826 45.1128 7.27273 73.8462 14.9254 44.586 87.0588 6.45161 66.6667 0 38.3764 80 23.1884 57.1429 23.2558 32 65.1163 44.4444 0 10.6952 71.9424 21.2766 0 40.5797 0 27.6923 26.6667 27.451 31.8841 0 15.3846 84.2105 48.2759 0 37.037 66.6667 49.0566 73.4463 35.2941 53.3333 25.2874 23.8806 52.8 48 87.3096 36.9231 47.0588 8.88889 62.4204 52.1739 43.2 28.5714 81.4815 58.7571 5.33333 8.79121 68.8525 70.6383 47.8873 39.6694 36.3636 50.6667 91.8919 60.6061 38.2979 60.4651 28.4987 37.2881 40.8163 19.0476 0 47.619 26.8657 26.087 42.4242 78.0952 13.4831 54.2636 23.5294 17.7778 54.902 0 73.6842 65.5738 43.4783 23.5294 30.9859 14.8148 32 0 47.0588 52.3077 30.7692 42.1053 19.5122 12.3077 8.24742 70.7692 30.5882 56.6372 38.3562 59.1549 60.241 40 12.1212 54.5455 57.6271 11.2676 19.0476 80 0 10.5263 57.1429 14.5455 73.6196 46.5116 18.1818 35.8974 33.1034 84.7458 0 27.957 60.9929 24.8175 66.6667 26.4151 47.7612 45.5446 34.6667 30.7692 73.6842 0 7.54717 12.766 0 68.9655 26.4591 47.619 42.7184 31.9328 39.0244 14.6341 35.2941 53.0612 18.1818 17.1429 55.3846 0 24.2424 0 68.3544 14.876 0 54.5455 0 20.5128 14.6341 0 26.6667 15.2381 75.5556 32 70.9677 64.6154 48 22.7848 93.1217 63.4483 14.8148 38.6555 57.1429 58.9928 95.7746 11.7647 8.69565 88.8889 81.0811 34.188 71.6981 24.4898 18.41 18.1818 0 84.2105 0 44.4444 42.2535 48.6486 39.5062 34.2857 19.1617 66.6667 34.1463 78.0488 58.4615 0 46.1538 8.95522 34.4498 57.732 80.9917 5.03145 33.6842 39.0244 0 0 14.1414 37.1681 26.6667 39.0244 20.2899 83.7093 77.3333 0 0 18.6047 13.3333 19.3548 25.4545 55.6962 30.7692 9.52381 26.087 80 70.5882 69.4737 0 0 41.3793 0 64.4628 0 29.8851 26.4151 0 11.7647 9.52381 34.0426 22.2222 19.3548 27.1186 78.2609 37.4532 26.087 39.4366 26.6667 17.4757 18.6047 53.913 13.7931 42.9224 19.3548 30.137 23.7288 0 41.8605 39.0244 37.8378 41.0256 68.9956 57.6577 47.482 50.7042 32.5581 18.6047 18.6047 44.9438 50.6329 18.3908 58.0645 23.2558 85.3333 0 62.3656 58.156 40 19.6078 17.8218 28.866 63.0542 50.6667 76.9231 81.6 72.4638 61.6822 40.1084 38.6555 50.7463 24.4898 88.4211 84.8485 42.2535 63.1579 44.0252 0 41.7582 16 58.5366 30.303 21.9178 23.6559 0 72.381 21.0526 0 32 53.9683 0 50.8475 59.2593 95.3271 7.40741 87.8049 84.2105 38.0952 33.9181 10.8949 11.0599 27.907 66.6667 11.236 29.5082 58.4615 48 79.5699 0 52.3077 74.0331 47.619 78.0488 89.2308 9.52381 69.7674 65.6842 63.7363 0 69.7674 28.7703 38.3838 31.3725 39.3443 25.3968 67.7966 33.5664 51.8519 22.6087 57.732 50.7614 49.4624 36.3636 28.9157 0 77.686 0 0 67.5325 22.6804 55.4455 0 14.1732 41.958 0 0 94.1176 33.6842 34.7826 48.4848 75.2941 42.6966 22.2222 25.1748 10.9091 6.45161 38.3838 80 44.4444 26.087 88.5246 60.7407 65.0602 81.4159 12.9032 70.1754 73.4694 39.4366 0 13.7931 0 40.5797 26.6667 11.7647 18.1818 0 31.5789 21.0526 73.0435 41.5842 32 14.8148 0 7.01754 71.6049 55.3248 36.6197 0 5.12821 35.8974 48.6486 65.4545 16 0 85.7143 31.405 32.5581 61.8182 0 53.7313 2.0202 40.2235 46.1538 64.9351 35.2941 32.4324 19.4175 30.3797 3.77358 17.3913 0 40 34.0426 0 27.027 21.3592 19.2893 16.9014 8 66.6667 8.69565 0 10.6195 12.0658 80 45.5696 41.2698 48.6486 46.5116 25.8065 57.1429 0 6.45161 86.2745 0 60.4651 71.028 18.4874 65.3061 0 32.2581 44.4444 56.5657 73.6842 0 63.1579 47.0588 28.5714 47.0588 40.678 0 0 23.5294 32.4324 62.1359 34.2857 65.9794 59.5745 0 1.65289 0 44.898 39.4366 62.3656 18.1818 1.90476 16 64.8276 57.1429 0 11.7647 25.8065 18.1818 0 48.2759 56.8807 20.9524 15.3846 34.9515 66.6667 34.4828 9.30233 58.6667 46.9136 26.8657 18.1818 38.7097 32.4324 5.12821 55.2381 36.9231 3.50877 16 37.2881 62.6506 32.381 19.0476 0 84.8168 48.6486 32.2581 56.0748 17.1429 94.1176 18.1818 27.027 6.34921 0 20.6897 3.63636 40 70.5882 51.4286 53.3333 36.3636 59.6491 41.3793 24.8062 66.6667 32.2581 23.5294 2.06186 54.321 16.9492 46.1538 6.45161 28.5714 18.8679 31.1688 12.973 7.40741 33.0275 64.7423 0 55.6213 0 31.1111 46.8468 26.4901 48 0 23.8683 25.641 48.6957 53.6585 0 37.8378 2.33918 0 22.8571 14.9254 22.4299 23.5294 35.5828 32.3353 3.77358 0 63.0137 25.2252 49.6124 0 39.0977 80.4734 48.4211 0 0 10.929 2.29885 10.2564 0 29.7872 13.3333 45.7143 15.3846 0 8.16327 0 43.6975 44.0678 11.3208 64.5161 39.0244 1.79372 66.6667 9.90099 0 76.9231 0 0 45.7143 45.1613 10.5263 41.5584 17.7778 3.63636 24.5614 39.1753 9.52381 84.3373 0 0 26.2009 82.4645 80 82.6667 64.3678 22.2222 7.19424 5.71429 5.71429 32.4607 0 65.9341 0 0 0 40.8163 27.4809 0 40 50.8475 76.1905 42.5197 16.8421 68.5714 98.5507 0 29.9065 40 69.3333 62.8571 29.2683 0 0 54.7945 44.586 48.7805 0 48.2759 38.2979 50.5263 30.7692 30.7692 53.3333 0 43.4783 5.58659 0 0 10.8108 58.5366 83.959 27.451 58.8235 15.6863 0 57.6271 21.0526 62.2222 22.6415 66.6667 28.5714 2.72109 0 25.731 54.1176 21.3333 11.2676 32.7044 34.5679 45.5696 34.7826 33.8462 17.0213 10.8108 61.244 51.9084 21.0526 32.967 56.4103 55.814 0 22.6804 9.41176 4.65116 22.2222 42.1053 27.451 25.4545 84.3537 4.25532 0 0 68.6327 51.8828 0 34.2541 25.4545 63.8298 29.7872 57.1429 4.12371 0 7.01754 42.1053 30.7692 21.978 0 12.2449 0 47.0588 15.3846 36.3636 25.5319 8.16327 15.444 7.92079 34.7826 13.9535 50.3597 33.8462 0 18.8679 12.1212 6.89655 61.6352 9.52381 0 39.4366 73.0539 12.2449 51.1628 73.1707 70.1754 10.8949 7.05882 24.0602 51.2821 0 47.0588 0 22.6415 9.91736 15.0943 38.3562 39.3443 46.6926 44.6281 0 64.5161 40.8163 52.1008 18.9474 40 25.641 48.8189 51.7647 13.3333 0 0 13.1148 42.4242 53.4351 8 0 19.5122 30.7692 57.3427 41.2371 40 74.2857 75.6303 56.1404 12.2449 0 43.9024 0 22.6415 73.4694 25.8065 59.1549 0 51.4286 70.4225 14.8148 54.2373 6.89655 40 20.202 0 7.40741 0 22.8571 47.619 25.641 41.5584 19.3548 61.6822 64.8084 60.8696 0 48 4.25532 0 52.1739 20.339 53.3333 30.1075 14.0541 52.459 20.6186 31.746 0 39.3443 12.1212 41.9753 48.9796 26.9663 33.0935 29.8851 87.1795 10.687 57.478 3.38983 90.9091 24.5614 28.9963 31.2925 24.3902 52.1739 25.4545 20.8092 37.2881 44.4444 4.65116 48 0 44.898 16 17.1779 53.8226 32 0 76.9231 56.1404 58.2278 54.9451 12.6984 68.0851 30.137 15.2672 34.7826 12.6582 12.3348 9.83607 43.4783 0 1.59363 72.5275 38.0952 8.69565 56 24.3902 10.1695 24.2424 20.5128 14.1176 11.2821 7.54717 41.7582 78.481 51.6129 30.137 59.8131 10.8108 20.1681 38.0952 26.8657 40.678 62.8571 6.34921 74.7253 23.5294 39.5604 8.79121 8.69565 53.0612 14.9254 56.9106 26.4151 28.0702 57.6271 46.0094 32.7869 14.9254 59.4595 16.4948 8.84956 67.052 3.24324 8.51064 61.3333 42.5532 34.4828 0 20.6897 16.4384 25.3968 10.1083 39.6694 24.8951 70.5882 16.2162 40.7767 50.6329 44.0678 0 37.3832 6.45161 13.7931 0 96.6292 23.3766 19.2771 50.6024 23.5294 5.40541 24.6154 18.1818 14.6341 23.2558 51.4286 23.5294 14.5078 54.5455 46.8085 45.3674 8.79121 29.5082 0 46.1538 0 43.299 7.84314 15.3846 64.4068 0 44.4444 17.0213 85.7143 49.4118 0 54.9118 38.0952 13.7931 16.9492 31.3725 58.2278 49.6124 58.2278 26.087 71.6981 28.0702 53.1646 58.0311 14.8148 9.34579 0 52.1739 31.5789 26.087 23.2558 14.8148 21.0526 16.3265 65.3465 76.9231 0 0 26.6667 0 12.9032 37.037 49.1803 32.7869 40.7932 13.3333 73.3728 33.5404 30.5221 0 21.4765 12.3077 49.0798 66.6667 34.0426 54.902 20.438 62.8571 88.6228 32.7273 9.7561 0 23.8806 8.84956 48.2759 29.7872 22.9508 0 35.4717 35.2423 0 44.7552 0 64.5161 6.06061 18.1818 28.5714 3.25203 21.5385 11.3475 36.6197 33.7079 24.6575 1.21212 8 0 7.84314 12.3894 0 69.3878 49.848 30.8123 85.7143 23.5294 4.08163 30.7692 8.69565 38.5542 8.88889 64.8889 0 58.1818 32 30.3797 0 30.1887 52.1212 0 0 27.8481 23.5294 38.7097 14.876 14.6341 61.2245 71.5596 50.7042 48.9796 33.0097 35.0877 69.3878 44.4444 54.2936 41.9672 15.1899 23.5294 6.89655 84.7059 18.6235 56.7376 47.1121 0 53.5211 84.9315 66.275 17.7778 17.9104 66.6667 36.1446 6.15385 16.4948 24.0964 32.1981 6.06061 32.4324 63.3484 19.469 38.3562 8.7619 46.6165 77.6119 10.5263 41.8605 19.3548 0 30.7692 51.2821 2.06186 56.8421 0 26.6667 6.89655 53.3333 39.4366 13.278 19.2893 63.4146 72.2689 10.929 11.4286 32.2581 40 45.1613 0 6.06061 0 14.5455 57.1429 22.2222 10.5263 47.2441 10.0719 30.303 0 9.23077 40.5063 49.9414 47.3118 37.8378 33.8983 3.50877 25.2632 0 65.8824 35.2941 0 5.71429 10.5263 50.6787 38.2979 12.1212 29.0909 16 52.0548 0 77.9221 60.4651 8.69565 0 10.6667 54.321 66.6667 0 83.7209 36.5591 67.8899 16.9492 42.6966 30.303 6.06061 47.3118 51.8519 0 12.1212 66.6667 58.6667 28.169 32 34.7826 ]

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:347) Done 295 files, 0 with no tgt_mats, 0 with other errors. [CROSS-VALIDATION, 0.0211143 min, fps62526.4]
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:353) AvgLoss: 2.07524 (Xent), [AvgXent: 2.07524, AvgTargetEnt: 0]
progress: []
FRAME_ACCURACY >> 46.1483% <<

WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Close():kaldi-io.cc:515) Pipe ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- | had nonzero return status 36096
