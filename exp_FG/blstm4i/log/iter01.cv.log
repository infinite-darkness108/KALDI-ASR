speech-HP-Z2-Tower-G9
nnet-train-multistream-perutt --cross-validate=true --randomize=false --verbose=0 --num-streams=10 --max-frames=15000 --feature-transform=exp_FG/blstm4i/final.feature_transform 'ark:copy-feats scp:exp_FG/blstm4i/cv.scp ark:- | apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_cv10/utt2spk scp:data-fbank/train_cv10/cmvn.scp ark:- ark:- | add-deltas --delta-order=2 ark:- ark:- |' 'ark:ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- |' exp_FG/blstm4i/nnet/nnet_iter01 
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:243) Not in compute-exclusive mode.  Suggestion: use 'nvidia-smi -c 3' to set compute exclusive mode
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:438) Selecting from 1 GPUs
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:453) cudaSetDevice(0): NVIDIA RTX A2000 12GB	free:11515M, used:515M, total:12031M, free/total:0.957153
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:501) Device: 0, mem_ratio: 0.957153
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:382) Trying to select device: 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:511) Success selecting device 0 free mem ratio: 0.957153
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:FinalizeActiveGpu():cu-device.cc:338) The active GPU is [0]: NVIDIA RTX A2000 12GB	free:11001M, used:1029M, total:12031M, free/total:0.91443 version 8.6
copy-feats scp:exp_FG/blstm4i/cv.scp ark:- 
apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_cv10/utt2spk scp:data-fbank/train_cv10/cmvn.scp ark:- ark:- 
add-deltas --delta-order=2 ark:- ark:- 
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:144) CROSS-VALIDATION STARTED
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Read():nnet/nnet-matrix-buffer.h:191) Read() started... Buffer size in MB: 0, max 3072, having 0 utterances.
ali-to-post ark:- ark:- 
ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl 'ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |' ark:- 
LOG (copy-feats[5.5.1074~1-71f3]:main():copy-feats.cc:143) Copied 296 feature matrices.
LOG (apply-cmvn[5.5.1074~1-71f3]:main():apply-cmvn.cc:159) Applied cepstral mean and variance normalization to 296 utterances, errors on 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:303) ### After 0 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:304) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.301148, max 0.331453, mean 0.00338207, stddev 0.0703498, skewness 0.0195238, kurtosis -0.27494 ) 
  f_w_gifo_r_   ( min -0.425459, max 0.408335, mean -0.000551618, stddev 0.0757716, skewness 0.00175504, kurtosis -0.0180304 ) 
  f_bias_   ( min -0.35375, max 1.26852, mean 0.220801, stddev 0.450276, skewness 1.07679, kurtosis -0.654243 ) 
  f_peephole_i_c_   ( min -0.361486, max 0.370627, mean -0.00531675, stddev 0.111826, skewness -0.0606021, kurtosis 0.538335 ) 
  f_peephole_f_c_   ( min -0.391899, max 0.49922, mean 0.00302868, stddev 0.121026, skewness 0.0268733, kurtosis 1.6242 ) 
  f_peephole_o_c_   ( min -0.446902, max 0.406077, mean -0.0172202, stddev 0.149183, skewness 0.245011, kurtosis -0.131909 ) 
  f_w_r_m_   ( min -0.429293, max 0.477106, mean 0.000492045, stddev 0.0937307, skewness -0.0028584, kurtosis -0.0415628 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -0.352832, max 0.322641, mean 0.00543303, stddev 0.0692203, skewness -0.0169963, kurtosis -0.308752 ) 
  b_w_gifo_r_   ( min -0.334211, max 0.300666, mean -0.000337552, stddev 0.0654048, skewness 0.000284943, kurtosis -0.550009 ) 
  b_bias_   ( min -0.250881, max 1.15985, mean 0.215062, stddev 0.443918, skewness 1.07694, kurtosis -0.675578 ) 
  b_peephole_i_c_   ( min -0.377552, max 0.250341, mean 0.00624115, stddev 0.0830032, skewness -0.139292, kurtosis 0.903416 ) 
  b_peephole_f_c_   ( min -0.495802, max 0.389639, mean 0.00757412, stddev 0.111162, skewness 0.30285, kurtosis 3.20524 ) 
  b_peephole_o_c_   ( min -0.374731, max 0.314081, mean -0.0146834, stddev 0.133316, skewness -0.0227591, kurtosis -0.450465 ) 
  b_w_r_m_   ( min -0.331195, max 0.332871, mean -2.75225e-05, stddev 0.0778926, skewness -0.00244976, kurtosis -0.230121 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.679417, max 0.603095, mean -0.00015592, stddev 0.100804, skewness 0.00461174, kurtosis 0.0489564 ) , lr-coef 1, max-norm 0
  bias ( min -0.30808, max 1.84159, mean 3.72529e-10, stddev 0.0580086, skewness 25.1988, kurtosis 792.427 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:305) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -11.004, max 12.2048, mean 0.00682255, stddev 0.969149, skewness 0.287893, kurtosis 3.83541 ) 
[1] output of <BlstmProjected> ( min -3.86348, max 4.35351, mean -0.00578224, stddev 0.704958, skewness 0.0676792, kurtosis 1.02862 ) 
[2] output of <Tanh> ( min -0.999119, max 0.999669, mean -0.00589188, stddev 0.496439, skewness 0.0151296, kurtosis -0.814405 ) 
[3] output of <AffineTransform> ( min -22.4529, max 19.6283, mean 0.0236951, stddev 1.75943, skewness 0.42045, kurtosis 2.55215 ) 
[4] output of <Softmax> ( min 1.87142e-14, max 0.995976, mean 0.000780404, stddev 0.0113356, skewness 46.3686, kurtosis 2607.15 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:334) ### After 79212 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:335) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.301148, max 0.331453, mean 0.00338207, stddev 0.0703498, skewness 0.0195238, kurtosis -0.27494 ) 
  f_w_gifo_r_   ( min -0.425459, max 0.408335, mean -0.000551618, stddev 0.0757716, skewness 0.00175504, kurtosis -0.0180304 ) 
  f_bias_   ( min -0.35375, max 1.26852, mean 0.220801, stddev 0.450276, skewness 1.07679, kurtosis -0.654243 ) 
  f_peephole_i_c_   ( min -0.361486, max 0.370627, mean -0.00531675, stddev 0.111826, skewness -0.0606021, kurtosis 0.538335 ) 
  f_peephole_f_c_   ( min -0.391899, max 0.49922, mean 0.00302868, stddev 0.121026, skewness 0.0268733, kurtosis 1.6242 ) 
  f_peephole_o_c_   ( min -0.446902, max 0.406077, mean -0.0172202, stddev 0.149183, skewness 0.245011, kurtosis -0.131909 ) 
  f_w_r_m_   ( min -0.429293, max 0.477106, mean 0.000492045, stddev 0.0937307, skewness -0.0028584, kurtosis -0.0415628 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -0.352832, max 0.322641, mean 0.00543303, stddev 0.0692203, skewness -0.0169963, kurtosis -0.308752 ) 
  b_w_gifo_r_   ( min -0.334211, max 0.300666, mean -0.000337552, stddev 0.0654048, skewness 0.000284943, kurtosis -0.550009 ) 
  b_bias_   ( min -0.250881, max 1.15985, mean 0.215062, stddev 0.443918, skewness 1.07694, kurtosis -0.675578 ) 
  b_peephole_i_c_   ( min -0.377552, max 0.250341, mean 0.00624115, stddev 0.0830032, skewness -0.139292, kurtosis 0.903416 ) 
  b_peephole_f_c_   ( min -0.495802, max 0.389639, mean 0.00757412, stddev 0.111162, skewness 0.30285, kurtosis 3.20524 ) 
  b_peephole_o_c_   ( min -0.374731, max 0.314081, mean -0.0146834, stddev 0.133316, skewness -0.0227591, kurtosis -0.450465 ) 
  b_w_r_m_   ( min -0.331195, max 0.332871, mean -2.75225e-05, stddev 0.0778926, skewness -0.00244976, kurtosis -0.230121 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.679417, max 0.603095, mean -0.00015592, stddev 0.100804, skewness 0.00461174, kurtosis 0.0489564 ) , lr-coef 1, max-norm 0
  bias ( min -0.30808, max 1.84159, mean 3.72529e-10, stddev 0.0580086, skewness 25.1988, kurtosis 792.427 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:336) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -7.66434, max 7.26584, mean -0.0147177, stddev 0.961893, skewness 0.577064, kurtosis 2.62679 ) 
[1] output of <BlstmProjected> ( min -3.7906, max 4.1617, mean -0.00302669, stddev 0.707901, skewness 0.0535107, kurtosis 1.3705 ) 
[2] output of <Tanh> ( min -0.998981, max 0.999515, mean -0.00324661, stddev 0.489878, skewness 0.00503568, kurtosis -0.712807 ) 
[3] output of <AffineTransform> ( min -15.422, max 18.6284, mean 0.0144158, stddev 1.75225, skewness 0.609794, kurtosis 3.70577 ) 
[4] output of <Softmax> ( min 3.26945e-11, max 0.992342, mean 0.000780965, stddev 0.0138296, skewness 40.4651, kurtosis 1915.37 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:346) PER-CLASS PERFORMANCE:
@@@ Frames per-class: [ 1670 30 8 79 45 6 24 17 70 220 22 36 34 39 10 53 43 120 22 20 34 66 38 35 87 73 52 35 43 17 86 33 19 32 16 75 106 10 5 1873 21251 22 63 135 22 30 80 98 44 43 9 67 66 27 41 51 107 54 285 40 48 24 308 222 21 35 16 33 248 15 54 49 139 81 14 4 45 35 47 26 17 66 4 14 44 25 18 49 111 11 94 64 72 34 66 27 32 33 78 42 15 34 9 135 37 103 31 64 37 21 67 0 93 69 23 2 34 10 32 37 25 34 18 19 28 43 46 13 4 26 88 25 22 43 33 62 12 98 32 25 22 78 11 62 24 13 88 37 45 30 117 35 60 38 37 129 16 23 64 982 29 24 10 20 31 33 34 16 52 44 64 25 22 25 3 9 30 11 42 35 13 12 11 8 32 32 9 20 32 48 32 42 56 36 35 41 27 16 5 29 35 31 12 7 9 45 27 81 21 16 19 72 29 0 46 70 68 37 26 33 50 37 32 9 5 26 23 15 14 128 52 51 59 20 20 8 73 5 17 32 22 16 4 39 60 11 16 7 19 20 8 7 52 22 37 15 32 12 39 94 72 13 59 17 69 35 25 11 13 18 58 26 24 119 16 14 9 5 13 35 18 40 17 83 10 20 20 32 21 19 33 104 48 60 79 47 20 14 27 49 56 7 20 34 199 37 4 3 21 22 15 27 39 6 10 11 22 25 47 21 6 14 6 60 3 43 26 14 8 10 70 4 15 29 11 133 11 35 22 51 21 57 14 109 15 36 29 5 21 20 18 19 114 55 69 35 21 21 64 44 39 43 15 21 37 31 46 70 32 25 50 48 101 37 32 62 34 53 184 59 33 24 47 16 35 9 79 14 45 12 20 16 36 46 8 52 28 13 12 31 17 29 40 53 13 20 9 73 85 128 108 21 7 44 30 32 12 46 15 32 90 31 20 32 52 21 237 45 11 21 215 49 25 30 31 29 71 13 57 48 98 46 27 41 19 60 21 4 38 48 50 5 63 71 0 0 25 47 11 16 42 44 49 71 27 15 49 22 22 11 30 67 41 56 15 28 24 35 7 14 15 34 7 8 38 0 9 28 57 50 12 13 39 28 40 746 35 13 19 19 18 27 12 16 52 60 21 82 1 33 49 89 6 38 42 18 51 39 26 11 19 7 23 28 18 51 98 35 37 10 34 11 56 273 7 39 31 18 21 15 24 22 15 25 6 21 53 59 24 25 15 4 49 28 14 9 8 3 8 29 34 0 8 18 51 52 48 23 3 60 16 24 35 46 16 52 37 72 3 38 59 15 5 40 14 54 52 32 51 4 14 21 187 40 33 16 15 18 19 52 32 28 37 29 41 52 10 6 95 18 46 53 17 8 5 18 31 15 14 27 7 8 17 7 93 28 14 64 22 15 8 48 40 29 32 15 10 26 38 92 13 54 242 3 84 0 22 55 226 137 6 121 19 57 20 16 55 85 4 17 33 53 42 81 83 26 30 36 55 64 9 66 84 47 14 13 91 43 19 13 23 37 17 19 27 24 0 59 88 26 15 20 111 16 50 7 32 22 5 17 15 28 38 22 27 28 48 52 41 28 16 114 105 37 112 43 4 69 17 87 95 11 45 15 22 24 24 65 3 27 29 73 63 47 17 34 18 53 17 37 17 20 6 14 36 78 20 8 14 23 47 6 6 22 3 11 89 3 5 18 20 146 25 8 25 44 29 28 22 26 34 24 73 34 85 42 37 35 79 40 39 34 32 23 18 104 65 9 45 19 21 17 48 42 21 40 28 25 27 73 23 34 2 186 119 39 90 27 23 23 38 48 0 28 66 71 45 13 24 7 25 19 16 23 24 129 50 34 21 69 32 14 26 16 14 79 10 15 35 83 24 21 61 28 128 42 66 19 12 25 28 26 60 26 36 30 128 60 8 15 24 59 47 2 19 63 42 22 4 29 30 49 65 12 8 20 32 71 48 32 17 59 28 24 21 20 22 26 24 15 35 0 17 35 13 29 14 47 49 7 13 6 17 31 19 38 15 53 143 34 68 37 23 6 34 29 7 46 92 30 48 31 3 30 16 40 24 44 69 43 19 65 170 29 5 28 134 73 20 11 82 86 29 13 21 12 10 24 12 81 163 12 193 6 28 39 45 31 23 36 65 11 39 113 30 11 28 125 45 52 34 87 20 29 16 97 42 97 26 45 39 15 109 53 18 59 52 33 29 52 31 45 25 45 136 11 24 33 61 26 28 29 106 30 33 18 48 56 86 92 23 37 23 14 2 14 36 346 138 60 357 25 129 51 39 29 11 53 15 14 13 44 38 41 41 8 18 32 16 20 21 17 8 96 16 23 156 45 91 21 6 43 48 25 45 29 8 22 23 31 42 3 198 10 14 29 25 39 64 39 11 26 28 39 96 13 53 5 34 9 11 21 67 28 24 50 19 10 11 22 23 15 13 30 30 176 37 84 80 124 21 74 32 81 43 23 25 68 52 83 27 20 0 33 56 14 23 30 22 132 113 9 71 18 15 16 5 38 61 32 70 35 44 36 82 12 19 25 56 12 24 164 178 24 42 24 19 11 41 67 112 11 27 62 39 5 26 82 58 18 39 25 15 60 20 122 54 35 24 51 28 24 13 180 152 39 59 14 42 123 70 441 14 35 36 425 22 33 13 41 32 48 41 161 16 18 110 56 36 262 66 33 47 21 15 29 84 58 48 47 2 22 14 22 35 120 98 20 59 274 17 15 22 15 59 16 0 27 31 40 47 63 69 16 68 32 39 426 46 55 29 28 47 14 42 42 14 17 28 110 23 16 27 12 36 25 38 21 57 45 37 40 25 7 21 46 54 29 44 49 16 46 13 48 16 103 37 35 12 34 ]
@@@ Loss per-class: [ 0.673628 5.0071 5.09086 3.99438 2.03964 7.21781 5.35361 4.53135 2.46209 4.32256 6.05904 5.36313 4.56276 4.17868 7.43641 5.84602 3.35639 5.0575 6.06203 2.86424 3.13947 2.7789 4.34398 2.91508 2.36855 4.0929 2.79596 7.15068 3.86377 4.61939 5.72013 4.59229 3.71021 7.43646 2.34589 1.80682 3.84706 6.3494 6.56857 0.9505 1.56162 5.01625 5.37987 3.62795 2.74421 3.73217 2.15533 6.23875 4.07366 4.43419 6.64838 2.88272 4.53688 5.81975 5.77278 3.81388 5.1276 3.24664 1.75609 2.49532 6.47919 3.78321 4.50298 2.69581 6.69963 2.65906 3.73535 4.99206 2.94955 3.37771 2.32452 3.60841 3.2275 3.48718 4.1532 8.20101 3.23373 1.72169 2.60491 4.46454 6.80134 4.25868 5.222 3.96243 4.12405 5.15215 6.88585 5.04179 3.83391 9.21199 5.44033 3.66228 2.27933 7.07374 5.20042 5.97125 3.57443 4.61241 1.88978 4.48923 7.15941 3.89001 7.6309 3.18443 2.00141 7.13463 4.53866 4.42795 4.61486 4.39403 4.53919 0 5.70644 2.05072 4.53267 6.65893 4.17453 7.14385 4.15401 4.97676 5.47511 7.64825 6.0891 5.33734 3.38748 3.02815 6.84411 6.27709 6.83351 2.92386 3.52235 4.47593 2.13957 3.28988 4.50719 4.57663 3.3061 1.65592 3.9339 5.48893 6.56632 2.67174 4.31996 2.54486 5.02151 7.30303 4.16399 6.00636 6.57059 2.82521 1.89408 3.64641 4.38551 5.08442 3.80013 1.64991 1.9089 4.51861 3.13704 3.18569 3.93727 4.00502 4.42129 6.1787 4.5331 4.25451 4.02779 5.54688 3.26484 5.46591 3.36739 4.68415 4.69385 3.10194 6.04362 4.86756 3.16423 6.28649 5.37335 5.68041 5.07965 6.13135 7.7132 5.3695 2.2015 4.34719 9.44 4.62727 6.60468 3.86686 4.05695 2.16055 2.88777 3.49038 4.94973 4.47513 2.87765 5.66067 6.54496 6.34669 4.71739 5.397 5.30465 7.38367 6.20179 3.75809 4.5098 2.42058 3.13245 5.76618 6.24001 3.37483 2.34364 0 3.39556 3.66452 4.07959 2.79007 3.16519 4.43273 2.40266 3.99558 4.55805 4.90399 7.65535 4.75759 7.94367 7.06078 4.36631 5.78245 4.11768 2.55418 3.76643 4.50548 4.95349 7.12049 3.12492 6.61696 5.08766 4.52871 8.8627 3.39698 4.21362 2.60805 5.83559 7.13503 6.16992 7.72569 4.8455 4.44556 8.35863 4.86726 5.14288 3.1262 3.73719 3.04607 3.58786 3.27647 3.68422 3.30159 2.64408 5.18399 5.19908 6.04451 4.45869 1.88495 4.57518 7.62513 2.16102 4.41832 3.1676 3.53527 6.13306 3.56409 4.45411 7.55032 2.94939 8.12592 4.67329 4.91945 3.58381 5.41913 6.51777 4.91544 2.48213 5.14009 5.42095 4.45132 9.74194 3.73327 3.70181 5.25473 4.08531 2.63482 4.32418 6.01528 4.77019 5.42458 5.76022 5.89376 3.27931 5.31564 5.44756 3.24299 2.17722 5.06 7.37281 4.06028 4.86969 5.53986 6.09928 4.47412 4.68057 6.12446 6.42989 5.83087 4.52606 3.99292 3.87663 6.89295 8.5988 6.52791 7.05981 3.13579 5.20323 5.88675 4.63239 7.3354 5.12528 6.86587 3.36741 7.60945 5.4507 4.77096 3.79743 3.49508 7.81592 4.66569 4.02229 6.16525 6.62795 2.09087 5.31495 3.09002 6.11166 5.25158 3.65634 7.20929 6.85395 6.26555 4.39178 5.04823 3.62263 3.99879 3.97937 3.38611 5.17641 8.06992 5.28293 4.55841 3.65308 4.34793 5.34415 5.14733 3.98612 5.80513 4.04748 3.57189 3.82631 6.58873 3.6408 3.82753 1.7675 5.68131 2.63522 4.0937 3.26745 3.05059 3.16639 3.52293 3.9959 4.04857 3.83664 5.90044 3.46582 5.8372 3.85722 6.61282 4.34023 6.58065 4.01486 5.93098 4.51901 4.8336 5.93775 3.53095 5.03844 7.32922 5.08106 3.51765 8.72463 4.00076 4.25159 2.82795 5.47033 3.324 3.72636 5.31511 3.81188 4.71712 6.47024 6.61845 4.38438 5.68809 4.66015 3.67194 5.48383 1.61405 5.87904 4.16837 3.41714 6.32016 1.69806 2.1632 5.20684 4.76448 2.83883 3.82065 7.21325 3.87529 4.98244 4.88888 4.81155 4.12321 4.01168 4.15586 4.91373 5.64156 4.83557 3.64095 1.95268 4.78086 4.91908 3.99578 6.81034 1.8384 7.74276 3.53289 3.75107 3.06109 4.27255 6.54397 5.76236 5.10406 0 0 2.84245 3.14654 7.20221 5.51694 2.38979 3.12632 5.52816 3.97673 5.33158 6.79037 6.78684 2.54682 3.25029 3.65741 2.97344 2.64493 5.39521 3.9449 5.0775 4.23461 3.12795 4.09369 4.80878 4.50332 7.27973 2.84209 6.51051 6.34061 6.06175 0 4.67939 4.87713 3.03122 4.22729 5.80089 6.11793 6.49608 7.56687 2.43103 2.32142 3.56401 4.2833 6.71143 5.92718 5.60947 2.43674 5.11943 5.59365 3.46463 4.2154 6.78784 1.55451 6.9602 3.1014 7.89814 4.93945 6.58363 4.45003 3.80008 4.83155 5.10862 3.44674 6.68248 6.86881 7.04427 6.52163 5.98908 6.85282 4.37683 4.17107 4.60483 5.07116 4.74983 4.14123 6.93604 6.36056 5.48666 4.7026 6.05418 4.24328 4.16847 3.0741 5.37868 5.48998 3.93519 7.77413 8.40524 4.38124 7.77631 5.22925 1.84015 3.92456 3.47515 5.62503 5.44811 6.21651 4.01782 4.2083 6.13003 3.98652 6.7804 6.07057 5.32734 4.82192 4.81924 0 4.77252 5.25236 2.6852 3.39069 2.91809 3.46284 6.12387 7.40517 7.36501 3.29218 3.81945 2.95699 6.47703 7.29871 5.1794 2.6701 5.16669 6.37555 5.97494 6.89777 7.85781 5.26427 6.80328 4.15654 7.01776 4.95695 4.24828 4.76357 5.27592 6.07946 2.88639 4.8486 6.3601 5.60214 5.06048 5.06947 5.80421 3.11923 3.68243 6.3775 4.74335 4.58717 2.98341 6.36621 6.90486 7.52823 3.28199 3.91263 4.11465 3.16471 5.62638 6.25664 7.78812 5.02727 6.71285 7.58407 6.00343 6.25399 4.33825 6.21613 5.36315 3.50947 5.27647 5.45937 5.27588 5.61323 5.5324 6.39483 5.65071 5.67798 3.41385 5.31473 2.95802 7.32093 5.70465 3.59639 5.69025 4.55151 6.14562 3.71092 1.50623 5.21358 2.4262 0 4.96273 4.17477 4.84237 2.86314 6.40163 2.27965 7.00159 4.69422 5.1948 7.43271 5.21617 6.31964 6.16744 5.52756 5.35006 2.56796 3.632 3.67664 2.13104 5.79519 8.31805 4.84434 5.24347 5.63255 9.12981 3.15782 2.48112 3.89412 6.57049 6.55225 5.13873 7.67879 6.90169 7.96178 5.17131 6.26824 3.63209 6.87165 8.29499 3.9813 0 3.78787 2.32907 4.99965 4.23953 4.41646 6.05371 3.40135 7.46647 8.01869 3.14728 6.77257 6.75986 5.91365 6.05994 5.61281 5.14129 5.82756 6.05416 4.08232 3.22982 6.42263 2.29865 5.20951 8.09285 4.76304 4.14175 3.4153 1.68239 3.43698 5.32072 4.89547 6.42667 5.27914 3.40916 8.04372 3.65499 4.57773 5.6001 8.39272 4.80006 6.71322 6.54212 1.474 4.60141 4.12033 2.37907 5.4894 5.88896 1.71571 6.51188 3.74078 3.70483 2.3472 3.63807 5.30077 5.94825 5.57381 4.25732 3.67479 3.37861 8.55531 3.04354 3.72696 3.33247 7.16071 6.32456 4.66858 6.62505 5.32986 6.25204 8.27375 6.3356 4.04789 5.79347 3.34984 6.21162 6.89332 5.46446 6.05086 5.15781 5.63637 5.50363 3.96163 3.84099 3.93685 5.53942 5.38532 3.77537 3.94138 4.64581 7.49696 6.94447 5.54957 3.87058 5.22566 4.86324 3.2357 6.69232 2.63582 4.89426 7.02643 5.32643 4.82898 2.56878 7.75671 5.43734 5.06883 5.77732 4.6953 3.46452 5.34096 4.88309 3.3498 5.68992 7.4495 6.34096 1.6253 4.59061 6.65799 1.61552 3.61539 4.97686 5.51815 3.02917 4.95188 0 5.07928 3.7619 3.64737 4.12895 7.67568 6.94684 4.96607 3.04036 6.21809 4.57544 5.22386 7.14976 4.82909 7.42015 4.79765 4.4681 5.21539 5.60707 6.66404 7.69777 5.06749 6.91273 2.68873 5.20291 5.63563 4.62178 1.7347 6.98601 5.1839 2.65427 3.45543 5.79038 4.24802 6.26608 4.53024 5.54926 4.27573 5.62818 7.27501 5.98857 4.47844 2.88152 4.2228 5.25367 4.00688 8.35972 4.3247 3.77478 3.84341 4.85114 6.0483 5.91535 3.07092 3.81064 5.03074 6.65663 5.65773 4.9737 3.6543 2.92129 5.81804 7.75156 7.46978 4.55118 3.07026 2.86956 4.17774 3.83383 1.96876 2.96291 6.34953 7.31498 6.16018 6.33765 2.17947 5.02653 4.47681 4.09098 0 4.6308 3.26841 5.20315 2.80629 7.29509 4.77353 3.706 4.17613 5.43333 3.51691 5.61953 5.77325 5.62109 4.30858 7.68178 2.62553 3.78048 2.81527 7.67466 4.50766 3.1063 9.49219 3.60601 6.21107 4.19329 4.13142 3.64007 5.55317 6.95776 4.80861 8.09903 3.77515 6.64142 5.80893 3.74582 5.54669 3.40201 5.34969 4.55069 4.02117 3.29047 6.19533 5.87051 5.33149 6.02908 4.39192 4.5262 6.69034 4.18343 4.37322 4.50371 4.88098 5.69992 5.23515 7.25932 3.27021 5.38761 3.49785 2.47193 5.41919 5.26198 5.22029 3.49332 3.13313 4.0385 7.00409 4.22757 3.79299 5.80123 6.65918 5.59538 4.36031 5.63461 4.48297 6.72912 6.46647 3.37469 3.40658 4.46745 2.03679 5.22922 8.21054 5.22664 6.42829 3.47316 4.6237 6.75145 3.71154 1.98554 5.1856 5.00441 3.35243 5.10687 4.54606 3.69933 6.40547 3.35896 6.45904 5.01722 6.09113 4.9319 3.78291 3.72079 2.44948 4.98263 4.91611 3.70071 3.98254 4.75529 3.95209 3.21063 4.43051 3.83677 2.78539 4.08966 4.50529 3.85562 5.99044 7.57852 3.82508 5.68774 3.48365 4.85227 6.58886 4.54073 3.97783 5.87476 4.09831 4.23857 3.28999 4.59306 2.39758 4.01881 3.76454 6.88566 4.3871 6.43854 8.3031 6.6349 1.80129 4.08928 6.03028 4.0214 5.36553 7.57418 4.21446 5.34046 5.95149 6.04023 5.59793 4.72859 4.59588 4.47501 6.85342 3.51854 6.0617 2.78199 5.67602 6.08802 7.20196 6.61701 5.46429 3.76347 3.31008 6.89306 4.0412 5.51202 4.08557 4.79552 4.98541 3.50319 5.42708 6.31725 3.80676 5.77446 3.19298 4.1053 3.36745 7.05044 2.80621 5.41015 4.15522 3.11542 5.1653 4.88008 5.78199 4.72472 5.79088 5.08141 5.86831 4.2329 4.41679 5.79928 3.44389 3.67731 8.43785 6.32501 4.94099 5.58491 4.5337 4.80508 4.19082 5.38916 4.45618 6.8551 3.55115 4.43143 6.2562 6.81683 4.26472 4.32277 4.83556 3.66165 5.42915 2.62668 4.81387 3.78336 3.14145 5.86862 5.21124 0 3.92594 4.81856 5.26171 5.27782 4.02839 7.15613 3.78153 3.71541 7.31787 3.91224 3.92013 3.85673 5.01887 2.42165 5.1073 7.64536 6.05755 4.32924 4.45991 5.0952 3.39039 8.08201 2.74677 6.6092 7.71369 5.36143 8.27771 3.15076 3.25454 3.27371 2.3467 4.65065 7.13918 5.66881 5.65302 2.75446 4.61569 3.04791 6.17067 5.0561 4.94565 3.93675 7.89827 4.31661 3.51477 6.78235 7.55774 3.75154 6.17045 4.1376 5.39657 5.55093 1.94655 2.24097 3.71963 6.22613 5.52292 3.88667 6.63454 5.03565 4.0255 2.24214 5.76951 7.5982 5.60014 3.7933 4.46647 3.12826 3.15628 5.74986 3.36984 3.22773 2.87016 5.54392 4.44306 6.47632 3.96719 6.16833 3.62601 5.97757 3.20741 7.41243 5.95108 3.26264 5.26364 4.15864 6.61338 2.66147 3.49704 5.8752 5.21688 4.87302 6.54272 6.56202 5.60614 6.02131 3.25305 5.16748 5.67866 5.61911 4.17021 3.40674 4.94874 4.56614 5.64397 3.44759 5.02878 7.39934 6.59514 5.50934 5.128 7.09602 7.27327 0 4.79371 4.16242 8.44962 4.73909 4.31434 5.51821 4.96209 5.04764 5.76253 5.24921 2.13766 4.42269 3.97598 4.60607 6.75713 5.3228 7.75142 2.62507 3.1671 6.67236 6.24686 5.28338 2.53948 4.74815 8.83293 5.38127 6.7525 4.97339 6.78183 3.23035 4.96614 5.55582 5.8463 3.57324 5.20878 1.80647 9.00533 5.21514 2.40582 3.77425 5.71667 3.71764 3.74776 3.90847 4.97908 5.15482 6.65228 5.88857 2.28403 4.68524 4.63202 5.3912 5.71233 ]
@@@ Frame-accuracy per-class: [ 87.8779 0 0 10.0629 59.3407 0 0 0 58.156 4.08163 0 0 2.89855 0 0 0 41.3793 1.65975 0 0 0 21.0526 0 30.9859 75.4286 2.72109 49.5238 0 13.7931 0 0 0 5.12821 0 48.4848 80.7947 2.8169 0 0 78.4094 4.41851 0 0 0 40 9.83607 42.236 0 0 0 0 38.5185 0 0 0 11.6505 0.930233 9.17431 72.1541 46.9136 0 0 0 56.6292 0 14.0845 0 0 0 0 45.8716 4.0404 2.15054 7.36196 0 0 17.5824 67.6056 46.3158 0 0 9.02256 0 0 0 0 0 0 7.17489 0 0 0 48.2759 0 16.5414 0 0 0 67.5159 0 0 0 0 9.5941 74.6667 0 6.34921 0 0 0 0 0 1.06952 53.2374 0 0 0 0 3.07692 2.66667 0 0 0 0 10.5263 36.7816 0 0 0 22.6415 2.25989 0 53.3333 4.5977 0 0 40 83.2487 9.23077 0 0 35.6688 0 30.4 0 0 16.9492 0 0 42.623 49.3617 19.7183 0 0 18.6667 77.2201 60.6061 0 38.7597 20.8651 0 8.16327 0 0 0 0 5.7971 0 13.3333 0 1.55039 0 0 15.6863 0 0 22.9508 0 0 0 0 0 0 0 33.8462 0 0 0 0 0 18.4615 70.5882 28.3186 21.9178 0 0 43.6364 0 0 0 0 0 0 0 0 10.989 3.63636 56.4417 46.5116 0 0 11.0345 47.4576 0 10.7527 11.3475 0 40 3.77358 0 39.604 0 24.6154 0 0 0 0 0 0 0 0 50.4854 3.36134 14.6341 0 0 21.7687 0 0 3.07692 0 6.06061 0 22.7848 0 0 0 0 0 9.7561 0 0 1.90476 13.3333 13.3333 19.3548 18.4615 0 0 21.164 28.9655 0 8.40336 0 0 67.6056 0 0 29.6296 5.40541 15.3846 3.77358 0 25.9414 6.06061 0 0 0 7.40741 0 32.4324 0 0 5.98802 57.1429 0 0 0 0 0 0 0 4.12371 38.0165 1.25786 0 0 0 0 0 30.0885 0 0 20.2899 62.6566 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 21.4876 0 0 0 0 0 0 35.461 0 0 0 0 32.2097 0 0 0 0 0 60.8696 0 14.6119 0 0 0 0 0 0 0 0 2.62009 0 28.777 22.5352 0 0 0 0 15.1899 4.5977 0 0 10.6667 0 2.15054 21.2766 24.6154 0 0 10.3093 66.0099 0 27.6923 0 11.5942 31.7757 39.5664 13.4454 5.97015 0 10.5263 0 19.7183 0 2.51572 0 2.1978 0 0 0 0 0 0 15.2381 0 0 0 0 0 13.5593 0 42.9907 0 9.7561 0 0 10.5263 0.77821 0 0 0 0 0 9.23077 0 62.3656 0 3.07692 19.8895 0 73.1707 58.4615 0 0 26.5263 6.59341 0 0 0 0 0 6.55738 9.52381 0 0 0 0 26.8041 59.8985 6.45161 0 14.4578 0 64.4628 0 0 2.5974 22.6804 13.8614 0 0 1.3986 0 0 0 29.4737 0 0 68.2353 26.9663 0 0 0 0 0 62.2222 8.88889 0 52.459 35.5556 0 10.6195 0 0 24.4898 2.8169 0 0 0 52.1739 0 0 0 0 0 0 34.7826 0 0 0 0 0 49.3827 34.0255 19.7183 0 0 0 0 58.1818 0 0 24.7619 0 0 72.7273 0 11.9403 0 0 0 0 14.1176 0 0 10.1266 0 0 0 0 0 0 5.40541 0 0 0 0 0 0 0 0 12.0658 0 2.53165 0 32.4324 0 0 0 0 0 3.92157 0 0 72.8972 1.68067 32.6531 0 0 0 20.202 0 0 0 0 0 0 0 0 0 0 0 46.6019 3.80952 18.5567 34.0426 0 0 0 36.7347 0 21.5054 0 0 0 35.8621 0 0 0 0 0 0 0 3.66972 0 0 0 0 0 0 47.4667 0 0 0 0 0 0 1.90476 0 0 0 0 19.2771 0 0 0 12.5654 5.40541 8.60215 29.9065 0 0 0 0 0 0 0 0 0 0 0 0 8.55615 0 0 0 0 0 0 4.12371 7.40741 0 12.3077 0 0 0 0 0 0 14.6789 63.9175 0 13.0178 0 0 5.40541 29.5806 33.4545 0 32.0988 0 0 0 0 0 0 0 0 0 42.9907 14.1176 24.5399 61.0778 0 0 0 0 0 0 24.0602 7.10059 4.21053 0 0 0 0 0 0 0 0 5.71429 0 0 0 0 15.1261 58.7571 0 0 9.7561 0 12.1212 0 0 30.7692 0 0 0 0 0 0 0 0 0 35.0515 0 72.2892 0 0 0 0.947867 24 74.6667 29.8851 0 0 0 1.14286 12.5654 0 10.989 0 0 0 8.16327 0 0 83.6364 0 1.36054 50.3937 12.6316 0 78.2609 0 11.215 22.8571 26.6667 0 0 0 0 0 15.2866 0 0 41.3793 12.766 12.6316 0 0 0 0 0 0 0 0 0 0 8.87372 0 0 0 0 0 7.01754 0 0 2.89855 0 0 0 0 11.7647 2.66667 0 0 0 20.2532 0 0 17.0213 0 47.8469 0 0 0 0 37.2093 0 0 0 0 0 17.5439 0 0 5.44218 0 0 0 66.4879 0 0 69.6133 14.5455 0 0 0 0 0 0 0 13.986 6.59341 0 0 0 27.451 0 0 0 0 0 0 0 0 0 0 0 0 0 0 46.5409 0 0 0 61.0778 0 0 39.0244 10.5263 1.55642 2.35294 3.00752 10.2564 0 0 0 0 0 0 21.9178 3.27869 0 0 0 0 28.5714 0 0 0 0 18.8976 23.5294 0 0 0 0 6.06061 18.3206 0 0 0 6.15385 19.5804 39.1753 15.3846 0 67.2269 28.0702 0 0 0 0 71.6981 0 6.45161 2.8169 0 17.1429 19.7183 0 44.0678 0 2.10526 6.06061 0 0 0 0 0 0 2.5974 0 42.9907 11.1498 40.5797 0 2.66667 21.2766 0 5.7971 0 0 0 2.16216 0 0 6.34921 0 22.9508 0 0 4.08163 4.49438 24.4604 0 0 0 36.9501 0 0 0 0 1.36054 4.87805 0 3.63636 9.24855 3.38983 0 0 0 0 8.16327 0 1.22699 56.2691 0 0 0 28.0702 35.443 43.956 0 8.51064 5.47945 0 0 0 0 0 0 0 0 26.3736 11.4286 2.89855 72 0 0 0 2.05128 7.05882 0 0 0 70.8861 0 0 11.215 0 0 0 0 40.678 0 0 0 0 24.1758 10.2564 43.4783 0 2.98507 8.13008 3.77358 3.50877 20.339 23.4742 0 2.98507 59.4595 16.4948 1.76991 3.46821 0 0 8 0 0 0 0 5.47945 1.443 0 14.876 11.1888 15.6863 0 36.8932 0 3.38983 0 7.47664 0 0 0 62.9213 0 0 9.63855 0 0 3.07692 0 0 0 0 0 10.3627 0 0 51.1182 0 16.3934 0 0 0 0 0 13.1868 37.2881 0 0 0 0 0 0 17.1285 0 0 10.1695 0 20.2532 13.9535 2.53165 0 71.6981 0 17.7215 27.9793 0 0 0 0 0 0 0 5.92593 3.50877 0 9.90099 0 0 0 0 0 0 0 3.27869 0 15.2975 0 47.3373 18.6335 0 0 1.34228 3.07692 11.0429 29.8851 0 19.6078 1.45985 22.8571 26.3473 0 0 0 29.8507 0 13.7931 0 13.1148 0 5.28302 13.2159 0 40.5594 0 0 0 54.5455 0 0 0 0 2.8169 0 2.73973 0 16 0 0 0 0 36.7347 13.3739 19.0476 57.1429 0 0 0 0 36.1446 0 48 0 0 0 7.59494 0 0 9.69697 0 0 25.3165 0 0 1.65289 0 67.7551 40.367 0 0 0 3.50877 0 0 0 62.2951 0 0 0 2.35294 9.7166 22.695 24.6886 0 5.6338 13.6986 0.235018 0 0 0 7.22892 0 0 0 1.23839 0 0 16.2896 0 8.21918 0.761905 51.1278 14.9254 0 0 0 0 0 3.4188 0 29.4737 0 0 0 0 11.2676 0 8.12183 0 20.1681 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 59.789 0 12.6126 0 0 0 0 56.4706 11.7647 0 0 0 32.5792 0 0 0 0 0 0 38.961 0 0 0 0 0 70.5882 0 0 21.5054 14.6789 0 4.49438 2.0202 0 0 0 0 0 62.8019 0 0 0 0 ]

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:347) Done 295 files, 0 with no tgt_mats, 0 with other errors. [CROSS-VALIDATION, 0.020613 min, fps64047.1]
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:353) AvgLoss: 3.4422 (Xent), [AvgXent: 3.4422, AvgTargetEnt: 0]
progress: []
FRAME_ACCURACY >> 14.4359% <<

WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Close():kaldi-io.cc:515) Pipe ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- | had nonzero return status 36096
