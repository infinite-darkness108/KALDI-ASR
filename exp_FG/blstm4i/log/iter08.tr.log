speech-HP-Z2-Tower-G9
nnet-train-multistream-perutt --cross-validate=false --randomize=true --verbose=0 --num-streams=10 --max-frames=15000 --learn-rate=0.00004 --momentum=0.9 --l1-penalty=0 --l2-penalty=0 --feature-transform=exp_FG/blstm4i/final.feature_transform 'ark:copy-feats scp:exp_FG/blstm4i/train.scp ark:- | apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_tr90/utt2spk scp:data-fbank/train_tr90/cmvn.scp ark:- ark:- | add-deltas --delta-order=2 ark:- ark:- |' 'ark:ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- |' exp_FG/blstm4i/nnet/nnet_iter07_learnrate0.00004_tr0.7455_cv1.9889 exp_FG/blstm4i/nnet/nnet_iter08 
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:243) Not in compute-exclusive mode.  Suggestion: use 'nvidia-smi -c 3' to set compute exclusive mode
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:438) Selecting from 1 GPUs
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:453) cudaSetDevice(0): NVIDIA RTX A2000 12GB	free:11620M, used:410M, total:12031M, free/total:0.96589
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:501) Device: 0, mem_ratio: 0.96589
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:382) Trying to select device: 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:511) Success selecting device 0 free mem ratio: 0.96589
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:FinalizeActiveGpu():cu-device.cc:338) The active GPU is [0]: NVIDIA RTX A2000 12GB	free:11106M, used:924M, total:12031M, free/total:0.923168 version 8.6
copy-feats scp:exp_FG/blstm4i/train.scp ark:- 
apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_tr90/utt2spk scp:data-fbank/train_tr90/cmvn.scp ark:- ark:- 
add-deltas --delta-order=2 ark:- ark:- 
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:144) TRAINING STARTED
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Read():nnet/nnet-matrix-buffer.h:191) Read() started... Buffer size in MB: 0, max 3072, having 0 utterances.
ali-to-post ark:- ark:- 
ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl 'ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |' ark:- 
LOG (copy-feats[5.5.1074~1-71f3]:main():copy-feats.cc:143) Copied 2624 feature matrices.
LOG (apply-cmvn[5.5.1074~1-71f3]:main():apply-cmvn.cc:159) Applied cepstral mean and variance normalization to 2624 utterances, errors on 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:303) ### After 0 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:304) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.686024, max 0.74657, mean 0.00389515, stddev 0.0880324, skewness 0.0111968, kurtosis 0.927297 ) 
  f_w_gifo_r_   ( min -0.424481, max 0.41372, mean -0.000587136, stddev 0.078712, skewness -0.00335778, kurtosis 0.0235715 ) 
  f_bias_   ( min -0.351808, max 1.38333, mean 0.211198, stddev 0.464319, skewness 1.06073, kurtosis -0.650819 ) 
  f_peephole_i_c_   ( min -0.622492, max 0.50455, mean -0.00364522, stddev 0.132115, skewness -0.0637931, kurtosis 2.1273 ) 
  f_peephole_f_c_   ( min -0.706738, max 0.759926, mean 0.00457814, stddev 0.173846, skewness 0.188114, kurtosis 3.4751 ) 
  f_peephole_o_c_   ( min -0.685102, max 0.497133, mean -0.0102499, stddev 0.192142, skewness 0.0583107, kurtosis -0.00488901 ) 
  f_w_r_m_   ( min -0.510778, max 0.50224, mean 0.000601838, stddev 0.10442, skewness 0.0018339, kurtosis 0.0251124 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -1.65804, max 1.10727, mean 0.00608395, stddev 0.0952527, skewness -0.172158, kurtosis 4.92182 ) 
  b_w_gifo_r_   ( min -0.349278, max 0.36747, mean -0.000199992, stddev 0.0722195, skewness 0.000315697, kurtosis -0.235974 ) 
  b_bias_   ( min -0.389591, max 1.22295, mean 0.20528, stddev 0.454039, skewness 1.03477, kurtosis -0.693059 ) 
  b_peephole_i_c_   ( min -0.423051, max 0.313857, mean 0.00695607, stddev 0.101171, skewness -0.135212, kurtosis 1.34845 ) 
  b_peephole_f_c_   ( min -0.563083, max 0.746481, mean 0.0166781, stddev 0.177391, skewness 0.57997, kurtosis 3.06575 ) 
  b_peephole_o_c_   ( min -0.575059, max 0.538464, mean -0.0157992, stddev 0.197698, skewness -0.163698, kurtosis 0.350608 ) 
  b_w_r_m_   ( min -0.409651, max 0.396487, mean -0.000354738, stddev 0.0940538, skewness -0.00200095, kurtosis -0.0343707 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.970407, max 0.758278, mean -0.000155915, stddev 0.108394, skewness 0.00561857, kurtosis 0.0595918 ) , lr-coef 1, max-norm 0
  bias ( min -0.0883832, max 2.50559, mean -1.30385e-09, stddev 0.0828127, skewness 22.8515, kurtosis 661.613 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:305) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -8.67671, max 9.02116, mean 0.0321104, stddev 1.01793, skewness 0.450612, kurtosis 3.20408 ) 
[1] output of <BlstmProjected> ( min -4.71433, max 4.76291, mean 0.00136622, stddev 0.781508, skewness -0.00681926, kurtosis 1.36616 ) 
[2] output of <Tanh> ( min -0.999839, max 0.999854, mean 0.00121565, stddev 0.512108, skewness -0.00266457, kurtosis -0.745456 ) 
[3] output of <AffineTransform> ( min -15.3336, max 22.2124, mean 0.00748214, stddev 2.4475, skewness 0.861812, kurtosis 3.3573 ) 
[4] output of <Softmax> ( min 1.09739e-15, max 0.999961, mean 0.000780181, stddev 0.0213246, skewness 36.9541, kurtosis 1472.56 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:307) 
### BACKWARD PROPAGATION BUFFER CONTENT :
[0] diff of <Input>  ( min -5.58056, max 5.21557, mean -0.00186287, stddev 0.273633, skewness -0.582292, kurtosis 26.32 ) 
[1] diff-output of <BlstmProjected> ( min -0.719293, max 0.888987, mean -0.000149397, stddev 0.0461152, skewness -0.0121687, kurtosis 8.62024 ) 
[2] diff-output of <Tanh> ( min -0.773884, max 0.945708, mean -0.000162802, stddev 0.0612682, skewness 0.0079764, kurtosis 5.64864 ) 
[3] diff-output of <AffineTransform> ( min -0.999997, max 0.967085, mean -1.38264e-07, stddev 0.0168711, skewness -22.6646, kurtosis 1885.79 ) 
[4] diff-output of <Softmax> ( min -0.999997, max 0.967085, mean -1.38264e-07, stddev 0.0168711, skewness -22.6646, kurtosis 1885.79 ) 
### END BACKWARD


LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:308) 
### GRADIENT STATS :
Component 1 : <BlstmProjected>, ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  ### Gradients 
  f_w_gifo_x_corr_   ( min -40.3244, max 45.2401, mean 0.091968, stddev 3.38007, skewness 0.382274, kurtosis 13.697 ) 
  f_w_gifo_r_corr_   ( min -34.5708, max 45.1052, mean 0.00260514, stddev 2.49807, skewness -0.0102456, kurtosis 9.15854 ) 
  f_bias_corr_   ( min -28.2787, max 30.5848, mean 0.07918, stddev 3.72329, skewness 0.138214, kurtosis 10.4541 ) 
  f_peephole_i_c_corr_   ( min -28.5443, max 114.571, mean 0.401143, stddev 7.9462, skewness 9.33364, kurtosis 132.621 ) 
  f_peephole_f_c_corr_   ( min -49.6144, max 34.4583, mean -0.668103, stddev 9.89514, skewness -0.79135, kurtosis 4.7 ) 
  f_peephole_o_c_corr_   ( min -250, max 235.025, mean -0.323912, stddev 23.4593, skewness -0.964667, kurtosis 70.6011 ) 
  f_w_r_m_corr_   ( min -34.5159, max 43.4876, mean -0.0036632, stddev 3.83763, skewness 0.122466, kurtosis 4.82754 ) 
  ---
  b_w_gifo_x_corr_   ( min -105.846, max 118.233, mean 0.151662, stddev 3.98677, skewness 4.72969, kurtosis 142.174 ) 
  b_w_gifo_r_corr_   ( min -53.3578, max 46.872, mean -0.00794243, stddev 2.79632, skewness -0.036335, kurtosis 9.52364 ) 
  b_bias_corr_   ( min -89.9375, max 39.8416, mean -0.361099, stddev 5.85268, skewness -2.53206, kurtosis 50.9262 ) 
  b_peephole_i_c_corr_   ( min -35.0107, max 250, mean 0.873931, stddev 14.5908, skewness 15.529, kurtosis 262.798 ) 
  b_peephole_f_c_corr_   ( min -55.7135, max 81.624, mean 0.662412, stddev 10.0088, skewness 0.979604, kurtosis 16.8994 ) 
  b_peephole_o_c_corr_   ( min -47.1018, max 64.5205, mean -0.202755, stddev 10.2218, skewness 0.819364, kurtosis 9.24029 ) 
  b_w_r_m_corr_   ( min -25.8371, max 23.5411, mean 0.0113393, stddev 3.87596, skewness 0.0203931, kurtosis 1.1058 ) 

  ### Activations (mostly after non-linearities)
  YI_FW(0..1)^   ( min 0, max 1, mean 0.422185, stddev 0.346855, skewness 0.392236, kurtosis -1.288 ) 
  YF_FW(0..1)^   ( min 0, max 1, mean 0.641061, stddev 0.319138, skewness -0.540721, kurtosis -0.981104 ) 
  YO_FW(0..1)^   ( min 0, max 1, mean 0.367558, stddev 0.354404, skewness 0.638178, kurtosis -1.13463 ) 
  YG_FW(-1..1)   ( min -1, max 1, mean 0.0207522, stddev 0.864588, skewness -0.0420563, kurtosis -1.7935 ) 
  YC_FW(-R..R)*  ( min -50, max 50, mean 0.233835, stddev 13.0004, skewness 0.0828895, kurtosis 9.81798 ) 
  YH_FW(-1..1)   ( min -1, max 1, mean 0.0335638, stddev 0.686513, skewness -0.0613059, kurtosis -1.24112 ) 
  YM_FW(-1..1)   ( min -1, max 1, mean 0.00364262, stddev 0.330232, skewness -0.0783697, kurtosis 2.83526 ) 
  YR_FW(-R..R)   ( min -4.40418, max 4.48115, mean 0.0186618, stddev 0.756509, skewness 0.0351282, kurtosis 1.33299 ) 
  ---
  YI_BW(0..1)^   ( min 0, max 1, mean 0.460044, stddev 0.342217, skewness 0.236541, kurtosis -1.39696 ) 
  YF_BW(0..1)^   ( min 0, max 1, mean 0.643491, stddev 0.292835, skewness -0.55363, kurtosis -0.76044 ) 
  YO_BW(0..1)^   ( min 0, max 1, mean 0.383344, stddev 0.36296, skewness 0.550675, kurtosis -1.27381 ) 
  YG_BW(-1..1)   ( min -1, max 1, mean 0.0111585, stddev 0.862658, skewness -0.0203877, kurtosis -1.79353 ) 
  YC_BW(-R..R)*  ( min -50, max 50, mean 1.21521, stddev 11.6289, skewness 1.05754, kurtosis 11.3171 ) 
  YH_BW(-1..1)   ( min -1, max 1, mean 0.0444963, stddev 0.696685, skewness -0.0570881, kurtosis -1.30387 ) 
  YM_BW(-1..1)   ( min -0.999998, max 1, mean 0.00498158, stddev 0.341373, skewness 0.0226893, kurtosis 2.25825 ) 
  YR_BW(-R..R)   ( min -4.71433, max 4.76291, mean -0.0159433, stddev 0.801719, skewness -0.0349329, kurtosis 1.40038 ) 

  ### Derivatives (w.r.t. inputs of non-linearities)
  DI_FW^  ( min -1, max 1, mean -3.53108e-05, stddev 0.0251861, skewness 0.66691, kurtosis 401.925 ) 
  DF_FW^  ( min -1, max 1, mean 5.57856e-05, stddev 0.02064, skewness 0.824248, kurtosis 596.536 ) 
  DO_FW^  ( min -0.932414, max 1, mean 6.90152e-05, stddev 0.0256874, skewness 0.419458, kurtosis 93.8111 ) 
  DG_FW   ( min -1, max 1, mean -8.69331e-06, stddev 0.030784, skewness -0.369705, kurtosis 426.017 ) 
  DC_FW*  ( min -46.8729, max 41.2296, mean -0.00137371, stddev 0.561537, skewness -15.9068, kurtosis 2447 ) 
  DH_FW   ( min -5.60038, max 4.7821, mean 5.13167e-05, stddev 0.122168, skewness 0.302976, kurtosis 89.0125 ) 
  DM_FW   ( min -6.6619, max 6.39441, mean -0.000175699, stddev 0.304602, skewness 0.0245249, kurtosis 18.8818 ) 
  DR_FW   ( min -1.69128, max 1.79397, mean -1.80977e-05, stddev 0.0864968, skewness -0.0260704, kurtosis 12.3716 ) 
  ---
  DI_BW^  ( min -1, max 1, mean -0.000127164, stddev 0.024496, skewness -0.562505, kurtosis 300.744 ) 
  DF_BW^  ( min -1, max 1, mean -8.76197e-05, stddev 0.0172343, skewness -0.986086, kurtosis 367.549 ) 
  DO_BW^  ( min -0.762256, max 1, mean -0.000158038, stddev 0.021392, skewness 0.356431, kurtosis 85.2135 ) 
  DG_BW   ( min -1, max 1, mean 4.35192e-06, stddev 0.0373519, skewness -0.586294, kurtosis 240.439 ) 
  DC_BW*  ( min -24.7134, max 11.9338, mean -0.00346763, stddev 0.336483, skewness -22.5887, kurtosis 1331.37 ) 
  DH_BW   ( min -4.40178, max 4.90671, mean 0.000420852, stddev 0.105646, skewness 0.371803, kurtosis 72.4431 ) 
  DM_BW   ( min -6.74986, max 7.00158, mean 0.000556507, stddev 0.262855, skewness -0.0048203, kurtosis 13.1932 ) 
  DR_BW   ( min -1.33613, max 1.56306, mean -0.00030426, stddev 0.0898086, skewness 0.0445629, kurtosis 9.04243 ) 
Component 2 : <Tanh>, 
Component 3 : <AffineTransform>, 
  linearity_grad ( min -48.5999, max 58.1163, mean -3.62076e-08, stddev 1.27444, skewness -0.250986, kurtosis 110.162 ) , lr-coef 1, max-norm 0
  bias_grad ( min -92.8568, max 67.9272, mean -1.78814e-08, stddev 3.79036, skewness -7.31782, kurtosis 360.454 ) , lr-coef 1
Component 4 : <Softmax>, 
### END GRADIENT

LOG (ali-to-pdf[5.5.1074~1-71f3]:main():ali-to-pdf.cc:68) Converted 2919 alignments to pdf sequences.
LOG (ali-to-post[5.5.1074~1-71f3]:main():ali-to-post.cc:73) Converted 2919 alignments.
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:168) MC05_98, missing targets
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:334) ### After 755062 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:335) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.753568, max 0.906113, mean 0.00394668, stddev 0.0902404, skewness 0.0115425, kurtosis 1.15702 ) 
  f_w_gifo_r_   ( min -0.421162, max 0.415142, mean -0.000591186, stddev 0.0792715, skewness -0.0031913, kurtosis 0.0371892 ) 
  f_bias_   ( min -0.349376, max 1.4383, mean 0.211131, stddev 0.465931, skewness 1.05943, kurtosis -0.647199 ) 
  f_peephole_i_c_   ( min -0.687176, max 0.491791, mean -0.00526809, stddev 0.135964, skewness -0.105526, kurtosis 2.50235 ) 
  f_peephole_f_c_   ( min -0.713593, max 0.972982, mean 0.00591965, stddev 0.182462, skewness 0.396163, kurtosis 4.62677 ) 
  f_peephole_o_c_   ( min -0.597303, max 0.506818, mean -0.0126943, stddev 0.196616, skewness 0.0679237, kurtosis -0.154991 ) 
  f_w_r_m_   ( min -0.507964, max 0.510147, mean 0.000573597, stddev 0.105838, skewness 0.00223551, kurtosis 0.0302627 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -1.70022, max 1.1857, mean 0.00603198, stddev 0.0981901, skewness -0.178156, kurtosis 5.58222 ) 
  b_w_gifo_r_   ( min -0.349673, max 0.374727, mean -0.000185871, stddev 0.0731482, skewness 0.000343579, kurtosis -0.205115 ) 
  b_bias_   ( min -0.388245, max 1.23297, mean 0.203728, stddev 0.455276, skewness 1.02718, kurtosis -0.695317 ) 
  b_peephole_i_c_   ( min -0.4486, max 0.318392, mean 0.00373275, stddev 0.105013, skewness -0.179622, kurtosis 1.72284 ) 
  b_peephole_f_c_   ( min -0.646684, max 0.759745, mean 0.014208, stddev 0.185454, skewness 0.576766, kurtosis 3.27424 ) 
  b_peephole_o_c_   ( min -0.599034, max 0.61779, mean -0.0155737, stddev 0.203541, skewness -0.0627148, kurtosis 0.522262 ) 
  b_w_r_m_   ( min -0.423328, max 0.401488, mean -0.000398769, stddev 0.0958159, skewness 0.00115021, kurtosis -0.0204606 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.992197, max 0.778834, mean -0.000155913, stddev 0.10923, skewness 0.0054761, kurtosis 0.0578189 ) , lr-coef 1, max-norm 0
  bias ( min -0.0940014, max 2.54685, mean -3.67872e-09, stddev 0.0848047, skewness 22.4854, kurtosis 643.798 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:336) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -14.8312, max 13.8231, mean 0.00769689, stddev 0.940803, skewness 0.731578, kurtosis 12.6034 ) 
[1] output of <BlstmProjected> ( min -4.23953, max 4.09027, mean 0.00183577, stddev 0.659753, skewness -0.0404821, kurtosis 3.05774 ) 
[2] output of <Tanh> ( min -0.999585, max 0.99944, mean 0.00194021, stddev 0.434763, skewness -0.00344267, kurtosis 0.0993648 ) 
[3] output of <AffineTransform> ( min -11.2034, max 23.2907, mean 0.00467292, stddev 2.088, skewness 1.03689, kurtosis 5.99667 ) 
[4] output of <Softmax> ( min 1.63905e-13, max 0.999969, mean 0.000781031, stddev 0.0195535, skewness 42.9119, kurtosis 1951.19 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:338) 
### BACKWARD PROPAGATION BUFFER CONTENT :
[0] diff of <Input>  ( min -2.19, max 2.76796, mean 0.0039857, stddev 0.189674, skewness 0.0195238, kurtosis 23.9705 ) 
[1] diff-output of <BlstmProjected> ( min -0.582163, max 0.688419, mean -0.000150953, stddev 0.0372365, skewness -0.0825435, kurtosis 18.2301 ) 
[2] diff-output of <Tanh> ( min -0.723547, max 0.700592, mean -0.00016073, stddev 0.050373, skewness -0.0349078, kurtosis 10.9999 ) 
[3] diff-output of <AffineTransform> ( min -0.998791, max 0.927135, mean -5.25612e-09, stddev 0.0136266, skewness -25.5949, kurtosis 3046.05 ) 
[4] diff-output of <Softmax> ( min -0.998791, max 0.927135, mean -5.25612e-09, stddev 0.0136266, skewness -25.5949, kurtosis 3046.05 ) 
### END BACKWARD


LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:339) 
### GRADIENT STATS :
Component 1 : <BlstmProjected>, ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  ### Gradients 
  f_w_gifo_x_corr_   ( min -40.4511, max 37.6252, mean -0.0606926, stddev 3.55345, skewness -0.158313, kurtosis 7.41932 ) 
  f_w_gifo_r_corr_   ( min -40.2305, max 33.9922, mean -0.00539443, stddev 3.06688, skewness -0.0296435, kurtosis 5.32151 ) 
  f_bias_corr_   ( min -32.3264, max 31.3707, mean -0.270277, stddev 4.27835, skewness 0.0018252, kurtosis 6.57514 ) 
  f_peephole_i_c_corr_   ( min -55.5237, max 202.206, mean 0.925351, stddev 13.4093, skewness 10.523, kurtosis 157.962 ) 
  f_peephole_f_c_corr_   ( min -64.9774, max 80.966, mean -0.357668, stddev 13.9474, skewness 0.59843, kurtosis 8.49931 ) 
  f_peephole_o_c_corr_   ( min -126.005, max 77.9591, mean -1.47621, stddev 16.1286, skewness -1.41515, kurtosis 13.9133 ) 
  f_w_r_m_corr_   ( min -33.4971, max 41.0364, mean 0.00192565, stddev 4.48534, skewness 0.0827325, kurtosis 3.37246 ) 
  ---
  b_w_gifo_x_corr_   ( min -56.8169, max 83.4447, mean 0.109542, stddev 4.30092, skewness 0.352806, kurtosis 17.643 ) 
  b_w_gifo_r_corr_   ( min -41.5386, max 49.9782, mean -0.00536581, stddev 3.06883, skewness -0.0201388, kurtosis 5.92602 ) 
  b_bias_corr_   ( min -41.4104, max 42.0601, mean -0.3361, stddev 6.11336, skewness 0.565251, kurtosis 8.99199 ) 
  b_peephole_i_c_corr_   ( min -32.5, max 37.8393, mean -0.128859, stddev 5.31831, skewness 0.466278, kurtosis 13.0069 ) 
  b_peephole_f_c_corr_   ( min -250, max 44.4852, mean -0.144375, stddev 17.3129, skewness -9.37766, kurtosis 133.252 ) 
  b_peephole_o_c_corr_   ( min -35.2987, max 66.0616, mean 0.795044, stddev 13.0507, skewness 1.2092, kurtosis 4.62288 ) 
  b_w_r_m_corr_   ( min -30.8929, max 37.5527, mean 0.00910401, stddev 4.56473, skewness 0.00423107, kurtosis 1.41628 ) 

  ### Activations (mostly after non-linearities)
  YI_FW(0..1)^   ( min 0, max 1, mean 0.314402, stddev 0.353392, skewness 0.785606, kurtosis -0.889016 ) 
  YF_FW(0..1)^   ( min 0, max 1, mean 0.473038, stddev 0.393186, skewness 0.0232802, kurtosis -1.61172 ) 
  YO_FW(0..1)^   ( min 0, max 1, mean 0.267301, stddev 0.343554, skewness 1.07821, kurtosis -0.374722 ) 
  YG_FW(-1..1)   ( min -1, max 1, mean 0.0186493, stddev 0.75264, skewness -0.0356934, kurtosis -1.40048 ) 
  YC_FW(-R..R)*  ( min -50, max 50, mean 0.222826, stddev 10.2406, skewness 0.304121, kurtosis 16.6522 ) 
  YH_FW(-1..1)   ( min -1, max 1, mean 0.0265895, stddev 0.588172, skewness -0.0406628, kurtosis -0.621175 ) 
  YM_FW(-1..1)   ( min -1, max 1, mean 0.00382052, stddev 0.278312, skewness -0.0843029, kurtosis 5.10635 ) 
  YR_FW(-R..R)   ( min -4.23953, max 4.09027, mean 0.00619366, stddev 0.64655, skewness 0.0446953, kurtosis 2.90117 ) 
  ---
  YI_BW(0..1)^   ( min 0, max 1, mean 0.33527, stddev 0.354925, skewness 0.663976, kurtosis -1.0713 ) 
  YF_BW(0..1)^   ( min 0, max 1, mean 0.473127, stddev 0.375726, skewness -0.0314604, kurtosis -1.52063 ) 
  YO_BW(0..1)^   ( min 0, max 1, mean 0.278051, stddev 0.351607, skewness 1.00645, kurtosis -0.558819 ) 
  YG_BW(-1..1)   ( min -1, max 1, mean 0.0106146, stddev 0.752747, skewness -0.0159729, kurtosis -1.40588 ) 
  YC_BW(-R..R)*  ( min -50, max 50, mean 0.855034, stddev 9.7658, skewness 1.48128, kurtosis 16.5795 ) 
  YH_BW(-1..1)   ( min -1, max 1, mean 0.0291353, stddev 0.5943, skewness -0.00804393, kurtosis -0.678771 ) 
  YM_BW(-1..1)   ( min -1, max 0.999999, mean 0.000254392, stddev 0.284715, skewness -0.0564134, kurtosis 4.49236 ) 
  YR_BW(-R..R)   ( min -4.21783, max 4.07793, mean -0.00254397, stddev 0.668808, skewness -0.11626, kurtosis 3.24608 ) 

  ### Derivatives (w.r.t. inputs of non-linearities)
  DI_FW^  ( min -1, max 1, mean -0.000121046, stddev 0.0260326, skewness -2.7549, kurtosis 444.82 ) 
  DF_FW^  ( min -1, max 1, mean 9.73417e-06, stddev 0.0199207, skewness 2.75919, kurtosis 687.936 ) 
  DO_FW^  ( min -1, max 0.753953, mean -0.000158546, stddev 0.0242811, skewness -2.08474, kurtosis 173.581 ) 
  DG_FW   ( min -1, max 1, mean -0.000229486, stddev 0.0278633, skewness -4.54243, kurtosis 588.77 ) 
  DC_FW*  ( min -35.4705, max 11.7746, mean -0.00440271, stddev 0.385548, skewness -28.7414, kurtosis 2298.55 ) 
  DH_FW   ( min -3.70832, max 4.39972, mean -3.80224e-05, stddev 0.116597, skewness 0.84123, kurtosis 144.2 ) 
  DM_FW   ( min -5.92857, max 7.67489, mean 0.000145273, stddev 0.289462, skewness 0.264756, kurtosis 40.998 ) 
  DR_FW   ( min -1.59685, max 1.85241, mean -0.000442616, stddev 0.0813176, skewness 0.0176162, kurtosis 28.2506 ) 
  ---
  DI_BW^  ( min -1, max 1, mean -0.000164736, stddev 0.0169521, skewness -3.34413, kurtosis 415.686 ) 
  DF_BW^  ( min -0.648543, max 0.375941, mean -0.000176767, stddev 0.0126639, skewness -6.36613, kurtosis 355.026 ) 
  DO_BW^  ( min -0.474165, max 0.489916, mean -5.37268e-05, stddev 0.0158439, skewness 0.335079, kurtosis 71.7188 ) 
  DG_BW   ( min -1, max 1, mean -0.000241603, stddev 0.0274482, skewness -3.48625, kurtosis 417.209 ) 
  DC_BW*  ( min -13.4536, max 3.53752, mean -0.00268111, stddev 0.201292, skewness -16.975, kurtosis 842.209 ) 
  DH_BW   ( min -2.89468, max 1.80875, mean 0.000132174, stddev 0.0881811, skewness -1.00917, kurtosis 79.1931 ) 
  DM_BW   ( min -3.12839, max 2.67425, mean -7.0394e-05, stddev 0.20302, skewness -0.125585, kurtosis 14.676 ) 
  DR_BW   ( min -0.916767, max 0.750845, mean -0.000523815, stddev 0.0691559, skewness -0.0919471, kurtosis 9.6448 ) 
Component 2 : <Tanh>, 
Component 3 : <AffineTransform>, 
  linearity_grad ( min -70.2946, max 71.3581, mean -4.64888e-08, stddev 1.59171, skewness -0.0763818, kurtosis 129.111 ) , lr-coef 1, max-norm 0
  bias_grad ( min -142.002, max 67.3109, mean 1.2517e-07, stddev 5.67436, skewness -13.037, kurtosis 357.226 ) , lr-coef 1
Component 4 : <Softmax>, 
### END GRADIENT

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:346) PER-CLASS PERFORMANCE:
@@@ Frames per-class: [ 181212 144 127 606 532 43 173 190 515 703 138 211 211 608 45 289 271 572 372 153 368 892 307 454 467 612 374 177 314 165 203 169 193 120 162 350 506 100 142 40889 89709 154 463 1777 286 231 687 352 545 459 79 394 481 237 200 663 1037 475 2210 428 257 167 4192 1910 93 230 151 223 4363 199 339 612 931 689 168 53 430 425 404 147 72 468 398 184 543 242 103 448 1376 60 779 717 668 146 541 163 358 830 561 408 249 268 103 1105 608 477 208 629 257 136 769 185 359 2082 165 69 303 74 247 336 172 147 117 128 443 612 117 125 52 316 956 165 305 574 202 297 222 1167 208 209 172 556 182 606 267 121 534 323 470 239 776 321 812 179 268 1139 537 191 554 5133 217 193 202 128 264 242 282 141 453 190 390 248 274 208 168 117 250 72 227 832 107 208 74 120 577 200 74 173 93 435 313 308 638 462 201 239 312 172 59 151 330 221 210 68 77 424 248 789 348 126 86 566 208 142 331 485 286 372 234 328 637 273 293 132 41 150 57 102 173 655 328 426 861 178 245 87 533 72 131 265 150 790 182 357 233 69 187 112 206 273 85 62 302 205 411 179 278 260 193 525 723 141 288 139 513 788 215 97 150 236 273 455 166 615 208 63 196 43 120 342 911 206 87 1156 192 146 131 166 31 221 252 417 373 499 401 233 179 409 182 439 579 96 189 251 1812 272 55 134 245 227 120 170 454 69 211 126 232 303 701 89 59 87 90 418 122 334 282 91 119 125 379 59 189 157 163 620 120 203 224 199 71 381 86 544 80 247 220 94 114 76 256 175 1186 420 378 336 135 140 331 666 306 356 196 203 291 152 221 543 214 149 360 365 670 264 264 692 216 616 923 524 408 234 395 301 375 105 525 80 359 98 456 204 309 258 288 286 181 90 101 263 53 253 167 354 150 174 194 393 579 494 437 273 142 236 159 226 115 606 198 255 508 136 282 304 327 234 1467 199 113 213 747 267 237 238 267 223 423 127 334 351 773 400 668 269 762 507 73 172 259 944 420 60 318 817 125 150 155 430 99 155 360 316 425 603 146 124 301 210 170 102 140 531 288 716 282 161 233 181 349 120 133 242 135 51 141 111 147 205 551 427 131 99 109 133 445 7274 204 271 135 128 125 377 120 372 449 808 108 557 45 280 47 596 57 276 400 121 347 315 193 77 146 96 105 216 193 341 1125 236 219 105 417 109 338 639 130 396 177 211 138 121 275 91 104 194 51 542 461 346 197 315 121 109 444 210 64 135 84 52 96 283 268 315 144 187 498 569 470 194 79 151 71 285 409 404 165 215 321 466 69 209 585 117 83 141 150 364 478 225 439 81 192 156 773 344 306 243 112 146 203 501 258 143 117 231 316 156 83 53 468 163 539 465 176 116 79 157 350 139 86 253 155 101 132 342 440 324 189 390 153 111 192 365 208 291 263 103 139 134 307 402 452 451 5943 88 406 111 189 488 786 548 86 632 99 382 216 141 365 388 65 90 187 430 300 463 604 247 79 314 305 463 45 473 718 330 223 226 1312 201 160 62 170 227 141 90 90 193 68 539 610 241 156 182 257 224 114 99 209 178 32 193 174 235 238 150 123 308 261 195 275 142 111 371 821 518 831 306 124 432 87 357 932 174 327 225 300 36 190 272 120 358 461 397 462 306 105 348 166 390 216 520 219 260 71 179 258 429 201 53 142 256 406 60 128 173 26 88 191 83 60 155 179 1000 125 44 191 230 383 687 216 257 321 233 417 202 1061 342 327 163 248 165 412 229 303 517 124 815 539 79 306 155 255 89 321 245 73 308 228 196 192 269 176 228 101 8591 469 337 966 237 430 177 258 334 85 304 414 487 401 57 178 99 260 178 151 248 261 926 126 248 230 264 241 147 220 155 139 646 219 209 393 1097 85 160 936 605 267 175 213 208 155 166 207 194 260 180 542 294 745 650 41 235 409 387 268 281 172 798 467 256 138 171 316 378 542 197 77 199 253 693 506 326 151 543 272 142 171 242 241 191 161 274 304 47 226 320 127 248 43 388 236 233 244 332 111 261 196 246 68 466 707 315 146 315 282 30 427 201 153 510 1026 278 186 316 45 464 106 316 404 415 543 346 257 452 1347 138 153 366 558 585 171 109 855 530 221 155 160 161 111 196 131 341 863 116 777 103 428 210 393 95 260 221 295 129 320 516 185 97 126 343 492 405 270 645 150 141 159 382 1153 426 252 535 332 205 417 575 597 292 372 180 236 213 343 301 154 349 528 290 243 378 616 213 266 230 676 309 338 165 324 314 639 310 148 365 184 119 116 199 264 1367 367 467 1442 213 598 641 335 156 122 351 114 129 98 489 213 162 386 116 74 274 197 265 173 129 167 753 211 214 1711 286 746 207 83 123 191 204 353 783 57 223 151 323 285 99 1197 111 116 189 130 401 488 428 96 280 248 468 506 117 284 315 328 129 334 124 557 265 243 383 268 66 97 474 153 221 139 391 194 634 156 322 748 309 195 326 651 462 370 172 385 490 392 610 304 249 70 501 324 335 228 370 206 587 560 108 850 256 209 153 168 206 131 204 232 340 200 284 174 301 124 107 220 67 370 1390 1253 311 434 127 136 104 493 416 778 103 335 338 242 66 208 459 231 96 291 160 220 368 145 728 458 351 255 350 348 137 125 1039 1466 285 221 232 361 605 549 4192 543 376 361 3105 243 319 117 225 232 542 309 859 113 244 913 235 271 853 881 252 243 274 129 90 371 274 176 453 107 172 140 172 411 267 778 147 558 2509 132 95 258 162 203 87 21 467 192 231 265 665 552 160 225 162 279 3235 297 303 235 317 297 155 692 494 82 179 192 734 178 103 204 87 225 63 351 225 260 282 352 367 219 77 182 384 386 147 340 273 260 234 61 251 141 663 194 320 94 144 ]
@@@ Loss per-class: [ 0.234851 0.699021 0.975028 0.596876 0.728465 0.905181 0.996869 0.664527 0.869649 1.17573 0.57479 1.15797 0.904064 0.762911 1.37605 0.674224 0.745661 0.994255 1.10996 0.878766 0.640043 0.894608 0.959204 0.644889 0.40642 0.771234 0.483065 1.81328 0.777405 1.37074 0.897654 0.437183 0.879362 1.18823 0.449419 0.694612 1.34119 1.32734 0.979879 0.784999 0.357586 0.985285 0.982682 0.821496 1.06245 0.839891 0.457722 1.23098 0.702217 0.522694 0.92039 1.26405 0.919349 1.2159 0.898179 0.915905 0.627165 0.525767 0.43622 1.0893 1.06812 0.825709 0.292254 0.330545 1.52543 1.24962 1.29988 1.01135 0.81856 0.518343 0.598394 0.480078 0.444221 0.90231 0.916476 1.12081 0.333856 0.256604 0.538746 0.648653 0.784928 0.904034 0.405642 0.596339 0.816921 1.01797 1.29025 1.04947 0.843395 0.616717 0.510278 0.597652 0.677016 1.76418 1.20436 1.71687 0.627989 0.395799 0.615271 0.464969 1.62761 0.719464 1.33565 0.484115 0.415182 0.618335 1.20343 0.964349 0.687965 0.729289 1.01345 1.16944 1.4685 0.444223 1.36499 2.35702 1.17478 0.672885 1.59463 0.999464 1.07779 1.71471 1.27359 1.00517 0.608403 0.498512 0.911793 1.49306 1.09156 0.544331 0.273266 1.07761 0.571668 0.725585 1.59695 0.435113 0.818863 0.367642 1.0229 1.16354 1.17947 0.747149 0.67089 0.641015 0.509422 0.840547 1.09986 0.709131 0.621107 0.490835 0.232205 0.907644 0.765107 1.38318 1.03128 0.581825 0.318745 0.523527 0.65525 0.373574 0.582998 1.06403 1.35147 2.99232 0.857377 1.24172 0.71841 1.10989 0.572737 1.37738 0.423556 1.02739 0.76738 0.891466 1.17114 0.592839 0.742552 0.67117 1.62076 0.288947 1.01131 1.07289 0.844932 0.837888 0.214473 0.993966 1.6915 1.16527 1.73142 0.646663 0.644239 0.968098 0.376352 0.581155 0.901391 0.997515 1.402 1.17072 1.19623 0.574319 1.57516 1.49337 1.17793 1.83985 1.55554 0.804142 1.74418 0.80832 0.830544 1.17293 1.00599 1.05024 0.647444 11.6428 1.37003 0.609556 1.21828 0.576045 0.55348 1.29287 0.834024 0.553873 1.56876 0.780061 0.97889 1.24425 1.86386 0.554385 0.703569 0.676537 1.0145 0.733396 0.742499 2.07628 0.715715 1.59236 0.528137 1.83951 1.02598 0.857041 0.64754 0.409441 0.864105 0.782542 1.10084 2.21967 0.624342 3.4308 1.52388 1.19993 2.14357 0.956564 1.073 1.13918 1.11876 0.634247 1.8842 0.770469 0.652862 0.472739 0.785519 1.22667 1.07793 2.03052 0.710731 0.633167 1.51561 1.14569 1.3823 1.10641 0.85148 0.979109 1.77482 1.54103 2.1832 1.25872 0.777439 1.98256 0.828848 1.57445 0.96872 1.06006 0.961808 0.606095 0.815309 1.44531 1.64499 0.864056 5.0038 1.91687 1.4094 1.6879 0.503032 0.319028 1.62244 0.901989 0.865773 0.658867 0.966933 1.25992 0.586124 0.751011 0.988547 0.738529 0.26433 0.627019 1.48651 1.03679 1.1541 1.79465 1.04178 1.11664 0.865157 0.910792 1.32644 1.62426 0.483937 1.10696 0.440008 1.0911 1.6755 0.701964 1.93444 0.451948 1.22317 2.16114 1.4057 2.50537 1.13592 0.743163 0.826093 1.28636 1.41987 0.925379 1.16748 0.571553 1.57314 1.40549 1.14581 0.715658 0.945068 0.861055 0.608829 0.867883 1.01081 0.854809 1.03709 1.1327 0.825659 1.23223 0.959565 1.09416 0.384189 0.225845 1.13552 0.903486 0.894775 1.09965 1.52927 0.358404 0.68313 1.8027 1.12249 2.24654 0.565307 1.49372 0.462918 1.06047 0.640336 0.85167 1.29804 1.37065 0.631977 1.04946 0.903765 0.580946 0.751982 0.699313 0.640793 0.967092 0.95385 0.953828 0.530263 0.754848 0.913403 1.11513 0.806951 1.11073 0.920654 1.26445 1.04341 1.13197 1.03716 0.589561 0.495389 0.897251 1.20259 0.894437 1.26092 0.679228 2.01157 0.546116 0.997995 0.814026 1.49794 1.04895 0.960607 1.17821 0.986424 1.14607 2.2122 1.8533 0.865911 1.01677 1.11674 0.624215 1.09565 0.485825 2.02401 0.82673 0.435413 0.623459 0.624059 0.529275 0.742007 1.08559 0.240899 0.465545 1.64235 0.667447 0.698954 1.57121 1.27618 1.22206 0.820488 0.571702 0.619158 0.826539 0.803335 0.931567 0.466879 1.3534 0.378886 1.25093 1.05686 0.709857 1.79292 0.863108 1.14371 0.31425 0.827162 1.32539 1.06087 0.515521 0.805491 3.71766 0.467396 0.922168 0.724896 1.1878 0.579031 0.899145 1.44218 0.982762 1.08488 1.81745 1.15189 0.982467 1.06415 1.09042 0.515797 0.391809 0.949418 0.590858 0.896565 0.47835 0.512637 0.705434 0.936322 0.994536 2.77739 1.49183 0.758676 1.10518 0.802603 0.404391 1.37125 0.982155 0.374942 1.21447 0.962963 1.14301 2.06455 1.48634 0.612294 0.530349 0.649145 1.18751 1.87389 0.612493 1.34989 0.921497 1.00514 1.57396 0.434385 0.959691 1.81662 0.557489 1.93668 0.816456 1.82834 0.438562 1.41615 0.82254 0.823529 1.16317 1.39135 0.690654 2.01212 1.30713 2.16454 1.04952 0.911101 2.46755 1.58688 0.938213 0.405169 1.4354 1.10326 2.06647 1.74424 1.88818 1.05571 0.805176 1.21948 1.57818 0.8587 0.98996 1.05475 1.61384 1.12237 1.22498 1.25589 1.39334 1.40814 0.156585 0.697515 0.993491 0.678342 0.843123 1.50571 2.51548 0.99241 0.626669 1.43298 0.92183 1.14064 1.5303 1.59042 1.66858 2.20847 1.42795 1.27773 0.950431 0.535608 0.617989 0.497125 0.784037 1.26985 1.21705 1.18264 0.914521 0.735278 0.698115 1.23733 1.72285 0.812601 0.711393 1.45441 1.00537 0.71169 1.81894 1.103 2.51087 0.916206 1.09908 0.532967 1.40912 1.1627 1.18194 0.597532 0.837077 0.625519 0.499803 1.47637 1.73442 2.1253 1.26209 1.81791 0.822081 1.51341 1.81041 2.01479 1.84738 0.989833 0.796279 0.661727 1.45673 0.355502 1.17274 0.778494 0.752954 1.10471 0.774293 0.89641 1.38136 1.85938 0.985811 0.931592 1.79002 1.39075 1.06286 1.22579 0.685641 0.894641 0.661822 1.11742 1.54467 0.987738 1.20167 2.26013 2.22006 1.0644 1.33576 0.623875 0.769993 0.63148 1.28862 1.05676 1.18662 0.564986 0.74136 0.590629 0.636788 0.691446 0.394267 1.59711 0.887043 0.653546 0.745183 3.8313 0.442233 1.58862 0.730747 1.35365 1.42786 0.763851 1.03232 3.80336 1.30781 0.842365 1.0142 0.50902 0.659305 0.689437 1.42763 1.47914 0.771975 1.3046 0.524193 4.57982 0.779778 0.545777 0.961162 2.01143 2.10966 0.768638 0.977378 1.69044 2.3359 1.60841 0.810363 0.995792 0.933321 0.603551 1.24963 1.21743 0.773259 0.647416 1.58272 1.61372 0.830588 1.3384 0.65132 1.32367 1.19052 1.48686 2.06245 2.03122 1.02295 0.700059 0.490592 0.797996 1.04485 1.00575 1.03188 0.928381 1.15348 0.610292 1.00459 2.7547 0.6705 0.343287 0.651383 0.381243 0.682483 1.14866 0.932777 0.773158 2.51051 1.70173 1.02008 1.03065 1.12186 1.09631 2.01944 1.3763 1.59116 1.22236 0.613471 0.808974 0.765868 1.10258 1.62409 1.24848 0.947052 1.43662 1.0715 1.40466 0.480842 1.1374 1.00397 0.681792 0.696901 0.8604 0.831481 0.88259 2.38728 1.0003 1.2366 0.926346 1.17265 1.64395 1.13743 1.3987 2.09075 1.32607 2.4581 1.7326 1.0646 1.33635 0.4467 1.64223 1.56823 1.98808 1.3827 1.28331 0.403741 1.32887 1.06251 0.851232 1.03322 0.91154 0.714102 1.12193 1.26085 1.82317 2.05458 0.986109 0.79635 0.762768 0.665549 1.22877 0.896632 2.10893 0.360735 0.613339 1.22067 1.06001 0.65858 0.919388 2.24839 0.661828 0.934617 2.48411 0.432126 1.22918 1.64549 1.06343 0.942386 1.77342 3.93338 1.15924 0.19268 0.486521 1.42888 0.521248 1.76367 1.03247 1.91296 0.868724 0.667866 3.50421 0.77249 0.987021 1.00181 1.26602 2.51619 1.7482 2.22309 0.562359 0.849526 1.3702 1.57263 1.05414 0.695297 2.29704 1.34899 1.11066 1.01523 0.851996 2.35825 1.31336 0.878354 1.40631 0.850655 1.05029 1.39882 1.18067 0.660649 2.44734 1.19462 0.57755 0.800647 0.861397 0.809351 1.50709 1.45794 1.3272 1.16038 1.59861 1.45876 1.75095 1.38299 0.830551 1.04589 0.397606 0.903923 1.4807 1.23998 1.3621 0.754454 1.01126 0.79943 1.76204 0.805606 0.933207 1.1896 1.48934 0.459833 1.15582 1.23825 0.788423 1.22838 1.54155 1.85525 0.986112 0.615217 0.917919 1.22898 0.74 0.573126 0.675364 2.16226 1.5498 1.24533 1.16112 1.58137 0.562492 0.926442 0.7412 0.81105 1.16733 0.819482 1.39697 0.58746 1.41009 0.732418 1.22589 1.2741 1.36254 0.831157 1.51482 0.817212 1.56667 0.692019 0.769778 0.599869 0.477934 0.962295 2.53806 0.750412 0.762831 3.13611 0.9021 1.79168 1.21225 0.805342 1.17912 1.12154 0.790429 1.38363 3.25524 1.43174 1.14681 0.994169 0.836218 1.2074 1.6048 1.71054 0.682623 0.821189 0.927655 1.57738 1.6951 0.985271 1.35275 1.09294 1.45479 1.81227 0.952293 0.775713 1.24496 1.56882 1.07598 1.29671 1.63463 0.851116 1.76144 0.844914 0.764456 0.652493 0.83957 1.27087 0.709385 0.579789 1.03525 1.11802 1.53714 1.17319 1.76448 1.331 1.98074 0.981811 1.42787 0.885304 2.63405 1.21846 0.697776 0.938192 1.06103 0.848893 1.27143 1.47231 1.37797 1.65063 0.964193 1.05215 1.56335 0.878432 0.519484 1.04116 0.49355 0.566138 1.2457 0.917519 0.67408 0.918349 1.06519 0.913998 0.94642 0.731953 1.34422 1.0944 1.00699 0.5898 0.96364 0.770535 0.87853 0.96878 1.44266 1.02708 0.607443 0.8848 0.82624 1.06587 0.809492 1.06818 0.904131 1.74139 1.69297 1.26989 0.895149 0.6769 1.29413 1.26651 1.46172 0.836042 1.56308 1.04209 0.439791 0.577921 1.12732 0.42619 0.961902 0.626927 2.05217 1.42 0.939058 1.60103 1.86999 0.933181 1.14069 1.02693 1.28508 2.08001 1.82618 1.37531 1.53107 1.26464 1.5001 1.42368 1.3631 1.69051 0.450508 1.88904 0.603501 1.90636 0.682164 1.89412 1.15483 1.90793 1.6391 1.48721 1.39249 0.356836 1.98754 0.736775 1.5579 1.27382 1.27427 2.54555 0.652563 1.6503 1.3552 0.879238 1.14312 0.719118 0.949558 0.657725 2.08584 0.927859 1.15296 1.27211 0.723755 1.33838 1.14888 1.17556 1.09446 1.79841 1.27222 1.69792 1.72838 1.61403 1.05157 0.477348 0.636404 1.07711 1.80766 0.634179 1.55261 1.71339 1.73785 0.997885 1.12664 0.821776 1.40339 0.581353 0.907363 0.74825 1.02955 1.05149 1.00043 1.21043 0.760239 1.48605 0.90677 1.37448 0.593262 0.588396 0.955061 1.72594 2.38581 1.45145 1.25066 1.65498 1.18108 0.602688 1.69948 0.689726 0.953226 0.910977 1.12443 1.77533 0.893292 0.9874 0.513735 1.16556 1.65911 0.496318 1.07103 1.06983 1.57721 0.903404 1.44421 0.904429 1.61837 2.34173 1.31115 1.28677 1.3152 0.307109 0.29874 0.666096 1.34965 1.37159 1.55797 1.20967 1.11704 1.19333 0.640623 1.0565 1.04044 0.854739 0.696407 2.76283 1.12361 0.717558 0.687343 1.26267 1.41153 1.46573 0.805722 1.9833 1.30659 0.715923 0.678794 0.517415 1.19463 1.05021 0.5383 0.747766 1.00412 0.690718 0.555528 1.37627 0.912269 1.10565 0.663958 0.754454 0.815133 0.423264 1.30363 1.11863 0.767009 0.504953 1.46356 1.04723 1.14637 1.25685 1.00278 1.42255 1.60095 0.46957 2.0855 0.419085 0.724029 1.19137 0.996261 1.32389 0.512908 0.79459 1.50521 1.45262 1.1295 2.29319 1.46991 0.744979 1.27071 1.34271 1.94876 1.18984 1.93551 0.804084 1.60023 1.9593 1.11092 1.06906 0.967084 0.588202 1.0587 1.46674 1.75882 0.687745 1.13556 1.80651 7.57757 1.0438 0.799345 1.58225 1.58481 1.11713 1.71053 1.86649 0.951586 0.52301 1.31465 0.619627 0.89357 1.38608 0.940049 1.56744 2.05098 1.48009 0.97009 1.31628 1.8406 2.0867 1.61789 0.570216 1.14415 1.07911 1.32034 1.83456 1.01712 3.00138 1.21864 1.37682 1.15973 1.31688 0.613419 1.08202 0.693852 1.87035 0.724735 0.577436 1.02816 1.24052 1.12186 0.91681 1.6405 0.873253 0.806946 0.909655 1.65158 0.668299 0.95088 1.55688 1.00535 1.22804 ]
@@@ Frame-accuracy per-class: [ 91.5472 81.6609 71.3725 83.9242 80 71.2644 72.6225 77.6903 71.387 60.5544 85.9206 68.0851 76.1229 79.2112 46.1538 81.8653 77.3481 68.1223 63.3557 68.4039 83.8535 67.7871 69.9187 81.8482 90.9091 79.0204 88.1175 34.9296 78.5374 57.4018 71.2531 85.5457 74.9354 55.6017 86.1538 79.8859 52.1224 52.7363 72.9825 71.1136 87.1379 70.5502 68.6084 72.8551 61.4311 72.1382 84.8 59.2908 76.6269 86.1806 66.6667 59.5691 69.3666 61.0526 77.8055 68.425 76.0482 84.7529 87.3558 61.1435 66.0194 71.0448 90.5665 89.6624 57.754 47.2885 56.7657 70.6935 77.3232 86.2155 81.296 85.7143 87.9227 70.6309 74.184 69.1589 89.4309 94.4771 83.3127 87.4576 77.2414 75.1334 90.3388 84.0108 78.5649 65.9794 72.4638 70.903 72.866 90.9091 83.7716 84.1812 80.9274 40.273 63.8966 51.9878 82.0084 87.8989 81.3891 87.6377 48.0962 80.8194 50.2415 87.2908 89.0715 80 67.6259 70.691 78.835 76.9231 64.7173 66.3073 59.5271 86.0024 58.006 27.3381 62.2735 91.2752 50.5051 68.3507 70.7246 58.3051 68.0851 64.5914 79.8196 82.9388 78.2979 58.1673 74.2857 82.7804 91.1657 65.2568 78.8871 79.0252 51.8519 88.7395 76.8539 87.1949 73.3813 63.0072 56.2319 76.1905 81.6438 78.1533 88.972 81.4815 69.7848 83.4621 77.7896 81.4196 93.3677 74.028 78.8923 48.468 69.6462 80.2984 90.6047 86.6841 79.3508 86.6076 81.8391 70.2842 62.716 17.1206 73.724 59.7938 79.646 72.7915 84.2337 53.0184 87.0679 70.0201 72.4954 73.8609 68.8427 80.8511 76.2475 78.6207 48.3516 90.5706 74.4186 65.2278 73.8255 71.3693 94.7186 66.3342 51.0067 60.5187 49.1979 80.597 83.2536 68.0713 90.8379 81.9459 69.9752 69.3111 48 64.3478 65.5462 83.1683 46.5961 47.4041 63.1829 42.3358 47.7419 75.6184 40.6439 75.8708 78.0488 67.1937 73.9884 63.5481 80.0959 0 59.4268 83.6251 61.7801 84.8322 81.0235 62.1005 74.8235 84.4607 55.1959 76.9811 79.5181 52.4917 36.5217 87.8049 78.3862 74.447 73.3638 75.4982 77.6553 35.2941 80.2444 51.4286 83.224 37.2414 60.076 73.823 81.0631 83.9975 75.6164 73.007 64.6681 15.8273 84.2667 21.3333 53.2688 64.351 37.4269 73.6 70.4132 69.5864 68.5298 81.8942 52.0646 74.856 81.137 88.6775 75.6047 60.7774 69.6707 54.4803 79.4547 82.3082 58.9327 63.5897 46.5116 76.1099 73.1261 64.5445 48.6486 48.2535 36.4508 58.2677 78.8804 41.3793 75.5187 50.5109 73.3955 68.7651 62.8571 82.3173 70.1299 59.3857 56.2738 79.2793 0 52.3702 60.198 48.6228 85.676 94.6947 52.5529 74.9465 76.8802 79.8535 77.2603 61.661 82.6575 79.7927 67.5462 75.1491 91.6414 83.3028 66.6667 65.4275 63.9511 41.3187 70.5394 65.6891 70.8471 67.6259 62.4113 50.5929 86.0215 69.8517 88.8097 72.6257 45.3782 82.2857 43.0939 87.2162 48.1633 28.4006 52.3894 40.4372 66.1088 81.2749 73.7813 58.8235 57.5198 75.5556 63.6086 80.7413 53.112 63.8821 65.4788 78.6967 71.3287 72.8702 86.7052 74.7475 63.354 75.9596 69.8413 61.3757 77.7293 62.7451 70.9552 64.3875 88.4113 93.4602 67.8996 74.2942 64.9446 72.5979 57.9186 88.9722 79.9347 51.0519 62.0865 31.941 86.7925 49.1803 88.4876 71.0212 78.3217 79.5987 57.4202 61.0123 78.7472 70.3214 68.431 86.6426 79.4457 78.0211 80.4548 71.4967 72.2154 66.5245 83.6915 72.6368 67.1105 69.1943 76.6889 54.6584 74.2698 65.9898 65.0602 67.9707 73.9903 77.3694 88.0416 70.5061 64.4628 76.2431 60.0985 77.7989 24.2991 89.5464 69.8507 75.8815 57.1429 70.4871 67.8663 63.5324 70.4055 65.5207 40.2286 50.8227 75.7895 74.4186 63.9498 86.9757 68.3983 85.9027 36.7758 75.1468 87.7089 76.1905 79.646 87.0279 80 74.6269 94.6508 82.7068 44.9339 79.1569 79.3311 54.2056 60.6316 64.9895 77.0093 80.9843 83.1169 78.4314 76.5321 76.2447 86.2314 63.6704 87.3598 63.8219 66.4918 80.3941 31.2925 73.0435 68.9788 89.8888 71.8193 44.6281 72.5275 83.4251 78.8845 0 84.8875 72.9384 72.3618 63.6656 80.9986 73.3017 51.2338 65.7829 66.8942 44.9799 62.6866 69.3587 70.9677 68.2927 83.9858 90.4986 69.3241 85.5548 75.0442 87.9257 83.94 79.8898 71.8169 65.5602 32.2097 56.9072 81.1808 71.8447 77.0318 92.3767 58.9831 74.4526 90.4805 63.6257 69.962 62.3116 28.3105 56.1798 80.8081 80.2392 78.7286 65.1934 43.5424 79.3774 55.7769 70.4636 65.5602 54.2282 88.3204 69.1404 36.8664 83.2287 37.3626 75.2228 61.0526 88.0134 59.1304 70.8861 75.6554 70.7819 60.4317 75.7528 36.1757 61.9355 36.8601 71.5026 72.0379 46.6513 52.1964 66.4714 88.0498 54.1226 72.8929 61.6114 45.988 32.8767 62.9247 71.6185 62.069 64.8172 72.6761 74.2317 67.148 41.1523 59.8911 63.388 60.2871 59.6401 58.2524 96.5899 78.2232 67.8211 85.5696 74.168 47.7366 30.137 74.9156 82.1853 51.1628 65.6827 60.355 60.9524 50.7772 49.7354 23.8361 58.6371 64.3599 69.3333 82.4473 82.3529 85.6536 78.6632 61.6352 69.967 71.3287 71.1033 76.6789 79.11 61.0272 55.2204 76.8274 81.0289 46.0432 72.5537 76.5158 51.9149 69.4611 26.8551 71.0963 64.7462 79.8328 51.8847 59.8407 63.8037 89.3506 79.2332 80.0259 84.4702 52.8548 43.5318 56.8889 61.4334 49.6314 75.1745 55.706 50.8711 30.6383 47.9482 68.2464 81.7891 82.6347 57.9439 89.4344 69.7248 75.2549 82.9216 64.5892 82.4034 70.4403 59.6825 41.3695 81.0036 71.6763 48.1262 51.4469 71.9212 67.9245 77.6642 70.6016 79.5069 64.9077 54.5455 60.5863 68.1614 32.2078 41.5869 71.4628 59.0051 83.4915 83.0918 86.0215 57.2491 67.6423 64.5963 86.6298 76.8549 79.0107 84.7458 75.5228 95.0673 56.9921 72.6714 81.8818 76.5725 8.09249 86.3241 49.2462 81.8301 60.5081 53.7102 83.9945 64.8649 9.16031 60.7735 69.3333 66.899 85.1913 78.1014 80.7279 51.7172 62.8931 79.8092 62.8478 85.8684 2.1978 77.0855 83.9248 66.8684 39.3736 38.8521 76.9524 68.4864 46.729 33.6 51.0264 76.4835 73.4982 80.663 88.3978 54.2636 72.9927 77.1084 81.4087 40.1656 51.1182 72.8767 53.5922 82.8508 55.8952 53.2663 50.1193 31.9328 24.6154 69.2506 80.2292 86.6242 75.4717 73.0897 63.9676 63.8574 76.8642 70.5882 83.8475 70.1754 25.1121 75.9085 91.2964 82.1601 88.8755 80.5873 70.6827 71.4451 70.8571 32.4476 47.8284 71.0602 69.9237 69.6231 55.2413 13.6986 55.1181 61.2844 65.5602 78.6611 75.1896 74.9686 68.3243 56.77 50.237 67.7188 56.4565 67.3496 55.4273 90.4899 68.3371 67.1785 82.5175 85.2368 74.2747 69.8487 71.464 31.7757 68.0702 69.3957 74.5387 71.0744 45.1362 67.4352 41.5094 48.5876 54.3081 29.9401 51.2397 61.7363 62.3955 88.056 43.8247 56.1798 27.6762 53.3623 60.7562 88.4364 57.2748 68.7379 78.0715 72.3769 73.0539 80.9877 60.0094 66.5693 41.2214 36.0856 72.0322 76.7372 75.6364 81.4815 68.2043 76.5217 47.3896 89.393 83.5959 60.3774 69.168 82.9582 74.364 49.162 80.2488 72.9124 36.7347 90.1135 66.9584 52.4173 71.1688 70.1299 40.7932 29.3217 68.9655 93.6274 83.2801 57.7778 86.8081 51.7895 73.6353 49.0141 72.7273 79.8206 17.5439 79.4745 63.2087 69.1282 64.01 43.4783 49.8599 26.1307 78.6948 78.4314 58.0858 53.9235 68.8337 77.2801 41.1067 57.1429 68.9805 68.8091 75.3623 42.7119 63.9456 72.6688 55.1971 72.2351 63.7813 62.5298 64.803 79.9089 17.5439 67.2897 80.1922 78.2824 69.5327 79.7721 57.6112 55.6355 56.5916 57.6577 53.494 56.0411 36.4683 55.4017 79.2627 68.2513 88.7995 71.7909 55.4217 61.5711 59.3407 75.0968 66.6667 74.9556 47.5362 73.6381 72.2995 67.8363 63.5379 88.6297 68.2464 63.9366 72.0737 54.6835 59.3548 48.1203 65.0888 81.9034 72.6555 62.7871 75.9076 89.7884 82.2018 42.1053 46.6472 60.6186 58.3851 47.5196 82.9721 71.4026 78.1609 84.2105 68.4327 79.5632 52.549 83.2998 52.8736 76.4479 64.2706 62.955 63.3947 77.594 49.3274 76.0994 48.3461 81.9473 80.292 84.03 87.9152 72.2662 24.5734 76.7036 78.9381 29.5082 73.6842 51.1166 69.7068 72.2821 62.0555 65.3501 77.2118 66.0348 19.7802 58.5576 74.1784 72.9858 78.6156 56.5584 51.7019 56.5657 85.4369 74.4751 70.0557 55.5957 44.2997 71.487 59.624 62.3399 61.8076 44.7489 70.7189 81.0556 63.6569 51.4469 69.7819 61.9195 52.0179 76.8448 35.7414 75.549 76.8964 84.1202 74.4695 67.6329 75.846 83.1354 71.4104 70.1571 52.975 63.6569 45.6853 64.8649 44.9298 71.4424 59.2992 69.7436 23.7154 62.8821 80.203 70.5302 66.1738 76.22 57.1429 55.8304 52.0376 46.7974 73.5154 67.7608 54.6535 71.5219 86.015 71.5328 80.7186 79.583 66.1088 69.4017 80 75.3463 69.7674 75.8782 73.9447 79.602 54.3689 71.2446 65.4683 82.2719 70.2259 77.9392 73.8037 71.6628 55.5347 66.8113 82.9268 75.2827 75.9232 68.2779 77.3498 65.8188 73.964 51.5298 51.1785 61.2859 73.1707 76.9874 61.8026 58.6466 57.0888 73.8574 60.1361 72.7273 83.0503 81.4988 61.32 89.3219 72.1311 80.5112 48.9796 60.3129 70.7424 51.7375 32.4873 72.1144 66.5105 76.9231 64.9418 37.7682 48.3221 58.2878 56.7089 59.5104 50.7205 59.4595 57.3134 52.8202 86.5248 46.62 79.7546 38.7435 76.3563 45.7831 68.2635 44.5344 49.6084 57.2127 56.2942 90.1085 26.087 79.1946 56.7657 56.8779 62.3468 20.1005 81.4196 51.1211 58.3691 76.5172 66.6667 76.9614 76.3562 78.4131 35.2332 73.0838 67.6056 54.8559 75.8144 61.2766 64.6749 65.9271 66.3623 46.332 62.7803 48.996 49.148 54.9906 71.4579 87.3533 79.702 64.6617 44.1026 82.824 54.0717 46.5011 40.1434 65.1341 69.9229 77.3838 58.7859 84.6512 73.0795 72.6979 70.0767 60.9495 67.9969 64.8649 80.1619 55.6522 69.7795 59.735 83.8217 84.0295 77.8325 52.9058 12.766 57.2283 66.5639 52.7571 62.5821 87.1795 38.7409 80.1702 65.1204 72.8111 64.08 42.885 67.3031 66.4495 88.4273 55.6901 34.981 89.4866 72.2581 67.8414 53.3666 70.6503 57.3066 70.3151 53.012 32.5581 60.3175 59.2593 62.3482 91.3341 92.142 81.2199 54.5455 61.9608 44.6886 59.3301 59.1692 59.5438 80.9249 71.4976 64.0835 72.969 79.1753 21.0526 66.6667 77.0403 80.3456 66.3212 61.4065 66.0436 74.3764 43.6906 60.4811 78.1057 80.0436 80.7966 67.319 74.1797 82.066 74.9091 67.7291 82.0587 81.6911 65.1489 67.2686 70.9677 81.3278 78.6127 74.6133 87.5134 54.2778 68.7915 75.5187 83.1106 57.0842 72.3005 73.1915 63.8581 67.957 52.5346 49.7577 84.9331 35.2423 91.6155 77.2852 65.3928 72.1915 63.7376 86.3301 77.2277 61.191 57.5592 69.4981 34.2541 53.5666 82.6958 66.8555 56.4498 50.2326 61.4493 47.6868 74.2029 39.3682 37.3832 67.9512 66.4407 69.2927 80.6934 66.4151 63.8743 52.6112 80 60.4423 41.1429 0 68.4492 78.4416 51.8359 54.6139 62.9602 47.2398 42.9907 74.0576 92.9231 58.3184 81.2239 73.9496 54.0362 72.6115 56.6929 50.084 57.2347 71.6245 53.3873 50.9091 43.454 45.7143 84.275 61.0644 69.5652 62.1027 37.7143 67.8492 11.0236 64.5804 66.5188 69.8656 61.9469 81.9858 72.1088 82.0046 38.7097 76.1644 85.3056 67.2704 57.6271 66.6667 66.1792 47.6008 75.9062 86.1789 69.5825 48.7633 80.4823 67.8663 56.1622 69.8413 65.0519 ]

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:347) Done 2622 files, 1 with no tgt_mats, 0 with other errors. [TRAINING, 0.342219 min, fps36772.9]
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:353) AvgLoss: 0.668621 (Xent), [AvgXent: 0.668621, AvgTargetEnt: 0]
progress: []
FRAME_ACCURACY >> 79.0955% <<

