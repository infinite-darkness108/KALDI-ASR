speech-HP-Z2-Tower-G9
nnet-train-multistream-perutt --cross-validate=true --randomize=false --verbose=0 --num-streams=10 --max-frames=15000 --feature-transform=exp_FG/blstm4i/final.feature_transform 'ark:copy-feats scp:exp_FG/blstm4i/cv.scp ark:- | apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_cv10/utt2spk scp:data-fbank/train_cv10/cmvn.scp ark:- ark:- | add-deltas --delta-order=2 ark:- ark:- |' 'ark:ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- |' exp_FG/blstm4i/nnet.init 
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:243) Not in compute-exclusive mode.  Suggestion: use 'nvidia-smi -c 3' to set compute exclusive mode
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:438) Selecting from 1 GPUs
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:453) cudaSetDevice(0): NVIDIA RTX A2000 12GB	free:11506M, used:524M, total:12031M, free/total:0.956436
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:501) Device: 0, mem_ratio: 0.956436
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:382) Trying to select device: 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:511) Success selecting device 0 free mem ratio: 0.956436
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:FinalizeActiveGpu():cu-device.cc:338) The active GPU is [0]: NVIDIA RTX A2000 12GB	free:10992M, used:1038M, total:12031M, free/total:0.913713 version 8.6
copy-feats scp:exp_FG/blstm4i/cv.scp ark:- 
add-deltas --delta-order=2 ark:- ark:- 
apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_cv10/utt2spk scp:data-fbank/train_cv10/cmvn.scp ark:- ark:- 
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:144) CROSS-VALIDATION STARTED
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Read():nnet/nnet-matrix-buffer.h:191) Read() started... Buffer size in MB: 0, max 3072, having 0 utterances.
ali-to-post ark:- ark:- 
ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl 'ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |' ark:- 
LOG (copy-feats[5.5.1074~1-71f3]:main():copy-feats.cc:143) Copied 296 feature matrices.
LOG (apply-cmvn[5.5.1074~1-71f3]:main():apply-cmvn.cc:159) Applied cepstral mean and variance normalization to 296 utterances, errors on 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:303) ### After 0 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:304) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.099996, max 0.0999941, mean 0.00010609, stddev 0.0578148, skewness -0.00337789, kurtosis -1.2066 ) 
  f_w_gifo_r_   ( min -0.0999999, max 0.0999987, mean -0.00010502, stddev 0.0576977, skewness 0.00133469, kurtosis -1.2007 ) 
  f_bias_   ( min -0.0999597, max 1.09993, mean 0.250171, stddev 0.43762, skewness 1.12302, kurtosis -0.646775 ) 
  f_peephole_i_c_   ( min -0.0991568, max 0.0985223, mean -0.00207917, stddev 0.0567129, skewness 0.0656165, kurtosis -1.11718 ) 
  f_peephole_f_c_   ( min -0.0990468, max 0.0999617, mean -0.000164859, stddev 0.0587502, skewness -0.0735029, kurtosis -1.20837 ) 
  f_peephole_o_c_   ( min -0.0998017, max 0.0994346, mean -0.00266001, stddev 0.0565345, skewness -0.0560794, kurtosis -1.16416 ) 
  f_w_r_m_   ( min -0.0999957, max 0.0999975, mean -7.01415e-05, stddev 0.0576926, skewness -0.000368113, kurtosis -1.19622 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -0.0999979, max 0.099998, mean 0.000174113, stddev 0.0576932, skewness -0.00299439, kurtosis -1.19799 ) 
  b_w_gifo_r_   ( min -0.0999997, max 0.0999994, mean -0.000119637, stddev 0.057754, skewness 0.00184605, kurtosis -1.19986 ) 
  b_bias_   ( min -0.0997427, max 1.09803, mean 0.247412, stddev 0.436501, skewness 1.12364, kurtosis -0.644405 ) 
  b_peephole_i_c_   ( min -0.0984194, max 0.0999497, mean 0.00087609, stddev 0.058852, skewness -0.0020349, kurtosis -1.23052 ) 
  b_peephole_f_c_   ( min -0.0993016, max 0.0992842, mean 0.000403948, stddev 0.0602817, skewness 0.014258, kurtosis -1.32415 ) 
  b_peephole_o_c_   ( min -0.0982414, max 0.0997063, mean 0.00132737, stddev 0.0540875, skewness -0.192741, kurtosis -0.978889 ) 
  b_w_r_m_   ( min -0.0999991, max 0.0999941, mean -0.000341081, stddev 0.057745, skewness 0.0040682, kurtosis -1.19955 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.493034, max 0.460515, mean -0.000155925, stddev 0.0999989, skewness 0.000777382, kurtosis 0.0159626 ) , lr-coef 1, max-norm 0
  bias ( min 0, max 0, mean 0, stddev 0, skewness -nan, kurtosis -nan ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:305) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -11.004, max 12.2048, mean 0.00682255, stddev 0.969149, skewness 0.287893, kurtosis 3.83541 ) 
[1] output of <BlstmProjected> ( min -1.37477, max 1.30359, mean -9.09425e-05, stddev 0.274117, skewness 0.00451224, kurtosis 0.193652 ) 
[2] output of <Tanh> ( min -0.879775, max 0.862645, mean -0.000106411, stddev 0.255503, skewness 0.00252559, kurtosis -0.296833 ) 
[3] output of <AffineTransform> ( min -3.23722, max 3.48163, mean -0.000788937, stddev 0.649102, skewness 0.00473783, kurtosis 0.222908 ) 
[4] output of <Softmax> ( min 2.38042e-05, max 0.0191937, mean 0.000781466, stddev 0.000577723, skewness 3.19662, kurtosis 25.4023 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:334) ### After 79212 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:335) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.099996, max 0.0999941, mean 0.00010609, stddev 0.0578148, skewness -0.00337789, kurtosis -1.2066 ) 
  f_w_gifo_r_   ( min -0.0999999, max 0.0999987, mean -0.00010502, stddev 0.0576977, skewness 0.00133469, kurtosis -1.2007 ) 
  f_bias_   ( min -0.0999597, max 1.09993, mean 0.250171, stddev 0.43762, skewness 1.12302, kurtosis -0.646775 ) 
  f_peephole_i_c_   ( min -0.0991568, max 0.0985223, mean -0.00207917, stddev 0.0567129, skewness 0.0656165, kurtosis -1.11718 ) 
  f_peephole_f_c_   ( min -0.0990468, max 0.0999617, mean -0.000164859, stddev 0.0587502, skewness -0.0735029, kurtosis -1.20837 ) 
  f_peephole_o_c_   ( min -0.0998017, max 0.0994346, mean -0.00266001, stddev 0.0565345, skewness -0.0560794, kurtosis -1.16416 ) 
  f_w_r_m_   ( min -0.0999957, max 0.0999975, mean -7.01415e-05, stddev 0.0576926, skewness -0.000368113, kurtosis -1.19622 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -0.0999979, max 0.099998, mean 0.000174113, stddev 0.0576932, skewness -0.00299439, kurtosis -1.19799 ) 
  b_w_gifo_r_   ( min -0.0999997, max 0.0999994, mean -0.000119637, stddev 0.057754, skewness 0.00184605, kurtosis -1.19986 ) 
  b_bias_   ( min -0.0997427, max 1.09803, mean 0.247412, stddev 0.436501, skewness 1.12364, kurtosis -0.644405 ) 
  b_peephole_i_c_   ( min -0.0984194, max 0.0999497, mean 0.00087609, stddev 0.058852, skewness -0.0020349, kurtosis -1.23052 ) 
  b_peephole_f_c_   ( min -0.0993016, max 0.0992842, mean 0.000403948, stddev 0.0602817, skewness 0.014258, kurtosis -1.32415 ) 
  b_peephole_o_c_   ( min -0.0982414, max 0.0997063, mean 0.00132737, stddev 0.0540875, skewness -0.192741, kurtosis -0.978889 ) 
  b_w_r_m_   ( min -0.0999991, max 0.0999941, mean -0.000341081, stddev 0.057745, skewness 0.0040682, kurtosis -1.19955 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.493034, max 0.460515, mean -0.000155925, stddev 0.0999989, skewness 0.000777382, kurtosis 0.0159626 ) , lr-coef 1, max-norm 0
  bias ( min 0, max 0, mean 0, stddev 0, skewness -nan, kurtosis -nan ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:336) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -7.66434, max 7.26584, mean -0.0147177, stddev 0.961893, skewness 0.577064, kurtosis 2.62679 ) 
[1] output of <BlstmProjected> ( min -1.46489, max 1.31457, mean 0.000522918, stddev 0.28191, skewness 0.0072616, kurtosis 0.202637 ) 
[2] output of <Tanh> ( min -0.898598, max 0.865428, mean 0.000445369, stddev 0.2618, skewness 0.00436605, kurtosis -0.313156 ) 
[3] output of <AffineTransform> ( min -3.25341, max 3.55156, mean -0.00268995, stddev 0.666985, skewness 0.0151843, kurtosis 0.243729 ) 
[4] output of <Softmax> ( min 2.11843e-05, max 0.0211484, mean 0.000781218, stddev 0.000604907, skewness 3.67562, kurtosis 36.0947 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:346) PER-CLASS PERFORMANCE:
@@@ Frames per-class: [ 1670 30 8 79 45 6 24 17 70 220 22 36 34 39 10 53 43 120 22 20 34 66 38 35 87 73 52 35 43 17 86 33 19 32 16 75 106 10 5 1873 21251 22 63 135 22 30 80 98 44 43 9 67 66 27 41 51 107 54 285 40 48 24 308 222 21 35 16 33 248 15 54 49 139 81 14 4 45 35 47 26 17 66 4 14 44 25 18 49 111 11 94 64 72 34 66 27 32 33 78 42 15 34 9 135 37 103 31 64 37 21 67 0 93 69 23 2 34 10 32 37 25 34 18 19 28 43 46 13 4 26 88 25 22 43 33 62 12 98 32 25 22 78 11 62 24 13 88 37 45 30 117 35 60 38 37 129 16 23 64 982 29 24 10 20 31 33 34 16 52 44 64 25 22 25 3 9 30 11 42 35 13 12 11 8 32 32 9 20 32 48 32 42 56 36 35 41 27 16 5 29 35 31 12 7 9 45 27 81 21 16 19 72 29 0 46 70 68 37 26 33 50 37 32 9 5 26 23 15 14 128 52 51 59 20 20 8 73 5 17 32 22 16 4 39 60 11 16 7 19 20 8 7 52 22 37 15 32 12 39 94 72 13 59 17 69 35 25 11 13 18 58 26 24 119 16 14 9 5 13 35 18 40 17 83 10 20 20 32 21 19 33 104 48 60 79 47 20 14 27 49 56 7 20 34 199 37 4 3 21 22 15 27 39 6 10 11 22 25 47 21 6 14 6 60 3 43 26 14 8 10 70 4 15 29 11 133 11 35 22 51 21 57 14 109 15 36 29 5 21 20 18 19 114 55 69 35 21 21 64 44 39 43 15 21 37 31 46 70 32 25 50 48 101 37 32 62 34 53 184 59 33 24 47 16 35 9 79 14 45 12 20 16 36 46 8 52 28 13 12 31 17 29 40 53 13 20 9 73 85 128 108 21 7 44 30 32 12 46 15 32 90 31 20 32 52 21 237 45 11 21 215 49 25 30 31 29 71 13 57 48 98 46 27 41 19 60 21 4 38 48 50 5 63 71 0 0 25 47 11 16 42 44 49 71 27 15 49 22 22 11 30 67 41 56 15 28 24 35 7 14 15 34 7 8 38 0 9 28 57 50 12 13 39 28 40 746 35 13 19 19 18 27 12 16 52 60 21 82 1 33 49 89 6 38 42 18 51 39 26 11 19 7 23 28 18 51 98 35 37 10 34 11 56 273 7 39 31 18 21 15 24 22 15 25 6 21 53 59 24 25 15 4 49 28 14 9 8 3 8 29 34 0 8 18 51 52 48 23 3 60 16 24 35 46 16 52 37 72 3 38 59 15 5 40 14 54 52 32 51 4 14 21 187 40 33 16 15 18 19 52 32 28 37 29 41 52 10 6 95 18 46 53 17 8 5 18 31 15 14 27 7 8 17 7 93 28 14 64 22 15 8 48 40 29 32 15 10 26 38 92 13 54 242 3 84 0 22 55 226 137 6 121 19 57 20 16 55 85 4 17 33 53 42 81 83 26 30 36 55 64 9 66 84 47 14 13 91 43 19 13 23 37 17 19 27 24 0 59 88 26 15 20 111 16 50 7 32 22 5 17 15 28 38 22 27 28 48 52 41 28 16 114 105 37 112 43 4 69 17 87 95 11 45 15 22 24 24 65 3 27 29 73 63 47 17 34 18 53 17 37 17 20 6 14 36 78 20 8 14 23 47 6 6 22 3 11 89 3 5 18 20 146 25 8 25 44 29 28 22 26 34 24 73 34 85 42 37 35 79 40 39 34 32 23 18 104 65 9 45 19 21 17 48 42 21 40 28 25 27 73 23 34 2 186 119 39 90 27 23 23 38 48 0 28 66 71 45 13 24 7 25 19 16 23 24 129 50 34 21 69 32 14 26 16 14 79 10 15 35 83 24 21 61 28 128 42 66 19 12 25 28 26 60 26 36 30 128 60 8 15 24 59 47 2 19 63 42 22 4 29 30 49 65 12 8 20 32 71 48 32 17 59 28 24 21 20 22 26 24 15 35 0 17 35 13 29 14 47 49 7 13 6 17 31 19 38 15 53 143 34 68 37 23 6 34 29 7 46 92 30 48 31 3 30 16 40 24 44 69 43 19 65 170 29 5 28 134 73 20 11 82 86 29 13 21 12 10 24 12 81 163 12 193 6 28 39 45 31 23 36 65 11 39 113 30 11 28 125 45 52 34 87 20 29 16 97 42 97 26 45 39 15 109 53 18 59 52 33 29 52 31 45 25 45 136 11 24 33 61 26 28 29 106 30 33 18 48 56 86 92 23 37 23 14 2 14 36 346 138 60 357 25 129 51 39 29 11 53 15 14 13 44 38 41 41 8 18 32 16 20 21 17 8 96 16 23 156 45 91 21 6 43 48 25 45 29 8 22 23 31 42 3 198 10 14 29 25 39 64 39 11 26 28 39 96 13 53 5 34 9 11 21 67 28 24 50 19 10 11 22 23 15 13 30 30 176 37 84 80 124 21 74 32 81 43 23 25 68 52 83 27 20 0 33 56 14 23 30 22 132 113 9 71 18 15 16 5 38 61 32 70 35 44 36 82 12 19 25 56 12 24 164 178 24 42 24 19 11 41 67 112 11 27 62 39 5 26 82 58 18 39 25 15 60 20 122 54 35 24 51 28 24 13 180 152 39 59 14 42 123 70 441 14 35 36 425 22 33 13 41 32 48 41 161 16 18 110 56 36 262 66 33 47 21 15 29 84 58 48 47 2 22 14 22 35 120 98 20 59 274 17 15 22 15 59 16 0 27 31 40 47 63 69 16 68 32 39 426 46 55 29 28 47 14 42 42 14 17 28 110 23 16 27 12 36 25 38 21 57 45 37 40 25 7 21 46 54 29 44 49 16 46 13 48 16 103 37 35 12 34 ]
@@@ Loss per-class: [ 7.2768 7.74657 5.9533 5.82128 6.86413 6.48197 7.27857 7.30918 6.53263 6.56177 6.89243 7.41855 6.57087 7.44106 6.75682 6.44958 7.56995 7.25305 6.64926 6.86319 8.63003 7.34202 7.47601 8.40408 7.26026 7.68815 8.15131 7.96319 6.72543 6.65449 7.81878 7.29179 6.97522 6.8685 7.32704 7.66016 7.54981 6.66193 7.06168 7.51911 7.32268 7.30941 7.80254 7.31025 7.50135 7.59779 7.33813 7.50145 6.96386 7.01349 7.9226 8.16793 7.58861 6.67121 7.6709 8.29775 7.37421 7.98168 7.65619 8.04649 7.92938 7.79105 6.9757 6.71877 6.95247 7.52993 6.42634 8.10598 6.80994 6.48031 8.40199 7.40029 7.28938 6.46601 7.97875 7.23497 7.60476 6.52704 7.08175 8.10576 7.51786 7.89045 6.83934 7.32294 7.80917 6.79331 7.7349 7.80933 7.02714 8.06977 7.66878 7.63312 7.60153 8.0563 7.97932 7.17615 8.21913 8.51351 7.49212 7.44253 7.22347 7.8335 6.96168 7.39332 7.40523 6.96529 7.72563 7.32484 7.09899 5.92945 7.14058 0 7.46348 8.13213 7.3598 5.40324 8.0598 7.25528 7.49245 7.56953 7.26005 6.74705 7.5934 7.868 6.5378 7.55746 6.20809 6.81144 6.24615 6.45912 8.05372 7.78815 6.38815 7.05121 7.36006 6.05417 7.74929 7.89793 7.43324 7.52963 6.78512 6.81539 6.75341 7.42192 8.04784 6.67257 7.02725 6.97245 7.38412 7.19464 7.0537 7.65162 7.40389 7.91259 6.5545 6.93627 8.21129 6.95875 8.32731 7.91251 7.4746 7.92794 6.0726 7.60065 7.91937 6.76051 7.2163 7.64257 7.0439 6.94753 7.3407 6.71657 8.02357 7.09717 6.4408 8.12746 8.1768 6.5729 7.71878 6.38797 6.85411 5.67412 6.88199 6.46304 7.85625 8.06134 6.95784 8.12559 6.19245 7.3162 7.2684 6.52301 7.43787 7.90078 7.07845 7.0667 7.51792 6.84006 6.10826 6.97255 7.46226 6.88809 6.05679 7.13347 6.65417 6.97326 6.94449 7.35599 6.93973 7.60776 6.86402 7.60633 7.16653 0 6.75375 8.03958 7.14393 6.8484 6.73031 6.57796 7.55324 6.80373 7.18739 6.55793 6.98947 7.47167 7.62754 7.48811 7.25357 6.59299 7.18718 7.29528 7.3828 6.45253 7.91145 6.84957 6.83171 6.50813 6.73293 7.2785 6.61953 6.6127 7.10177 6.25419 8.18154 7.2777 7.31356 6.86353 8.19395 7.36992 6.97187 7.87127 5.96283 8.77021 6.63463 5.87605 6.00089 7.86986 5.70842 7.58502 7.40762 7.32862 7.4525 7.39044 6.79022 7.68802 7.2624 5.88038 6.80446 7.59042 7.52364 8.36525 7.83231 7.62233 7.75051 6.88067 7.28012 7.05245 6.99717 8.10846 6.84526 7.14104 7.22632 7.33716 6.53512 7.44333 8.06531 7.24089 7.41435 8.24534 6.53425 7.43001 7.6912 7.82459 8.04551 6.95815 8.00514 6.88858 7.12145 7.05068 7.31519 7.2924 6.23431 8.50713 8.2315 7.44343 6.89026 5.83165 7.27106 6.33362 6.91543 7.68259 6.90338 6.3672 6.29643 7.41729 7.4657 7.18047 6.92514 6.55916 7.01824 5.90903 6.5823 5.84866 6.32583 7.59187 7.09285 7.07719 7.53895 7.51366 8.12268 6.27789 6.59849 7.24358 6.31337 7.52986 7.62179 7.13867 7.60088 7.36449 7.367 8.16319 7.41456 8.09951 7.68967 8.2545 6.64568 6.84389 7.4861 7.55272 7.56996 7.44302 7.23116 6.41883 6.6873 8.96764 5.79623 7.23051 7.83527 7.08743 8.009 7.32662 7.80955 6.66513 6.86468 7.0721 7.69927 6.79618 6.67763 6.44477 6.93119 7.02453 7.11029 7.60538 6.93915 7.69728 6.56141 6.93067 8.22686 5.90936 8.25632 7.26546 7.53278 7.44042 5.93954 7.45395 7.64872 8.09834 6.98098 7.48118 5.88337 6.93749 8.21602 6.4634 7.52412 6.79703 7.68283 7.3618 6.78276 7.51221 7.346 6.90158 7.05146 8.02065 6.6508 7.8762 7.30261 7.9068 7.11821 6.92287 6.77287 7.38743 7.08803 7.8694 6.84509 7.4129 7.6255 6.75389 6.30051 8.15429 6.5919 7.15166 7.30671 7.11735 7.76113 6.93434 7.90685 7.5172 7.21671 8.05775 7.17551 6.13344 7.2269 7.27588 7.66179 7.36841 7.49648 7.06703 7.52644 7.40256 7.78325 7.02399 7.2757 6.39753 6.61806 7.53814 7.84417 6.61425 8.51229 7.301 7.58813 7.15316 8.26794 7.44756 0 0 7.11934 8.2158 5.848 7.01368 7.62987 7.00175 7.50245 6.60578 6.40503 7.26191 7.71231 6.99479 7.62517 6.60613 7.01582 7.495 6.55739 8.38117 7.30581 6.90347 7.49327 7.97902 7.97818 7.2773 6.30643 7.41403 5.48492 6.97495 7.59662 0 7.81378 6.67599 7.06441 6.9333 7.1488 6.88041 7.21156 7.45147 7.15763 6.91447 7.79564 6.98796 6.34328 7.03581 6.98663 6.60133 7.08051 7.17035 8.12492 7.20326 7.34569 7.54923 4.84561 6.60853 7.43822 7.4607 7.08151 7.86721 7.41415 6.57107 6.76988 7.87015 7.0332 6.96687 8.60964 6.41109 7.371 8.11735 7.36241 7.10505 6.1299 6.92594 7.38427 6.47601 6.64736 5.95498 7.83135 7.08014 7.19813 8.05601 6.39247 6.90131 8.13113 7.66463 7.36407 6.94415 8.16751 6.41593 7.29845 7.17968 7.91139 5.95297 7.84558 7.26966 7.26531 6.32719 7.46172 7.70447 6.61073 7.93257 7.09154 6.37196 6.71208 6.73144 7.79306 0 6.86901 7.89731 7.43367 8.06508 6.6847 7.34662 5.53784 7.18605 7.41 8.17373 7.09759 6.80029 6.96017 7.28295 6.62701 6.79692 5.9721 7.68166 7.07635 6.23583 6.83123 6.66813 6.97357 8.2803 6.83617 6.30019 6.76764 6.42098 6.35626 6.32339 7.8678 7.25688 6.62027 7.50413 7.36148 7.22686 7.46113 7.34508 7.2318 6.92774 6.84287 7.93636 6.34041 7.17903 7.60625 8.0687 6.75025 7.1044 7.05799 6.66948 7.56275 7.1778 6.24556 7.29011 6.97749 7.0142 7.28648 7.8874 6.7276 8.04704 7.38403 7.51736 6.64739 7.29701 7.45232 6.93569 7.28492 7.96551 6.71953 7.02425 7.55864 7.38336 6.10593 6.54497 7.55728 7.28819 7.99536 8.64216 6.20926 7.94391 7.86524 5.86657 7.59926 0 6.45944 7.41908 7.84876 8.33844 7.00595 7.23157 6.69516 6.13724 7.03556 6.61698 7.54843 7.60717 6.33944 7.20576 7.50324 7.7073 8.94544 7.36605 6.32536 7.29717 6.48116 6.37377 7.34595 7.15507 6.58004 6.44604 8.06701 7.18047 6.9975 6.52351 6.3948 6.9285 6.91025 7.00981 6.75584 8.01193 6.94495 7.1445 7.38821 7.02306 0 7.75895 7.36339 7.43795 6.96908 7.24015 7.004 6.85527 6.9976 7.33929 6.57355 7.17471 5.98218 7.20917 7.07432 7.14472 7.25538 6.8598 6.86898 6.85432 7.26625 7.18551 7.56842 7.55854 6.63056 6.38915 7.41152 7.11151 6.88241 7.30467 5.98477 7.13333 7.26486 7.3504 6.84289 6.99867 7.67249 6.41365 6.96655 7.59759 7.9501 6.30772 6.05629 6.15844 8.0508 6.42114 7.73174 7.7066 7.6172 7.12357 6.38656 7.50662 8.28809 7.34992 7.21644 7.3714 6.53697 6.74223 7.67118 7.32243 8.10937 7.5572 6.89143 7.82924 6.78426 7.5854 6.68121 8.3637 6.44336 6.51292 8.16415 6.50078 7.2877 6.01484 6.52494 7.74788 6.99229 6.81422 6.60911 5.51385 7.29081 6.84638 6.25934 7.18489 7.92195 7.24228 8.40161 7.46727 7.26296 6.602 7.31711 6.67125 7.5134 8.07674 7.70706 7.84172 7.3897 7.03214 7.25643 7.21136 7.69115 6.75547 6.96129 7.40199 7.58109 6.91771 7.33716 7.56193 6.76367 7.35474 7.5893 6.3893 7.82345 6.93599 7.37624 7.41818 5.19285 8.20514 8.08606 7.3683 8.11295 7.33664 7.38257 7.37374 7.45775 6.89403 0 6.83882 7.91555 7.19627 8.14607 6.61251 7.23978 6.22203 8.65712 7.31832 5.89316 7.33194 7.17515 6.21768 7.29131 7.55931 6.63795 7.06917 6.61557 7.25673 7.45604 6.56068 7.32921 6.70855 6.36248 5.9281 7.88917 7.63749 7.47758 5.96098 6.31362 6.99125 7.25913 7.54503 6.13144 6.44175 6.56741 6.66401 7.12825 6.79772 7.01268 6.90223 7.30969 5.85395 7.22247 7.61438 7.52298 7.08293 7.61424 7.312 7.16928 5.9077 6.39424 7.84656 7.01826 6.68269 6.40501 6.95893 7.40956 7.58394 6.92009 7.84342 7.09405 7.00919 6.70162 6.9297 7.87579 6.99786 7.9312 8.19839 7.47887 7.54627 7.13034 6.52379 7.16149 5.81435 7.11277 6.77503 7.65858 0 7.16467 7.91074 7.97701 7.775 8.29645 6.82149 6.2127 7.34945 6.10207 5.7456 8.10685 6.73184 7.66571 6.98658 7.33842 7.6695 8.49288 8.19774 7.18217 7.33384 6.54993 7.54132 8.26645 7.44602 6.89845 6.66681 6.76369 7.20205 6.93965 7.90029 4.93217 7.70597 6.89451 7.44054 7.32662 7.96633 6.47954 7.00169 6.87167 6.0649 6.94689 7.9341 6.52663 7.61392 7.43616 5.80872 6.09515 6.11926 5.6423 7.19172 7.52934 7.13716 7.84635 7.55136 6.37527 8.67697 7.33069 6.90808 7.41782 6.30029 7.28924 6.66266 7.61981 7.72908 7.90573 7.09733 7.3155 7.70503 7.82848 7.28374 7.19003 7.0849 7.23741 7.76708 7.31558 6.94924 6.82513 7.94061 6.53981 7.21488 6.49513 7.74527 7.33089 7.02227 6.46826 7.73048 8.20477 8.11457 6.35059 6.6773 6.69844 9.08961 6.93967 7.3011 7.8589 6.74115 7.68862 7.29318 7.27237 7.46485 7.77254 7.10024 7.42321 6.55525 5.9814 7.20391 7.51871 5.95383 7.10109 7.20859 6.95674 7.65176 6.65191 7.53997 7.77002 7.75254 7.0324 5.58071 7.87073 7.77269 7.94974 7.30994 6.80985 7.63036 6.78811 7.28236 7.58507 7.14892 7.43206 7.28448 7.66065 7.1405 8.02415 7.34461 7.03363 7.05433 8.13515 7.35659 7.12949 7.50448 7.0277 7.18506 7.53247 7.6957 7.05617 6.67122 6.36305 7.95204 6.39084 6.82703 6.16069 7.39308 8.10029 7.25324 7.54169 8.05571 6.71036 7.86272 7.00831 6.89036 6.77504 8.00829 7.23764 7.22963 6.84416 7.46079 6.25111 8.19153 7.90159 6.38363 7.32202 7.24353 7.20338 7.74061 6.24577 7.49217 7.83359 6.9777 6.72117 7.09158 7.84038 7.92778 7.41344 6.84749 8.07411 6.93738 6.28607 7.10382 7.38943 7.83558 6.98933 7.68476 7.09274 8.08046 6.72315 7.24271 6.65394 6.84708 7.2707 7.16142 6.99862 7.36793 7.51356 7.91962 7.36202 8.00235 7.54109 7.77178 6.83791 6.85032 7.51807 6.99958 7.34722 6.70806 7.59891 8.13607 8.0274 8.08626 7.84857 7.4628 0 7.70754 7.20738 6.69031 7.39994 7.74124 8.07679 7.21623 7.63267 7.38973 7.98768 8.25669 6.67376 6.53012 6.7964 7.81896 7.2796 7.75381 6.60333 6.57694 6.95349 8.24814 7.45313 6.61209 6.95054 6.49544 6.76329 6.82096 6.91542 6.87458 6.57243 6.29088 7.44275 6.46514 6.81454 6.41655 7.11917 7.54561 8.68082 7.46694 7.26736 6.20978 6.78653 7.39733 7.94656 7.5653 7.55423 7.10454 7.03482 7.20278 7.12221 7.43999 7.952 7.81396 7.32095 6.86338 6.14157 9.06533 8.04294 7.8321 6.99461 7.58334 8.03264 7.86304 6.42904 7.28556 7.4904 7.06598 6.71472 6.80509 6.78456 6.2096 6.32889 7.19447 7.46946 7.36988 6.81381 7.64219 6.87331 6.90758 7.62971 7.51938 7.53017 8.21899 5.69748 7.15912 7.82007 7.40134 7.03001 6.79468 8.02623 7.25382 7.07106 7.2101 6.91131 7.5881 6.91706 7.77534 5.97968 6.89687 7.49983 6.84533 8.65985 7.15953 6.54958 6.382 7.78375 6.96528 6.90545 7.03364 7.65634 7.58736 6.85882 8.09022 0 7.12782 7.50872 8.25555 7.34414 7.77924 6.82464 8.02862 6.52455 6.68741 7.16873 7.62265 7.94365 7.33353 7.14652 7.39078 6.70599 7.53523 6.98248 7.23336 7.1093 7.12954 7.22016 6.71422 7.34143 7.26598 7.32872 7.96799 6.99383 7.33116 6.9812 6.65271 7.52182 7.16677 7.991 8.28856 7.61181 7.14043 8.12294 7.37543 8.30253 6.54802 6.59222 7.2208 6.9266 7.18327 7.1267 7.38044 7.34245 7.85094 7.68148 7.32591 7.70327 6.79115 ]
@@@ Frame-accuracy per-class: [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4.8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7.61905 0 0 0 0 0 10.1266 0 0 0 0 0 0 0 0 17.3913 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4.65116 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6.06061 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.36134 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.05344 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 11.236 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 24.0602 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1.52672 0 0 0 0 0 0 0 0 8.48485 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 19.4595 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8.4507 0 0 0 0 0 0 0 0 0 0 0 0 15.3846 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1.0929 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ]

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:347) Done 295 files, 0 with no tgt_mats, 0 with other errors. [CROSS-VALIDATION, 0.0207319 min, fps63679.7]
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:353) AvgLoss: 7.36127 (Xent), [AvgXent: 7.36127, AvgTargetEnt: 0]
progress: []
FRAME_ACCURACY >> 0.114882% <<

WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Close():kaldi-io.cc:515) Pipe ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- | had nonzero return status 36096
