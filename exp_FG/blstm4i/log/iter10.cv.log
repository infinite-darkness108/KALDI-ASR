speech-HP-Z2-Tower-G9
nnet-train-multistream-perutt --cross-validate=true --randomize=false --verbose=0 --num-streams=10 --max-frames=15000 --feature-transform=exp_FG/blstm4i/final.feature_transform 'ark:copy-feats scp:exp_FG/blstm4i/cv.scp ark:- | apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_cv10/utt2spk scp:data-fbank/train_cv10/cmvn.scp ark:- ark:- | add-deltas --delta-order=2 ark:- ark:- |' 'ark:ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- |' exp_FG/blstm4i/nnet/nnet_iter10 
WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:243) Not in compute-exclusive mode.  Suggestion: use 'nvidia-smi -c 3' to set compute exclusive mode
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:438) Selecting from 1 GPUs
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:453) cudaSetDevice(0): NVIDIA RTX A2000 12GB	free:11620M, used:410M, total:12031M, free/total:0.96589
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:501) Device: 0, mem_ratio: 0.96589
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuId():cu-device.cc:382) Trying to select device: 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:SelectGpuIdAuto():cu-device.cc:511) Success selecting device 0 free mem ratio: 0.96589
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:FinalizeActiveGpu():cu-device.cc:338) The active GPU is [0]: NVIDIA RTX A2000 12GB	free:11106M, used:924M, total:12031M, free/total:0.923168 version 8.6
copy-feats scp:exp_FG/blstm4i/cv.scp ark:- 
apply-cmvn --norm-means=true --norm-vars=true --utt2spk=ark:data-fbank/train_cv10/utt2spk scp:data-fbank/train_cv10/cmvn.scp ark:- ark:- 
add-deltas --delta-order=2 ark:- ark:- 
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:144) CROSS-VALIDATION STARTED
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Read():nnet/nnet-matrix-buffer.h:191) Read() started... Buffer size in MB: 0, max 3072, having 0 utterances.
ali-to-post ark:- ark:- 
ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl 'ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |' ark:- 
LOG (copy-feats[5.5.1074~1-71f3]:main():copy-feats.cc:143) Copied 296 feature matrices.
LOG (apply-cmvn[5.5.1074~1-71f3]:main():apply-cmvn.cc:159) Applied cepstral mean and variance normalization to 296 utterances, errors on 0
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:303) ### After 0 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:304) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.777469, max 0.928414, mean 0.00377631, stddev 0.090768, skewness 0.0132324, kurtosis 1.20399 ) 
  f_w_gifo_r_   ( min -0.422528, max 0.425772, mean -0.000579251, stddev 0.0793185, skewness -0.00330333, kurtosis 0.0386755 ) 
  f_bias_   ( min -0.350854, max 1.43578, mean 0.2118, stddev 0.466028, skewness 1.05924, kurtosis -0.647292 ) 
  f_peephole_i_c_   ( min -0.713937, max 0.47721, mean -0.00508839, stddev 0.136989, skewness -0.120386, kurtosis 2.62616 ) 
  f_peephole_f_c_   ( min -0.747354, max 0.948171, mean 0.00549824, stddev 0.182186, skewness 0.291555, kurtosis 4.50682 ) 
  f_peephole_o_c_   ( min -0.590781, max 0.51535, mean -0.0117647, stddev 0.196349, skewness 0.0939897, kurtosis -0.14291 ) 
  f_w_r_m_   ( min -0.513627, max 0.515774, mean 0.000598835, stddev 0.106355, skewness 0.00315993, kurtosis 0.0335686 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -1.70632, max 1.21681, mean 0.00593237, stddev 0.0988247, skewness -0.1756, kurtosis 5.75895 ) 
  b_w_gifo_r_   ( min -0.348945, max 0.381961, mean -0.000195101, stddev 0.0733887, skewness 0.000752165, kurtosis -0.196383 ) 
  b_bias_   ( min -0.404941, max 1.23176, mean 0.20444, stddev 0.455367, skewness 1.02693, kurtosis -0.694856 ) 
  b_peephole_i_c_   ( min -0.469378, max 0.326105, mean 0.0032962, stddev 0.106106, skewness -0.164644, kurtosis 1.902 ) 
  b_peephole_f_c_   ( min -0.627952, max 0.755993, mean 0.0164509, stddev 0.186839, skewness 0.598042, kurtosis 3.24216 ) 
  b_peephole_o_c_   ( min -0.594783, max 0.602041, mean -0.0153227, stddev 0.203649, skewness -0.0708237, kurtosis 0.566784 ) 
  b_w_r_m_   ( min -0.426877, max 0.407225, mean -0.000416145, stddev 0.0965273, skewness 0.0011581, kurtosis -0.0150232 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.994223, max 0.803241, mean -0.000155911, stddev 0.109613, skewness 0.00547685, kurtosis 0.0591156 ) , lr-coef 1, max-norm 0
  bias ( min -0.0940893, max 2.61175, mean -4.1211e-09, stddev 0.0865484, skewness 22.7343, kurtosis 655.559 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:305) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -11.004, max 12.2048, mean 0.00682255, stddev 0.969149, skewness 0.287893, kurtosis 3.83541 ) 
[1] output of <BlstmProjected> ( min -4.67178, max 4.33733, mean 0.00040013, stddev 0.775326, skewness -0.00148835, kurtosis 0.870785 ) 
[2] output of <Tanh> ( min -0.999825, max 0.999658, mean 0.000456753, stddev 0.523259, skewness -0.00332187, kurtosis -0.888113 ) 
[3] output of <AffineTransform> ( min -14.2828, max 20.9693, mean 0.00667543, stddev 2.5212, skewness 0.691234, kurtosis 2.26128 ) 
[4] output of <Softmax> ( min 4.04802e-13, max 0.999991, mean 0.000780193, stddev 0.019036, skewness 39.8947, kurtosis 1759.57 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:334) ### After 79212 frames,
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:335) num-components 4
input-dim 78
output-dim 1280
number-of-parameters 2.04864 millions
component 1 : <BlstmProjected>, input-dim 78, output-dim 640, cell-dim 2x320 ( learn_rate_coef_ 1, bias_learn_rate_coef_ 1, cell_clip_ 50, diff_clip_ 1, grad_clip_ 250 )
  Forward Direction weights:
  f_w_gifo_x_   ( min -0.777469, max 0.928414, mean 0.00377631, stddev 0.090768, skewness 0.0132324, kurtosis 1.20399 ) 
  f_w_gifo_r_   ( min -0.422528, max 0.425772, mean -0.000579251, stddev 0.0793185, skewness -0.00330333, kurtosis 0.0386755 ) 
  f_bias_   ( min -0.350854, max 1.43578, mean 0.2118, stddev 0.466028, skewness 1.05924, kurtosis -0.647292 ) 
  f_peephole_i_c_   ( min -0.713937, max 0.47721, mean -0.00508839, stddev 0.136989, skewness -0.120386, kurtosis 2.62616 ) 
  f_peephole_f_c_   ( min -0.747354, max 0.948171, mean 0.00549824, stddev 0.182186, skewness 0.291555, kurtosis 4.50682 ) 
  f_peephole_o_c_   ( min -0.590781, max 0.51535, mean -0.0117647, stddev 0.196349, skewness 0.0939897, kurtosis -0.14291 ) 
  f_w_r_m_   ( min -0.513627, max 0.515774, mean 0.000598835, stddev 0.106355, skewness 0.00315993, kurtosis 0.0335686 ) 
  Backward Direction weights:
  b_w_gifo_x_   ( min -1.70632, max 1.21681, mean 0.00593237, stddev 0.0988247, skewness -0.1756, kurtosis 5.75895 ) 
  b_w_gifo_r_   ( min -0.348945, max 0.381961, mean -0.000195101, stddev 0.0733887, skewness 0.000752165, kurtosis -0.196383 ) 
  b_bias_   ( min -0.404941, max 1.23176, mean 0.20444, stddev 0.455367, skewness 1.02693, kurtosis -0.694856 ) 
  b_peephole_i_c_   ( min -0.469378, max 0.326105, mean 0.0032962, stddev 0.106106, skewness -0.164644, kurtosis 1.902 ) 
  b_peephole_f_c_   ( min -0.627952, max 0.755993, mean 0.0164509, stddev 0.186839, skewness 0.598042, kurtosis 3.24216 ) 
  b_peephole_o_c_   ( min -0.594783, max 0.602041, mean -0.0153227, stddev 0.203649, skewness -0.0708237, kurtosis 0.566784 ) 
  b_w_r_m_   ( min -0.426877, max 0.407225, mean -0.000416145, stddev 0.0965273, skewness 0.0011581, kurtosis -0.0150232 ) 
component 2 : <Tanh>, input-dim 640, output-dim 640, 
component 3 : <AffineTransform>, input-dim 640, output-dim 1280, 
  linearity ( min -0.994223, max 0.803241, mean -0.000155911, stddev 0.109613, skewness 0.00547685, kurtosis 0.0591156 ) , lr-coef 1, max-norm 0
  bias ( min -0.0940893, max 2.61175, mean -4.1211e-09, stddev 0.0865484, skewness 22.7343, kurtosis 655.559 ) , lr-coef 1
component 4 : <Softmax>, input-dim 1280, output-dim 1280, 

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:336) 
### FORWARD PROPAGATION BUFFER CONTENT :
[0] output of <Input>  ( min -7.66434, max 7.26584, mean -0.0147177, stddev 0.961893, skewness 0.577064, kurtosis 2.62679 ) 
[1] output of <BlstmProjected> ( min -3.82737, max 4.72627, mean -0.000971076, stddev 0.726276, skewness -0.0450941, kurtosis 1.65218 ) 
[2] output of <Tanh> ( min -0.999053, max 0.999843, mean 0.000655516, stddev 0.486905, skewness -0.00982646, kurtosis -0.581735 ) 
[3] output of <AffineTransform> ( min -13.3892, max 21.5857, mean 0.0124578, stddev 2.36615, skewness 0.884318, kurtosis 3.77229 ) 
[4] output of <Softmax> ( min 1.10222e-13, max 0.999978, mean 0.000780904, stddev 0.0202113, skewness 36.4623, kurtosis 1463.05 ) 
### END FORWARD

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:346) PER-CLASS PERFORMANCE:
@@@ Frames per-class: [ 1670 30 8 79 45 6 24 17 70 220 22 36 34 39 10 53 43 120 22 20 34 66 38 35 87 73 52 35 43 17 86 33 19 32 16 75 106 10 5 1873 21251 22 63 135 22 30 80 98 44 43 9 67 66 27 41 51 107 54 285 40 48 24 308 222 21 35 16 33 248 15 54 49 139 81 14 4 45 35 47 26 17 66 4 14 44 25 18 49 111 11 94 64 72 34 66 27 32 33 78 42 15 34 9 135 37 103 31 64 37 21 67 0 93 69 23 2 34 10 32 37 25 34 18 19 28 43 46 13 4 26 88 25 22 43 33 62 12 98 32 25 22 78 11 62 24 13 88 37 45 30 117 35 60 38 37 129 16 23 64 982 29 24 10 20 31 33 34 16 52 44 64 25 22 25 3 9 30 11 42 35 13 12 11 8 32 32 9 20 32 48 32 42 56 36 35 41 27 16 5 29 35 31 12 7 9 45 27 81 21 16 19 72 29 0 46 70 68 37 26 33 50 37 32 9 5 26 23 15 14 128 52 51 59 20 20 8 73 5 17 32 22 16 4 39 60 11 16 7 19 20 8 7 52 22 37 15 32 12 39 94 72 13 59 17 69 35 25 11 13 18 58 26 24 119 16 14 9 5 13 35 18 40 17 83 10 20 20 32 21 19 33 104 48 60 79 47 20 14 27 49 56 7 20 34 199 37 4 3 21 22 15 27 39 6 10 11 22 25 47 21 6 14 6 60 3 43 26 14 8 10 70 4 15 29 11 133 11 35 22 51 21 57 14 109 15 36 29 5 21 20 18 19 114 55 69 35 21 21 64 44 39 43 15 21 37 31 46 70 32 25 50 48 101 37 32 62 34 53 184 59 33 24 47 16 35 9 79 14 45 12 20 16 36 46 8 52 28 13 12 31 17 29 40 53 13 20 9 73 85 128 108 21 7 44 30 32 12 46 15 32 90 31 20 32 52 21 237 45 11 21 215 49 25 30 31 29 71 13 57 48 98 46 27 41 19 60 21 4 38 48 50 5 63 71 0 0 25 47 11 16 42 44 49 71 27 15 49 22 22 11 30 67 41 56 15 28 24 35 7 14 15 34 7 8 38 0 9 28 57 50 12 13 39 28 40 746 35 13 19 19 18 27 12 16 52 60 21 82 1 33 49 89 6 38 42 18 51 39 26 11 19 7 23 28 18 51 98 35 37 10 34 11 56 273 7 39 31 18 21 15 24 22 15 25 6 21 53 59 24 25 15 4 49 28 14 9 8 3 8 29 34 0 8 18 51 52 48 23 3 60 16 24 35 46 16 52 37 72 3 38 59 15 5 40 14 54 52 32 51 4 14 21 187 40 33 16 15 18 19 52 32 28 37 29 41 52 10 6 95 18 46 53 17 8 5 18 31 15 14 27 7 8 17 7 93 28 14 64 22 15 8 48 40 29 32 15 10 26 38 92 13 54 242 3 84 0 22 55 226 137 6 121 19 57 20 16 55 85 4 17 33 53 42 81 83 26 30 36 55 64 9 66 84 47 14 13 91 43 19 13 23 37 17 19 27 24 0 59 88 26 15 20 111 16 50 7 32 22 5 17 15 28 38 22 27 28 48 52 41 28 16 114 105 37 112 43 4 69 17 87 95 11 45 15 22 24 24 65 3 27 29 73 63 47 17 34 18 53 17 37 17 20 6 14 36 78 20 8 14 23 47 6 6 22 3 11 89 3 5 18 20 146 25 8 25 44 29 28 22 26 34 24 73 34 85 42 37 35 79 40 39 34 32 23 18 104 65 9 45 19 21 17 48 42 21 40 28 25 27 73 23 34 2 186 119 39 90 27 23 23 38 48 0 28 66 71 45 13 24 7 25 19 16 23 24 129 50 34 21 69 32 14 26 16 14 79 10 15 35 83 24 21 61 28 128 42 66 19 12 25 28 26 60 26 36 30 128 60 8 15 24 59 47 2 19 63 42 22 4 29 30 49 65 12 8 20 32 71 48 32 17 59 28 24 21 20 22 26 24 15 35 0 17 35 13 29 14 47 49 7 13 6 17 31 19 38 15 53 143 34 68 37 23 6 34 29 7 46 92 30 48 31 3 30 16 40 24 44 69 43 19 65 170 29 5 28 134 73 20 11 82 86 29 13 21 12 10 24 12 81 163 12 193 6 28 39 45 31 23 36 65 11 39 113 30 11 28 125 45 52 34 87 20 29 16 97 42 97 26 45 39 15 109 53 18 59 52 33 29 52 31 45 25 45 136 11 24 33 61 26 28 29 106 30 33 18 48 56 86 92 23 37 23 14 2 14 36 346 138 60 357 25 129 51 39 29 11 53 15 14 13 44 38 41 41 8 18 32 16 20 21 17 8 96 16 23 156 45 91 21 6 43 48 25 45 29 8 22 23 31 42 3 198 10 14 29 25 39 64 39 11 26 28 39 96 13 53 5 34 9 11 21 67 28 24 50 19 10 11 22 23 15 13 30 30 176 37 84 80 124 21 74 32 81 43 23 25 68 52 83 27 20 0 33 56 14 23 30 22 132 113 9 71 18 15 16 5 38 61 32 70 35 44 36 82 12 19 25 56 12 24 164 178 24 42 24 19 11 41 67 112 11 27 62 39 5 26 82 58 18 39 25 15 60 20 122 54 35 24 51 28 24 13 180 152 39 59 14 42 123 70 441 14 35 36 425 22 33 13 41 32 48 41 161 16 18 110 56 36 262 66 33 47 21 15 29 84 58 48 47 2 22 14 22 35 120 98 20 59 274 17 15 22 15 59 16 0 27 31 40 47 63 69 16 68 32 39 426 46 55 29 28 47 14 42 42 14 17 28 110 23 16 27 12 36 25 38 21 57 45 37 40 25 7 21 46 54 29 44 49 16 46 13 48 16 103 37 35 12 34 ]
@@@ Loss per-class: [ 0.613284 1.32786 2.49843 1.06455 1.12647 1.51036 1.22094 0.458854 3.16616 1.09847 1.8607 2.67297 0.487963 2.43325 1.558 2.54742 1.77781 2.28532 4.53537 0.792438 1.02007 1.62176 2.35802 1.20671 1.25251 1.13849 0.838251 3.88387 0.981007 1.11716 4.30678 5.76168 1.55939 4.61685 0.118693 0.338169 2.01303 1.24341 2.75931 1.26925 1.95538 3.87851 4.80964 1.54828 3.52118 2.74372 1.48736 7.29212 0.571918 0.312361 1.86966 2.25497 1.78974 2.30745 0.585241 1.62736 1.75508 0.563798 1.17388 1.45894 2.84008 1.74831 0.596459 1.62975 2.92945 1.50431 1.25917 1.54689 1.08842 2.15055 1.42424 1.23829 1.91885 1.45318 1.26332 0.654966 2.46661 0.242513 2.16498 0.822046 1.29829 1.68042 3.65543 0.610376 1.77835 1.4805 2.1252 2.74067 1.02411 1.69551 1.46127 0.348723 1.3302 2.74894 4.52549 2.61433 0.811637 1.57464 1.13855 0.830287 4.15531 2.00192 3.05503 1.27106 1.02872 3.22978 2.12844 1.73789 2.13874 1.17465 1.79059 0 3.01013 1.31344 1.26439 1.41181 1.38466 1.10567 1.63245 1.65409 1.52059 2.62846 3.25642 1.26503 0.314192 1.83118 2.50364 2.69883 0.313154 0.909155 0.68956 1.37324 1.09761 2.56458 2.80515 0.532246 2.52265 0.503494 1.75536 2.30004 3.2027 1.12877 1.34363 2.04651 2.21586 0.0793737 1.50681 1.28707 1.55035 1.09823 0.958811 1.13858 3.18882 1.00469 1.16407 0.595118 1.11197 1.85348 1.24913 3.83946 1.1692 2.2434 2.03386 4.02181 1.08107 2.26678 2.10979 1.50937 0.890968 2.2856 0.631024 1.71821 1.78439 2.03769 3.13254 0.733366 1.05592 1.49688 2.85181 2.07584 0.977428 2.29349 4.33694 0.635852 0.670243 1.44563 0.556256 1.37988 1.337 2.03305 1.11492 2.17354 1.96503 1.45228 1.33361 1.10716 1.4975 1.673 1.33097 0.334576 2.54758 1.98672 0.859349 1.88788 2.33267 1.50913 1.24295 1.28576 1.60116 2.2212 2.36587 2.27892 0.467335 0 1.5768 0.726518 1.62467 1.46989 1.61031 1.11905 2.69417 2.59717 1.95083 1.29702 1.4271 2.01592 3.6511 1.33831 0.379879 2.42388 1.01137 2.03505 1.18607 1.91491 3.79457 3.39165 1.10133 2.44843 1.96289 1.1184 2.61958 1.85575 3.5429 0.989387 1.84522 2.94622 0.689246 4.84193 1.72223 1.9333 2.45688 1.27507 2.92849 0.666737 2.2682 0.617118 1.19112 0.422393 2.34723 0.336687 1.26658 1.01836 1.65732 1.23686 1.9884 0.380148 1.09195 0.888338 0.5079 0.796857 2.29358 0.858366 3.73695 3.90618 1.73843 1.91234 0.335514 3.02496 1.17754 1.67906 1.17851 1.0688 1.1212 3.77756 0.839725 1.57977 0.358142 0.549624 7.23217 1.09916 1.19827 1.68393 0.680365 0.913503 3.15359 3.59247 1.50548 2.64707 2.87289 5.68797 1.13744 1.05329 2.42321 1.97322 0.393863 0.543793 3.44233 1.78097 4.51493 4.21718 1.66149 1.41791 1.64914 0.448219 2.78689 2.12054 1.37835 1.28007 1.66205 5.03027 2.95393 1.62337 3.98803 0.551712 2.6816 2.68852 2.46414 2.53395 2.34622 0.567014 2.14803 1.51849 1.91814 2.28926 0.706109 1.3025 1.76101 1.74761 1.80509 1.39095 0.999252 1.89606 0.982185 1.13071 2.61597 1.9315 1.56438 1.72244 0.492121 2.41571 2.87263 1.20883 0.298529 1.09125 2.38044 2.09341 1.4724 1.56984 2.79316 1.71161 1.12712 3.07764 1.3671 1.69877 0.667178 3.15018 0.916818 1.39468 3.09482 2.46807 3.11767 2.34132 1.01536 1.41559 0.470442 0.0766671 0.392467 0.517965 2.34903 1.75145 1.79168 1.81676 0.463798 0.358089 1.14345 1.37893 2.05312 2.39004 1.47328 3.05352 1.45098 1.41102 2.87685 2.1206 2.04858 1.04333 1.57665 3.58439 1.82424 2.31668 1.88616 0.494836 1.34936 0.893478 2.46572 0.74312 1.411 1.98464 1.46769 3.01237 4.92005 2.16476 0.341676 1.85505 2.31048 0.906832 0.60276 0.349315 2.57147 1.70731 1.97165 0.578466 0.433439 0.673995 3.87496 1.59008 0.640005 0.829302 3.61087 0.975439 1.04263 2.07642 1.70605 1.57736 2.26959 1.29993 1.14 1.6259 2.35467 1.49546 0.759747 1.02125 2.49958 2.44424 5.75626 1.11505 4.0526 4.09327 2.12819 2.2993 1.83556 1.44921 1.69465 1.70067 0 0 0.783912 1.13595 0.311384 1.54486 0.396295 1.22175 2.29293 1.16129 3.45404 1.35563 1.5354 0.648172 0.862067 2.82269 0.818878 1.17594 2.26085 0.570524 2.05696 0.725741 0.118255 3.20266 4.44 1.57469 2.90767 1.25524 1.12196 3.00784 2.82841 0 1.79194 2.74133 0.66036 1.00864 0.83379 2.21008 1.93087 6.52559 2.06926 2.17432 1.41272 5.11253 3.19127 0.515355 1.28945 0.645101 1.93474 5.47565 1.03983 1.0949 1.7419 0.673999 3.29989 1.15621 5.91489 1.49931 1.66615 1.15604 2.29306 1.98365 2.30496 2.11729 1.8204 1.29554 3.64038 1.69126 2.25616 0.968402 2.5398 0.540941 1.45824 2.87821 3.07129 1.63876 4.1269 1.26175 3.195 4.13778 1.85655 1.84253 1.36979 1.14002 1.92821 2.28906 1.03861 5.23757 2.53722 1.29795 2.82218 0.697281 0.903665 1.27232 1.37187 1.0976 2.52842 2.94846 1.5695 0.262442 3.48551 1.05814 0.859771 1.60001 2.08773 3.15417 2.90238 0 1.92684 1.68156 1.01212 2.10785 0.813181 1.386 0.690402 6.21394 1.4324 2.50298 1.78946 1.01818 3.97442 5.61336 3.93473 1.04835 0.980429 3.83678 2.70275 1.9236 0.682986 3.67697 2.01447 1.82819 6.29165 3.34184 0.585595 0.838705 0.449993 0.633366 1.26861 2.59845 2.65313 2.1396 0.981573 1.79595 3.32306 0.984859 1.71912 3.94339 3.77574 3.21899 1.00737 2.80615 1.97055 0.849904 0.370219 1.58807 1.35931 1.03895 3.88028 0.381821 1.13682 1.67263 4.27432 0.962932 1.77023 3.72007 1.34255 0.64219 1.51915 2.59663 5.74618 0.503867 1.25707 2.72124 0.74402 1.59871 5.2047 3.82227 2.03439 2.68415 0.877327 1.31031 1.39531 1.1817 1.79249 4.40484 2.27406 2.48809 0.477354 0.516592 1.0176 0 1.77536 1.59297 5.86161 1.5457 3.86346 0.893958 0.768654 0.744886 0.670657 4.82323 1.03885 8.01287 4.10141 1.58977 1.92203 1.9562 3.04722 3.64447 1.24532 3.27052 5.03826 1.10036 1.86873 0.832657 5.58816 0.993861 1.36199 1.84809 5.34594 7.05413 3.68467 2.29436 2.53628 5.70632 2.68426 2.14308 1.19938 3.48801 7.29937 2.94513 0 1.16576 2.33796 2.48693 1.98848 2.63793 5.62172 1.01678 2.49375 1.09184 0.94984 4.29546 1.55677 1.58895 0.692274 2.00695 1.82243 1.9275 2.2451 2.77171 2.48089 2.61185 0.822765 1.70294 4.81316 1.15883 0.598011 1.14938 0.730551 0.903387 2.97701 4.14972 0.495581 4.35526 2.58282 6.16423 1.53417 0.889135 2.35028 3.17422 2.17246 4.14579 0.715426 0.978216 1.18806 1.05548 1.86865 5.42063 0.863559 1.06704 3.51015 1.94661 2.10145 2.59392 1.52715 2.29466 0.537036 2.46215 1.03084 1.51133 0.801372 1.17322 1.98034 2.00501 1.6679 1.09167 1.73261 2.02229 0.839482 1.12321 6.52443 2.49727 1.826 1.59746 1.68538 1.18634 1.92017 0.757764 1.8266 1.90302 1.38468 4.09824 1.87955 1.77256 0.893139 1.08192 4.38588 2.53657 2.10049 1.3923 2.84435 2.39202 1.62887 1.64269 1.40442 1.15965 1.35014 1.76971 3.28962 1.14272 1.79587 1.37079 1.4363 0.883286 1.89208 4.14531 0.792808 3.25472 2.69489 2.73546 2.50325 1.49553 1.53755 0.692661 6.2358 2.99978 4.3529 1.76468 0.718021 2.78902 1.53422 1.76942 1.24709 3.17933 1.55674 3.13031 0 2.44119 1.52639 1.24511 2.22647 2.87466 2.45854 1.49141 2.4951 1.00983 1.13617 2.23261 5.34138 1.96997 6.23896 2.17306 1.06417 2.19404 1.25502 1.6443 2.3688 6.65134 2.08984 0.96094 2.63851 1.22752 0.24465 0.886038 2.87885 1.09739 0.800603 0.668093 4.7779 2.25757 5.8676 1.16546 2.85584 1.6312 2.38044 2.86676 2.93893 2.55527 0.40255 1.30094 1.49271 1.19582 2.16695 1.4963 2.58856 1.57427 1.23866 2.42437 2.38258 1.70368 1.2551 2.46526 3.24036 0.903184 1.68592 1.62089 1.60473 1.88851 2.34305 2.7206 2.38492 1.51803 0.97935 1.29514 0.786511 1.10982 0.821503 4.34355 2.1113 2.97699 4.68793 1.70315 0.841968 2.37593 1.3682 0 2.57115 0.960184 2.06514 1.49196 1.57824 1.2644 1.75263 2.10565 2.26838 2.22053 2.56812 0.938782 2.50852 1.27505 0.249124 1.19946 0.804039 1.23988 4.85375 1.18552 1.12697 3.73836 1.30533 3.26219 1.08799 3.03248 3.36622 1.25627 1.00279 2.64918 2.69957 2.05469 3.58304 0.798475 1.22368 3.03148 2.10705 2.20761 0.694306 2.5999 2.65529 2.49683 1.41045 2.58673 3.47687 1.37108 2.24781 1.47155 1.30249 1.81738 2.06769 2.02789 2.12405 2.30827 1.84724 1.92971 1.44775 2.62341 0.892956 1.13201 4.93781 1.22648 2.96991 2.2155 2.39138 2.91332 1.46072 1.53881 2.80182 1.92877 3.03198 2.16864 1.77692 0.588187 4.66727 1.91798 0.689187 1.22566 3.03826 1.29438 1.06827 3.51361 1.07424 2.47099 3.91118 1.95371 3.70576 2.17167 0.715837 1.19853 3.10909 1.45049 2.65474 1.19927 2.42413 3.55908 1.36587 1.93329 1.94911 1.34342 1.77796 1.25697 4.30869 3.40492 1.73154 2.25322 1.54088 1.82425 1.29118 0.74284 1.92075 1.4267 2.77451 0.663856 1.94657 4.27558 1.1434 4.18636 2.44346 1.3279 1.22696 1.41671 1.96413 1.97484 1.83975 2.57774 5.29275 1.91706 2.90525 0.709034 1.6643 1.8063 1.42697 1.35018 3.90971 2.00347 0.876149 3.46918 4.03729 0.704851 2.83038 3.9737 2.22422 1.38166 4.31127 1.68039 1.57382 2.3919 3.16198 0.904684 1.34557 4.77378 0.7506 2.13976 3.56987 2.00434 1.30901 5.16172 1.24548 5.20012 2.59165 1.94729 2.49455 1.81348 3.02558 1.10884 1.86382 1.1074 1.20194 2.56556 0.895011 1.2361 2.60548 3.44375 1.57688 1.20963 1.5579 1.46683 2.32502 0.672103 2.08443 1.54093 1.17048 2.87382 3.00031 3.09179 1.35294 1.4342 3.51987 3.76444 2.83913 2.8062 1.8323 0.451227 0.651562 2.30246 2.55445 0.944724 1.97466 1.93643 1.77657 2.04924 1.19608 2.76947 2.0272 1.00146 2.56025 2.52786 2.2293 2.057 5.37239 3.1528 1.12829 2.34032 1.44861 3.26468 0.988762 0.833787 0.800527 5.02025 0 2.10826 2.7615 2.50111 0.939856 0.576191 3.13035 1.28405 3.17035 2.97015 2.58472 3.04634 0.602195 4.38697 2.759 1.77058 3.99477 1.00797 3.43694 2.39451 2.44814 1.66602 5.39948 0.916559 2.81415 4.68575 5.86086 2.17478 1.0842 1.92447 3.04647 1.46732 2.28722 2.61251 2.62086 1.72615 1.20028 3.8328 1.40476 3.1224 0.684912 0.880285 1.41655 3.9654 2.5297 1.40811 3.09731 4.56347 2.73132 3.50844 3.18055 2.70241 2.25068 1.39726 0.685792 1.36484 2.27119 1.11741 2.59284 0.817658 1.66541 0.53709 1.9308 4.61083 2.08283 3.33179 0.437644 2.59307 0.910027 1.30657 6.10353 0.772708 0.941237 0.993499 2.1082 1.39293 1.15161 3.2351 1.09173 2.14694 3.45279 1.20632 3.0605 0.979419 1.45724 2.51492 1.69362 4.84236 2.71697 1.00565 2.23204 1.36073 1.56377 3.54091 2.76684 0.897398 2.87653 1.81507 3.15416 2.44401 3.21745 0.92338 1.92491 2.97577 2.90807 1.47865 0.867339 3.3362 1.28969 1.9164 2.44944 1.09843 4.35338 1.7428 0 2.97473 1.13704 3.95925 2.49569 2.14667 2.78733 1.67068 2.38153 1.92095 1.57778 1.35574 2.24546 1.29813 1.82091 5.02865 1.91166 2.81945 0.953688 1.36531 1.71318 2.13119 3.67777 0.655677 2.09993 2.04034 1.44269 2.04724 1.09676 4.09143 0.901307 0.981654 2.05338 3.15399 1.09235 1.70554 0.802301 3.24757 0.567954 1.16231 1.55311 1.50803 1.59534 1.16145 1.71259 2.30094 1.94558 2.05001 3.16264 1.52984 1.66709 2.8083 2.51511 1.47632 ]
@@@ Frame-accuracy per-class: [ 81.3529 72.1311 23.5294 65.4088 72.5275 30.7692 69.3878 91.4286 19.8582 69.8413 48.8889 35.6164 84.058 35.443 38.0952 41.1215 43.6782 37.3444 31.1111 82.9268 84.058 69.1729 33.7662 61.9718 66.2857 68.0272 72.381 11.2676 66.6667 62.8571 11.5607 2.98507 41.0256 9.23077 96.9697 90.0662 38.4977 57.1429 18.1818 62.5567 41.1077 13.3333 40.9449 68.6347 26.6667 26.2295 50.9317 4.06091 78.6517 91.954 21.0526 25.1852 51.1278 36.3636 79.5181 50.4854 53.9535 84.4037 73.2049 61.7284 32.9897 40.8163 78.1199 65.1685 23.2558 45.0704 48.4848 50.7463 65.1911 58.0645 62.3853 70.7071 33.6918 63.8037 62.069 88.8889 50.5495 92.9577 42.1053 75.4717 57.1429 57.1429 0 68.9655 38.2022 47.0588 16.2162 26.2626 70.852 69.5652 56.0847 89.9225 57.931 31.8841 33.0827 32.7273 67.6923 53.7313 68.7898 77.6471 12.9032 46.3768 0 65.6827 66.6667 29.9517 60.3175 44.9612 34.6667 65.1163 42.963 0 23.5294 69.0647 46.8085 0 52.1739 76.1905 33.8462 56 58.8235 34.7826 0 46.1538 87.7193 41.3793 23.6559 37.037 88.8889 64.1509 83.6158 50.9804 44.4444 32.1839 20.8955 88 32 85.2792 58.4615 47.0588 17.7778 73.8854 34.7826 35.2 28.5714 96.2963 51.9774 56 35.1648 62.2951 69.7872 59.1549 38.0165 57.1429 58.6667 78.7645 54.5455 38.2979 65.1163 22.1883 54.2373 44.898 38.0952 0 66.6667 38.806 37.6812 30.303 76.1905 26.9663 75.969 43.1373 35.5556 19.6078 0 63.1579 68.8525 52.1739 35.2941 47.8873 74.0741 16 17.3913 70.5882 76.9231 61.5385 84.2105 48.7805 33.8462 45.3608 70.7692 47.0588 54.8673 63.0137 67.6056 62.6506 47.2727 42.4242 18.1818 91.5254 19.7183 44.4444 88 13.3333 31.5789 54.9451 69.0909 73.6196 55.814 24.2424 35.8974 40 81.3559 0 47.3118 75.1773 37.9562 66.6667 45.283 65.6716 23.7624 48 52.3077 42.1053 72.7273 45.283 21.2766 58.0645 89.6552 35.7977 70.4762 46.6019 62.1849 58.5366 0 35.2941 70.7483 0 22.8571 70.7692 0 36.3636 0 70.8861 52.8926 34.7826 66.6667 13.3333 46.1538 34.1463 11.7647 53.3333 20.9524 75.5556 48 90.3226 52.3077 96 30.3797 94.1799 66.2069 66.6667 53.7815 57.1429 47.482 90.1408 54.902 69.5652 81.4815 81.0811 39.3162 79.2453 24.4898 20.9205 42.4242 20.6897 94.7368 18.1818 51.8519 33.8028 59.4595 64.1975 40 23.9521 76.1905 48.7805 82.9268 86.1538 0 66.6667 56.7164 51.6746 80.4124 62.8099 16.3522 23.1579 43.9024 20.6897 18.1818 12.1212 60.177 53.3333 34.1463 34.7826 85.213 85.3333 0 28.5714 32.5581 0 38.7097 50.9091 37.9747 92.3077 0 34.7826 57.7778 62.7451 52.6316 9.30233 15.3846 48.2759 0 77.686 0 16.092 30.1887 34.4828 35.2941 76.1905 34.0426 44.4444 38.7097 16.9492 78.2609 62.1723 43.4783 39.4366 35.5556 52.4272 79.0698 33.0435 75.8621 68.4932 25.8065 60.274 44.0678 36.3636 88.3721 43.9024 48.6486 61.5385 92.5764 66.6667 23.0216 42.2535 32.5581 51.1628 29.4574 53.9326 63.2911 16.092 58.0645 46.5116 82.6667 9.52381 70.9677 66.6667 33.8462 35.2941 29.703 28.866 75.8621 53.3333 92.3077 99.2 92.7536 78.5047 22.2222 53.7815 56.7164 48.9796 86.3158 90.9091 56.338 63.1579 60.3774 34.4828 59.3407 32 53.6585 42.4242 32.8767 32.2581 11.7647 72.381 45.614 7.40741 40 41.2698 34.2857 88.1356 61.7284 65.4206 29.6296 78.0488 42.1053 51.7007 57.3099 17.1206 9.21659 32.5581 93.3333 49.4382 39.3443 70.7692 88 92.4731 12.9032 36.9231 46.4088 73.0159 82.9268 86.1538 24.7619 51.1628 80.8421 72.5275 0 69.7674 67.7494 32.3232 50.9804 52.459 41.2698 64.4068 58.7413 51.8519 36.5217 57.732 75.1269 77.4194 32.7273 36.1446 0 74.3802 0 0 46.7532 35.0515 59.4059 18.1818 42.5197 58.7413 0 0 82.3529 65.2632 95.6522 54.5455 87.0588 62.9213 36.3636 71.3287 3.63636 51.6129 58.5859 84.4444 66.6667 26.087 72.1311 69.6296 26.506 88.4956 38.7097 66.6667 97.9592 28.169 26.6667 68.9655 19.3548 55.0725 53.3333 0 23.3766 0 42.1053 31.5789 85.2174 63.3663 64 29.6296 35.443 3.50877 41.9753 48.7609 56.338 0 10.2564 92.3077 64.8649 69.0909 56 0 60.9524 54.5455 60.4651 76.3636 0 68.6567 6.06061 59.2179 61.5385 62.3377 40 48.6486 48.5437 40.5063 33.9623 60.8696 10.2564 53.3333 34.0426 84.2105 37.8378 87.3786 65.9898 22.5352 34.6667 57.1429 23.1884 52.1739 19.469 19.3784 53.3333 53.1646 57.1429 59.4595 37.2093 45.1613 57.1429 13.3333 12.9032 58.8235 0 83.7209 82.243 67.2269 57.1429 50.9804 32.2581 44.4444 62.6263 94.7368 20.6897 52.6316 70.5882 57.1429 23.5294 27.1186 5.7971 0 11.7647 59.4595 73.7864 40 82.4742 59.5745 85.7143 1.65289 42.4242 28.5714 36.6197 68.8172 6.06061 9.52381 34.6667 62.069 28.5714 18.1818 23.5294 45.1613 72.7273 12.3457 48.2759 38.5321 17.1429 27.6923 83.4951 66.6667 89.6552 88.3721 64 41.9753 38.806 36.3636 83.871 59.4595 20.5128 66.6667 43.0769 28.0702 13.3333 13.5593 72.2892 32.381 38.0952 61.5385 84.8168 27.027 60.2151 71.028 11.4286 94.1176 54.5455 37.8378 22.2222 83.871 48.2759 7.27273 53.3333 70.5882 62.8571 26.6667 33.1551 84.2105 62.069 31.0078 88.8889 58.0645 0 14.433 61.7284 27.1186 67.6923 51.6129 66.6667 60.3774 51.9481 7.56757 37.037 33.0275 84.9485 85.7143 62.7219 0 44.4444 55.8559 28.2561 45.8182 0 70.7819 71.7949 85.2174 78.0488 0 73.8739 7.01754 0 40 17.9104 44.8598 37.6471 26.9939 55.0898 0 0 60.274 39.6396 72.8682 0 64.6617 59.1716 33.6842 0 7.40741 22.9508 16.092 35.8974 0 38.2979 32 68.5714 20.5128 0 20.4082 0 67.2269 40.678 41.5094 32.2581 48.7805 2.69058 60.6061 41.5842 53.3333 64.6154 0 18.1818 57.1429 77.4194 45.614 51.9481 53.3333 50.9091 21.0526 39.1753 43.8095 65.0602 24.5614 6.06061 60.262 77.7251 69.3333 79.1111 78.1609 0 5.7554 85.7143 11.4286 35.6021 0 59.3407 64.5161 35.5556 0 44.898 13.7405 57.1429 58.1818 67.7966 57.1429 51.9685 16.8421 62.8571 78.2609 16.2162 39.2523 40 10.6667 68.5714 39.0244 92.3077 34.4828 57.5342 50.9554 73.1707 58.8235 48.2759 46.8085 61.0526 61.5385 46.1538 48.8889 57.1429 78.2609 7.82123 0 18.1818 37.8378 48.7805 76.4505 27.451 70.5882 27.451 31.4607 71.1864 24.5614 62.2222 45.283 72.4638 73.4694 9.52381 26.087 25.731 51.7647 21.3333 22.5352 52.8302 59.2593 50.6329 66.6667 61.5385 34.0426 27.027 63.1579 36.6412 63.1579 57.1429 76.9231 32.5581 5.71429 72.1649 21.1765 23.2558 22.2222 42.1053 47.0588 43.6364 74.8299 8.51064 0 0 63.807 78.6611 20.2532 47.5138 32.7273 76.5957 25.5319 54.5455 12.3711 0 17.5439 46.6165 60.1399 37.3626 7.40741 28.5714 40 27.451 71.7949 60.6061 29.7872 12.2449 42.471 11.8812 43.4783 51.1628 35.9712 55.3846 68.9655 33.9623 0 27.5862 69.1824 28.5714 70.9677 95.7746 69.4611 4.08163 69.7674 79.6748 87.7193 12.4514 32.9412 28.5714 61.5385 8 39.2157 24.5614 30.1887 16.5289 41.5094 87.6712 65.5738 52.9183 59.5041 35.2941 51.6129 40.8163 48.7395 50.5263 40 25.641 62.9921 65.8824 26.6667 0 64.4068 45.9016 54.5455 59.542 32 11.7647 34.1463 43.0769 62.9371 74.2268 61.5385 62.8571 67.2269 80.7018 12.2449 23.2558 34.1463 0 30.1887 89.7959 32.2581 53.5211 0 28.5714 61.9718 29.6296 54.2373 20.6897 63.1579 56.5657 26.6667 37.037 15.3846 22.8571 76.1905 30.7692 62.3377 90.3226 61.6822 85.7143 69.5652 5.83942 61.3333 63.8298 46.1538 57.971 37.2881 53.3333 21.5054 14.0541 72.1311 74.2268 41.2698 28.5714 45.9016 24.2424 76.5432 65.3061 38.2022 41.7266 39.0805 76.9231 47.3282 54.5455 13.5593 54.5455 35.0877 23.7918 57.1429 24.3902 52.1739 72.7273 41.6185 30.5085 51.8519 41.8605 64 38.0952 36.7347 56 23.3129 77.6758 40 13.4367 30.7692 35.0877 20.2532 48.3516 22.2222 46.8085 63.0137 32.0611 52.1739 40.5063 47.5771 59.0164 86.9565 0 28.6853 79.1209 60.9524 26.087 57.1429 63.4146 27.1186 60.6061 46.1538 21.1765 35.8974 22.6415 26.3736 83.5443 64.5161 32.8767 50.4673 37.8378 52.1008 7.61905 26.8657 57.6271 43.8095 38.0952 61.5385 47.0588 65.9341 25.641 17.3913 57.1429 50.7463 68.2927 37.7358 59.6491 74.5763 38.4977 55.7377 26.8657 86.4865 32.9897 15.9292 80.9249 24.8649 29.7872 64 51.0638 41.3793 0 41.3793 46.5753 34.9206 8.66426 49.5868 29.0909 70.5882 41.6988 56.3107 55.6962 64.4068 17.3913 59.8131 64.5161 13.7931 22.2222 76.4045 25.974 33.7349 55.4217 58.8235 16.2162 55.3846 48.4848 39.0244 18.6047 68.5714 47.0588 7.25389 66.6667 29.7872 47.2843 46.1538 61.2022 4.65116 46.1538 0 51.5464 39.2157 28.5714 61.0169 0 57.7778 38.2979 76.1905 70.5882 0 73.0479 47.619 41.3793 6.77966 50.9804 70.8861 60.4651 55.6962 26.087 83.0189 42.1053 40.5063 54.9223 22.2222 20.5607 0 60.8696 63.1579 8.69565 13.9535 11.8519 7.01754 48.9796 89.1089 76.9231 28.5714 34.7826 62.2222 21.2766 32.2581 37.037 59.0164 62.2951 41.3598 40 68.6391 17.3913 33.7349 27.907 34.8993 9.23077 44.1718 71.2644 25.5319 66.6667 20.438 70.4762 76.6467 80 24.3902 0 26.8657 19.469 27.5862 51.0638 81.9672 0 64.1509 38.7665 0 40.5594 5.40541 90.3226 12.1212 0 49.3506 4.87805 83.0769 18.4397 30.9859 31.4607 54.7945 23.0303 64 25.641 15.6863 19.469 48 81.6327 55.9271 38.6555 61.2245 30.5882 24.4898 25.641 34.7826 67.4699 22.2222 64 17.3913 76.3636 73.6 55.6962 0 33.9623 59.3939 11.9658 0 43.038 35.2941 32.2581 34.7107 43.9024 74.2857 82.5688 39.4366 36.7347 56.3107 49.1228 65.3061 44.4444 83.6565 57.7049 2.53165 33.6134 6.89655 91.7647 44.5344 79.4326 66.3647 6.89655 76.0563 68.4932 63.2197 31.1111 68.6567 66.6667 21.6867 86.1538 30.9278 16.8675 65.0155 24.2424 59.4595 64.2534 24.7788 52.0548 19.8095 22.5564 74.6269 40 69.7674 58.0645 6.77966 24.8521 80.3419 24.7423 27.3684 0 31.1111 6.89655 66.6667 25.3521 27.3859 25.3807 63.4146 70.5882 35.7013 62.8571 32.2581 35.5556 58.0645 5.04202 30.303 0 18.1818 76.1905 37.037 25.2632 47.2441 12.9496 36.3636 7.29927 55.3846 48.1013 64.4783 45.1613 59.4595 33.8983 3.50877 52.6316 34.4828 68.2353 58.8235 62.069 45.7143 10.5263 75.1131 17.0213 48.4848 50.9091 24 57.5342 27.451 77.9221 79.0698 38.2609 0 56 51.8519 62.7451 13.3333 79.0698 55.914 47.7064 30.5085 58.427 60.6061 36.3636 15.0538 29.6296 39.1753 0 58.9372 42.6667 16.9014 16 52.1739 ]

LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:347) Done 295 files, 0 with no tgt_mats, 0 with other errors. [CROSS-VALIDATION, 0.0215209 min, fps61345]
LOG (nnet-train-multistream-perutt[5.5.1074~1-71f3]:main():nnet-train-multistream-perutt.cc:353) AvgLoss: 2.00023 (Xent), [AvgXent: 2.00023, AvgTargetEnt: 0]
progress: []
FRAME_ACCURACY >> 47.8665% <<

WARNING (nnet-train-multistream-perutt[5.5.1074~1-71f3]:Close():kaldi-io.cc:515) Pipe ali-to-pdf exp_FG/tri_8_2000_ali/final.mdl "ark:gunzip -c exp_FG/tri_8_2000_ali/ali.*.gz |" ark:- | ali-to-post ark:- ark:- | had nonzero return status 36096
